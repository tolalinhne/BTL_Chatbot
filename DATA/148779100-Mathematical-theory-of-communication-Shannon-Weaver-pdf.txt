
THE  MATHEMATICAL  THEORY
OF  COMMUNICATION
by Claude E. Shannon and Warren Weaver
THE   UNIVERSITY   OF   ILLINOIS  PRESS  .   URBANA·   1964
Tenth  printing,   1964
Also  printed  in   paperbound  edition,   1963 and  1964
First   paperbound  edition,   1963
Copyright   1949 by  the  Board  of   Trustees  of   t ho  Umvorsity  of   Illinois.
Manufactured  in  the  United  States of   America.
Library  of   Congress  Catalog  Card  No.   49-11922.
Preface
Recent   years   have   witnessed   considerable   research   activity   in
communication   theory   by   a   number   of   workers   both   here   and
abroad.   In  view  of   the   widespread   interest   in   this   field,   Dean
L.   N.   Ridenour   suggested  the   present   volume   consisting  of   two
papers  on this  subject.
The  first   paper  has  not   previously  been  printed  in  its   present
form,   although   a   condensation  appeared   in  Scientific   American,
July,   1949.   In  part,   it   consists  of   an   expository  introduction  to
the  general   theory  and  may  well   be  read  first   by  those  desiring  a
panoramic  view  of  the  field  before  entering  into  the  more  mathe-
matical   aspects.   In   addition,   some   ideas   are   suggested   for
broader   application  of   the   fundamental   principles   of   communi-
cation  theory.
The  second  paper is  reprinted  from  the  Bell  System  Technical
Journal,   July  and  October,   1948, with  no  changes  except the  cor-
rection   of   minor   errata   and   the   inclusion   of   some   additional
references,   It   is   intcnded   that   subscquent   developments   in   the
field will be treated in a projected work dealing with more general
aspects  of  information  theory.
It gives  us pleasure to  express our thanks  to  Dean Ridenour for
making this  book  possible,   and  to  the  University  of  Illinois  Press
for  their  splendid  cooperation.
C.E.SHANNON
September,   1949
CONTENTS
RECENT  CONTRIBUTIONS   TO  THE  MATHEMATICAL  THEORY
OF   COMMUNICATION   1
Warren  Weaver,   The  Rockefeller  Foundation
THE  MATHEMATICAL  THEORY  OF   COMMUNICATION
Claude  E.   Shannon,   Bell   Telephone  Laboratories
29
RECENT  CONTRIBUTIONS  TO  THE  MATHEMATICAL
THEORY OF  COMMUNICATION
By  Warren Weaver
-11------------
    
___                           LuuleS
1.1.   Communication
The  word  communication  will be  used  here  in  a  very  broad  sense
to  include   all   of   the   procedures   by   which   one   mind  may   affect
another.   This,   of   course,   involves   not   only   written   and   oral
speech,   but  also  music,   the  pictorial   arts,   the  theatre,   the  ballet,
and  in  fact   all   human  behavior.   In  some  connections   it  may  be
desirable   to   use   a   still   broader   definition   of   communication,
namely,   one   which   would   include   the   procedures   by   means   of
which   one   mechanism   (say   automatic   equipment   to   track   an
airplane   and   to   compute   its   probable   future   positions)   affects
another  mechanism  (say  a  guided  missile  chasing this   airplane).
The language of this memorandum  will often  appear to  refer to
the  special,   but  still  very  broad  and  important,   field  of  the  com-
munication   of   speech;   but   practically   everything   said   applies
1 This  paper   is  written  in  three   main  sections.   In   the  first   and  third,   W.   W.
is  responsible  both  for   the  ideas  and  the  form.   The  middle  section,   namely
"2),   Communication  Problems   of   Level   A"   is   an  interpretation   of   mathe-
matical   papers   by   Dr.   Claude   E.   Shannon   of   the   Bell   Telephone   Labora-
tories.   Dr.   Shannon's   work   roots   back,   as   von   Neumann   has   pointed   out,
to   Boltzmann's   observation,   in   some   of   his   work   on   statistical   physics
(1894),   that   entropy  is  related  to   "missing  information,"   inasmuch  as   it   is
related  to   the   number   of   alternatives   which   remain   possible   to  a   physical
system  after   all   the   macroscopically   observable   information   concerning   it
has   been  recorded.   L.   Szilard   (Zsch.   f.   Phys.   Vol.   53,   1925)   extended   this
idea  to  a   general   discussion   of   information  in   physics,   and   von   Neumann
(Math   Foundation  of   Qll,antum  Mechanics,   Berlin,   1932, Chap.   V)   treated
information   in   quantum   mechanics   and   particle   physics.   Dr.   Shannon's
work   connects   more   directly   with   certain   ideas   developed   some   twenty
years   ago  by  H.   Nyquist   and   R.   V.   L.   Hartley,   both   of   the   Bell   Labora-
tones;   and  Dr. Shannon  has himself   emphasIzed  that  communication theory
owes   a   great   debt   to   Professor   Norbert   Wiener   for   much   of   its   basic
philosophy.   Professor  "vViener,   on  the  other  hand,   points out   that   Shannon's
eaIly  work   on  switching  and  mathematical   logic  antedated  his   own  interest
in   this   field;   and   generously  adds   that   Shannon   certainly   deserves   credit
for   independent   development   of   sueh   fundamental   aspects  of   the  theory  as
the   introduction   of   entropic   ideas.   Shannon   has   naturally   been   specially
concerned   to   push   the   applications   to   engineering   communication,   while
Wiener   has   been   more   concerned   with   biological   application   (central
nervous  system  phenomena,   etc.).
4
  The  Mathematical   Theory  of   Communication
equally well to  music  of  any sort,   and to  still or  moving pictures,
as  in  television.
1.2.   Three  Levels   of   Communications   Problems
Relative to  the  broad subject of  communication,  there  seem to  be
problems at three levels.   Thus it seems reasonable to ask,   serially:
LEVEL  A.   How  accurately  can  the  symbols  of  communication  be
transmitted?   (The  technical   problem.)
LEVEL  B.   How  precisely  do  the   transmitted  symbols  convey  the
desired meaning?   (The semantic problem.)
LEVEL  C.   How  effectively  does  the   received  meaning  affect   con-
duct in the desired way?   (The effectiveness problem.)
The   technical   problems   are   concerned   with   the   accuracy   of
transference  from  sender  to  receiver  of   sets   of  symbols   (written
speech),  or  of  a  continuously  varying  signal   (telephonic  or  radio
transmission of voice or music) , or of a continuously varying two-
dimensional   pattern   (television),   etc.   Mathematically,   the   first
involves   transmission   of   a   finite   set   of   discrete   symbols,   the
second  the  transmission  of  one  continuous  function  of  time,   and
the  third  the  transmission  of  many  continuous  functions  of   time
or   of   one  continuous   function  of   time   and  of   two  space   coordi-
nates.
The semantic problems  are  concerned with  the  identity,  or sat-
isfactorily  eIose approximation,   in  the   interpretation  of  meaning
by  the  receiver,   as   compared  with  the   intended  meaning  of   the
sender.   This is a very deep and involved situation, even when one
deals only with the relatively simpler problems of communicating
through  speech.
One  essential   complication  is  illustrated  by  the  remark  that  if
Mr. X  is suspected not to  understand  what Mr.   Y  says, then it is
theoretically  not   possible,   by  having  Mr.   Y  do  nothing  but  talk
further   with  Mr.   X,   completely  to   clarify  this   situa.tion   in  a.ny
finite   time.   If  Mr.   Y  says  "Do  you   nm·v  understand  me?"   and
Mr.   X  says  "Certainly,   I   do;"  this  is  not   necessarily  a  eertiftea-
tion  that  understanding  has   been  achIeved.   It  may  just   be  that
Mr.   X  did  not   understand  the  question.   If  this  sounds   silly,   try
Some   Recent   Contributions:   Weaver   5
it   again   a8  "Czy   pan   mnie   rozumie?"   "Nith   the   ans\ver   "Hai
wakkate  imasu."   I   think that  this  basic  difficulty2 is,  at  least   in
the   restricted  field  of   speech  communication,   reduced  to  a  toler-
able   size   (but   never   completely   eliminated)   by   "explanations"
which  (a)   are  presumably never more  than  approximations to  the
ideas   being   explained,   but   which   (b)   are   understandable   since
they   are   phrased  in   language   which   has   previously   been   made
reasonably  clear  by  operational  means.   For  example,  it does  not
take  long  to  make  the  symbol   for   "yes"  in  any  language  opera-
tionally  understandable.
The  semantic  problem  has   wide  ramifications  if   one  thinks   of
communication in  general.   Consider,   for  example, the  meaning to
a Russian of a U.S. newsreel   picture.
The effectiveness  problems  are  concerned  with  the success  with
which  the  meaning  conveyed  to  the  receiver   leads  to  the  desired
conduct   on   his   part.   It   may   seem  at   first   glance   undesirably
narrow to imply that the  purpose of all   communication is to  influ-
ence the  conduct of  the  receiver.   But with  any  reasonably  broad
definition of conduct, it is clear that communication either affects
conduct   or   is  without   any  discernible  and  probable  effect   at   all.
The  problem  of   effectiveness   involves   aesthetic   considerations
in  the  case of the  fine arts.   In the  case  of speech,   written or  oral,
it involves  considerations  which  range  all   the  way  from  the  mere
mechanics  of   style,   through  all   the   psychological   and  emotional
aspects of propaganda theory, to  those value judgments which  are
necessary   to   give   useful   meaning   to   the   words   "success"   and
"desired"  in  the  opening  sentence  of  this  section  on  effectiveness.
The   effectiveness   problem  is   closely   interrelated  with   the   se-
mantIc   problem,   and   overlaps   It   In   a   rather   vague   way;   and
2 "When   Pfungst   (1911)   demonstrated   that   the   horses   of   Elberfeld,   who
were   showmg   marvelous   hngUIstiC  and   mathematIcal   abIhty,   were   merely
reactmg  to  movements  of  the  trainer's  head,   Mr.   Krall   (1911),   their  owner,
met   the  criticism  in  the  most   direct   manner.   He   asked  the   horses  whether
they  could  see   such  small   movements   and  in   answer   they  spelled  out   an
emphatic  'No.'   Unfortunately  we  cannot   all   be   so  sure   that   our   questions
are   understood   01'   obtain   such   clear   answers."   See   Lashley,   K.   S.,   "Per-
sistent  Problems in  the Evolution  of  Mind" in  Quat terly  Review of Biology,
v.   24,   March,   1949,   p.   28.
6   The  Alathematical   Theory  of   Communication
there  is  in  fact   overlap  betvv"een  all   of  the  suggested  categories  of
problems.
1.3.   Comments
So stated,   one  would  be  inclined  to  think  that  Level   A  is  a  rela-
tively   superficial   one,   involving  only   the   engineering  details   of
good  design  of  a  communication  system;   while Band  C  seem  to
contain most if not  all   of the  philosophical   content of  the  general
problem  of  communication.
The   mathematical   theory   of   the   engineering  aspects   of   com-
munication,   as  developed  chiefly  by  Claude  Shannon  at   the   Bell
Telephone  Laboratories,   admittedly  applies   in  the   first   instance
only  to  problem A, namely,   the  technical   problem  of  accuracy  of
transference  of   various   types  of   signals   from  sender  to  receiver.
But the  theory has,   I  think,  a deep  significance which  proves  that
the  preceding  paragraph  is seriously  inaccurate.   Part of  the  sig-
nificance of the new theory  comes from  the  fact that levels Band
C, above, can  make use only  of those  signal accuracies which turn
out  to  be  possible  when  analyzed  at  Level   A.   Thus   any  limita-
tions   discovered  in   the   theory   at   Level   A  necessarily   apply   to
levels Band C.   But a  larger  part  of  the  significance  comes  from
the  fact that the analysis at Level  A discloses that this level over-
laps the other levels more  than one could  possible  naively suspect.
Thus the theory of Level A is, at least to  a significant degree, also
a  theory  of   levels Band  C.   I   hope  that   the  succeeding  parts  of
this  memorandum  will   illuminate  and  justify  these  last  remarks.
-2-------
--------1etJmmunication-Problems at---l-eYel-A--------
2.1.   A  Communication  System  and  Its  Problems
The  communication  system  considered  may  be symbolically  rep-
resented  as  follows:
Some  Recent   Contributions :   Weaver
INFORMATION
SOURCE   TRANSMITTER
7
RECEIVER   DESTINATION
~
r"1
f-.-
SIGNAL   L  oJ   RECEIVED
SIGNAL
MESSAGE   MESSAGE
NOISE
SOURCE
The information source. selects  a desired message out of a set of
possible  messages   (this  is a  particularly important remark,   which
requires   considerable   explanation   later).   The   selected   message
may  consist of written or spoken words,   or  of pictures, music,   etc.
The  transmitter  changes  this   message  into  the  signal   which   is
actually  sent over the  communication  channel   from  the  transmit-
ter to  the  receiver.   In the  case of telephony, the channel is a wire,
the   signal   a   varying   electrical   current   on   this   wire;   the   trans-
mitter   is   the   set   of   devices   (telephone   transmitter,   etc.)   which
change  the sound  pressure  of  the  voice  into  the  varying electrical
current.   In  telegraphy,   the   transmitter   codes   written  words   into
sequences of interrupted currents of varying lengths   (dots, dashes,
spaces).   In  oral   speech,   the  information  source  is  the  brain,   the
transmitter  is  the  voice  mechanism  producing  the  varying  sound
pressure   (the   signal)   which   is   transmitted  through  the   air   (the
channel).   In  radio,   the  channel   is  simply  space   (or   the  aether,   if
anyone  still   prefers  that   antiquated  and  misleading  word),   and
the  signal   is the  electromagnetic  wave  which  is transmitted.
The receiver is a sort of inverse transmitter, changing the trans
mitted  signal   back  into  a  message,   and  banding  this  message  on
to  the  destination.   When  I  talk  to  you,   my  brain  is the  informa-
tion  source,   yours  the  destination;   my  vocal   system  is  the  trans-
mitter,   and   your   ear   and   the   associated   eighth   nerve   is   the
receiver.
In  the  process  of  being  transmitted,   it is  unfortunately  charac-
teristic that certain thmgs  are  added  to  the signal  which  were  not
intended  by   the   information  source.   These   unwanted   additions
8   The  Mathematical   Theory  of  Communication
may be distortions of  sound   (in  telephony,   for  example)   or static
(in   radio),   or   distortions   in  shape  or   shading  of   picture   (tele-
vision),   or   errors   in  transmission   (telegraphy  or   facsimile),   etc.
All of these changes in the transmitted signal are called  noise.
The  kind  of  questions  which  one  seeks  to  ask  concerning  such
a  communication system  are:
a.   How  does  one  measure  amount   of information?
b.   How  does   one   measure   the   capacity   of   a   communication
channel?
c.   The  action  of  the  transmitter  in  changing  the  message  into
the  signal  often  involves  a  coding  process.   What  are  the  charac-
teristics  of   an  efficient  coding  process?   And  when  the   coding  is
as   efficient   as   possible,   at   what   rate   can   the   channel   convey
information?
d.   What   are   the   general   characteristics   of   noise?   How  does
noise   affect   the  accuracy  of   the  message   finally  received   at   the
destination?   How  can   one   minimize   the   undesirable   effects   of
noise,   and  to  what extent can  they  be  eliminated?
e.   If   the   signal   being   transmitted   is   continuous   (as   in   oral
speech   or   music)   rather   than  being   formed   of   discrete   symbols
(as   in  written  speech,   telegraphy,   etc.),   how  does  this  fact   affect
the problem?
We   will   now  state,   without   any  proofs   and  with   a   minimum
of mathematical terminology, the  main results which Shannon has
obtained.
2.2.   Information
The  word  information,   in  this   theory,   is  used  in  a  special   sense
that must not be  confused  with  its  ordinary usage.   In particular,
information must not be confused with  meaning.
In   face,   two   messages,   one   of   which   is   heavily   loaded   with
meaning  and  the  other of  which  is  pure   nonsense,   can  be  exactly
equivalent,   from  the   present   viewpoint,   as   regards   information.
It  is  this,   undoubt.edly,   that   Shannon  means  when  he  says   that
"the semantic  aspects  of communication  are  irrelevant to  the  en
gineering  aspects."   But   this   does  not   mean  that   the   engineering
aspects are necessarl1y  Irrelevant to  the  semantic  aspects.
To   be   sure,   this   word   information   in   communication   theory
relates not so much  to what you  d(1 sa.y, as to what you  CQuld say.
Some Recent  Contributions:   Weaver   9
That is, information is a measure of one's  freedom of choice when
one selects  a message. If one is confronted with a very elementary
situation where  he has to  choose one of two  alternatIve messages,
then  it   is   arbitrarily  said  that   the  information,   associated  with
this situation, is unity.   Note that it is misleading  (although often
convenient)   to  say  that   one  or   the  other   message, conveys   unit
information.   The  concept of  information  applies not to  the  indi-
vidual   messages   (as   the  concept   of   meaning  would),   but   rather
to  the  situation  as  a  whole,   the  unit   information  indicating  that
in   this   situation   one   has   an   amount   of   freedom  of   choice,   in
selecting a message, which it is convenient-to regard as a standard
or unit amount.
The  two  messages   between  which   one  must   choose,   in  such   a
selection,   can  be anything one likes.   One might be the text of the
King  James  Version  of  the  Bible,   and  the  other might   be  "Yes."
The transmitter  might   code these  two  messages   so that "zero"  is
the  signal for  the  first,   and  "one"  the signal  for  the  second;   or  so
that  a  closed  circuit   (current flowing)   is  the  signal   for   the  first,
and  an  open  circuit   (no  current  flowing)   the  signal   for   the  sec-
ond.   Thus  the  two  positions,   closed  and  open,   of   a  simple  relay,
might correspond to  the two  messages.
To   be  somewhat   more   definite,   the   amount   of   information  is
defined, in the simplest cases, to  be measured  by  the logarithm of
the  number  of  available  choices.   It  being  convenient  to  use  log-
arithms"  to  the  base 2, rather than  common  or   Briggs'   logarithm
to the  base  10, the  information,   when  there  are  only  two  choices,
is  proportional   to  the  logarithm  of   2  to  the  base  2.   But   this   is
unity; so that a two-choice situation is characterized by  informa-
tion   of   unity,   as   has   already   been   stated   above.   This   unit   of
information  is  called  a  "bit,"  this  "Nord,   first   suggested  by  John
bers  are expressed  in  the  binary  system there are only  two  digits,
namely  0  and  1;   just   as  ten  digits,   0  to  9  inclusive,   are  used  in
the   decimal   number   system  which   employs   10  as   a   base.   Zero
and  one may be taken symbolically to  represent any  tvlO  choices,
as noted above;  so that "binary digit" or "bit" is natural   to asso-
ciate  with  the   two-choice  situation  which   has   unit   information.
If one has  available  say  16 alternative  messages   among  which
• When  m"  =   y,   then  z  is said  to  be  the  logarithm  of  y  to  the  base  m.
10   The  Mathematical   Theory  of  Communication
he is equally free to choose, then since 16   2
4
80 that log2  16   4,
one  says  that   this   situation  is   characterized  by  4  bits   of   infor-
mation.
It doubtless seems queer,   when one  first  meets  it,   that informa-
tion  is  defined  as  the  logarithm  of  the  number  of   choices.   But  in
the  unfolding  of   the   theory,   it   becomes   more   and  more  obvious
that   logarithmic   measures   are   in   fact   the   natural   ones.   At   the
moment,   only  one   indication  of   this   will   be  given.   It   was   men-
tioned  above  that   one   simple   on-or-off   relay,   with  its   two   posi-
tions   labeled,   say,   0  and  1  respectively,   can  handle  a  unit   infor-
mation  situation,   in  which  there  are  but  two  message  choices.   If
one  relay  can  handle unit  information,   how  much  can  be  handled
by  say  three   relays?   It   seems   very   reasonable   to   want   to   say
that   three  relays   could  handle  three   times   as   much  information
as   one.   And  this   indeed  is   the  way  it   works   out   if   one   uses   the
logarithmic  definition  of   information.   For  three  relays  are  capa-
ble  of   responding to 2
3
or  8  choices,   which  symbolically  might  be
written   as   000,   001,   011,   010,   100,   110,   101,   111,   in   the   first   of
which  all   three  relays  are  open,   and  in  the  last of  which  all   three
relays   are  closed.   And  the  logarithm  to  the  base  2  of   2
3
is  3,   so
that   the   logarithmic   measure   assigns   three   units   of   information
to  this  situation,   just  as  one  would  wish.   Similarly,   doubling  the
available   time   squares   the   number   of   possible   messages,   and
doubles  the  logarithm;   and  hence  doubles  the  information  if  it  is
measured  logarithmically.
The   remarks   thus   far   relate   to   artificially   simple   situations
where  the  information  source  is  free   to  choose  only  between  sev-
eral   definite   messages -like  a   man  picking  out   one   of   a   set   of
standard  birthday  greeting  telegrams   A  more  natura.l   and  more
important situation is that in  which  the information source makes
a   sequence  of   choices   from  some  set   of   elementary  symbols,   the
selected  sequence   then   forming   the   message.   Thus   a   man  may
pick out one  word after another, these individually selected  words
then  adding  up  to  fonn  the  message.
At this  point an important consideration  which  has  been in  the
background,   so   far,   comes   to   the   front   for   major   attention.
Namely,   the  role  whIch  probabIhty plays  III the  generatIOn of  the
message.   For  as   the  successive  symbols  are  chosen,   these  choices
are, at least  from  the  point of view of the  communication  system,
Some Recent  Contributions :   Weaver   11
governed  by  probabilities;   and  in  fact   by  probabilities  which  are
not   independent,   but   which,   at   any  stage  of  the  process,   depend
upon   the   precedmg   chOIces.   Thus,   If   we   are   concerned   wIth
English  speech,   and  if   the   last   symbol   chosen  is  "the,"  then  the
probability that the  next word  be an  article,   or a  verb  form  other
than a verbal, is very small.   This probabilistic  influence  stretches
over  more  than  two  words,   in  fact.   After  the  three  words  "in  the
event"  the  probability  for  "that"  as  the  next  word  is  fairly  high,
and  for "elephant" as  the  next word  is very low.
That there are  probabilities which exert a certain degree  of con-
trol  over the  English language also  becomes obvious if one thinks,
for  example,  of  the  fact   that  in  our   language  the  dictionary  con-
tains no words  whatsoever in  which  the  initial   letter j   is  followed
by  b, c, d,   I, g, j,   k,   I, q, r,   t,   v, w, x, or  z ; so that the  probability
is   actually   zero  that   an   initial   j   be   followed   by   any   of   these
letters.   Similarly,  anyone  would  agree  that the  probability  is  low
for   such   a   sequence   of   words   as   "Constantinople   fishing   nasty
pink."   Incidentally,   it   is   low,   but   not   zero;   for   it   is   perfectly
possible  to  think  of  a  passage  in  which  one  sentence  closes  with
"Constantinople fishing,"  and  the next begins  with  "Nasty  pink."
And   we   might   observe   in   passing  that   the   unlikely   four-word
sequence  under   discussion  has occurred  in  a  single  good  English
sentence, namely the  one above.
A system  which  produces  a  sequence  of   symbols   (which   may,
of   course,   be   letters   or   musical   notes,   say,   rather   than  words)
according  to   certain   probabilities   is   called  a   stochastic   process,
and  the  special   case  of   a  stochastic  process   in  which  the  proba-
bilities depend on the previous events, is called a  11;Jarkoff process
or  a  Markoff  chain.   Of  the   Markoff  processes   which  might   con-
ceivably  generate  messages,   t.here  is   a.   special   class   which   is   of
primary  importance  for  communication  theory,   these  being  what
are  called ergodic processes.   The  analytical  details here  are   COllI-
phcated and  the  reasonmg so deep  and involved that it has taken
some of  the  best  efforts  of  the  best mathematicians  to  create  the
associated  theory;   but   the   rough  nature  of   an  ergodic   process   is
easy  to  understand.   It is one 'Nhich produces  a  sequence  of  sym
boIs which would be a poll-taker's dream, because any  reasonably
large   sample   tends   to   be   representative   of   the   sequence   as   a
whole.   Suppose   that   two   persons   choose   samples   in   different
12   The  Mathematical   Theory  of   Communication
ways,   and  study   what   trends   their   statistical   properties   would
show  as   the  samples   become   larger.   If  the  situation  is  ergodic,
then   those   two   persons,   however   they   may   have   chosen   their
samples,   agree   in  their  estimates  of   the  properties  of   the   whole.
Ergodic  systems,   in  other  words,   exhibit   a  particularly  safe  and
comforting sort of statistical  regularity.
Now  let   us   return  to  the  idea  of  information.   When  we  have
an   information  source  which  is   producing  a   message   by  succes-
sively   selecting  discrete   symbols   (letters,   words,   musical   notes,
spots   of   a   certain   size,   etc.),   the   probability   of   choice   of   the
various  symbols  at   one  stage  of   the  process   being  dependent   on
the   previous   choices   (i.e.,   a   Markoff   process),   what   about   the
information  associated  with  this procedure?
The  quantity  which   uniquely  meets   the   natural   requirements
that   one  sets  up  for   "information"  turns   out   to   be  exactly  that
which  is known  in thermodynamics  as  entropy.   It is expressed  in
terms  of  the  various  probabilities  involved -   those  of   getting  to
certain stages in  the  process  of  forming  messages,   and the  proba-
bilities   that,   when   in   those   stages,   certain   symbols   be   chosen
next.   The fortnula,   moreover, involves the logarithm  of probabil-
ities,   so  that   it   is   a   natural   generalization   of   the   logarithmic
measure spoken of above in  connection with simple cases.
To   those   who   have   studied   the   physical   sciences,   it   is   most
significant that  an  entropy-like  expression  appears  in  the  theory
as  a  measure of  information.   Introduced  by  Clausius  nearly  one
hundred  years   ago,   closely   associated  with  the  name   of   Boltz-
mann,   and  given  deep  meaning  by  Gibbs  in  his   classic  work  on
statistical  mechanics,   entropy  has  become so  basic  and  pervasive
a  concept that Eddington  remarks "The law that entropy  always
increases   the  second  law  of   thermodynamics   holds,   I   think,
the  supreme  position  among  the  laws  of   Nature. "
In  the  physical   sciences,   the  entropy  associated  with  a   situa-
tion  is   a   measure  of   the  degree   of   randomness,   or   of   "shufHed-
ness" if  you  will,   in  the  situation;   and  the  tendency  of   physical
systems  to  become   less  and  less  organized,   to   become  more   and
more   perfectly   shuffled,   is   so  basic   that   Eddington  argues   that
it is  prImarIly  thIS tendency  which  grves  tIme  ItS  arrow -   which
would  reveal  to  US,   for  example,   whether  a  movie  of the  physical
world  is being run  forward  or  backward.
Some  Recent Contributions:   Weaver   13
Thus when one meets  the  concept of entropy in  communication
theory,   he  has   a  right   to  be  rather   excited   a   right   to  suspect
that one has  hold  of something that may turn out to  be basic  and
important.   That information be measured by  entropy is, after all,
natural   when  we  remember   that   information,   in   communication
theory,   is   associated  with  the   amount   of   freedom  of   choice   we
have  in  constructing messages.   Thus  for  a  communication  source
one  can  say,   just   as   he  would   also  say  it   of   a   thermodynamic
ensemble, "This situation  is highly  organized,   it is not character-
ized by a large degree of randomness or of choice -   that is to  say,
the  information   (or  the  entropy)   is  low."   We  will   return  to  this
point   later,   for   unless   I   am  quite   mistaken,   it   is   an   important
aspect of the  more  general  significance  of  this  theory.
Having   calculated   the   entropy   (or   the   information,   or   the
freedom  of  choice)   of  a  certain information  source,   one  can  com-
pare this to  the  maximum  value  this  entropy  could  have,   subject
only to the  condition that the  source  continue to  employ the same
symbols.   The   ratio   of   the   actual   to   the   maximum  entropy   is
called  the  relative  entropy  of  the  source.   If  the  relative  entropy
of a  certain  source  is,  say  .8, this  roughly  means that  this  source
is, in  its   choice of  symbols  to  form  a  message,   about 80  per  cent
as   free   as   it   could   possibly   be   with   these   same   symbols.   One
minus  the  relative  entropy  is  called  the  redundancy.   This  is  the
fraction  of the  structure  of  the   message  which  is  determined  not
by   the   free   choice   of   the   sender,   but   rather   by   the   accepted
statistical   rules  governing  the  use  of  the  symbols  in  question.   It
is sensibly  called  redundancy,   for   this  fraction  of  the  message  is
in  fact   redundant  in  something  close  to  the  ordinary  sense;   that
is to  say,   this  fraction  of  the  message  is  unnecessary   (and  hence
repetitive  or   redundant)   in  the   sense  that   if   it  were  missing  the
message  would  still   be  essentially  complete,   or  at   least   could  be
completed.
It  is  most   interesting  to  note   that  the  redundancy  of   English
is   just   about   50   per   cent,.   so  that   about   half   of   the   letters   or
words we choose in  writing or  speaking are  under our free  choice,
and  about  half   (although  we are   not ordinarily  aware  of   it)   are
really   controlled   by   the   statistical   structure   of   the   language.
t  The   50   per   cent   estimate   accounts   only   for   statistical   structure   out   to
about   eight  letters,  so  that   the   ultimate  value  is  presumably  a  little  higher.
14
  The  Mathematical   Theory  of Communication
Apart  from  more  seriom3  implications,   which  again  we  Yv'ill   post-
pone   to   our   final   discussion,   it   is   interesting   to   note   that   a
language   must   have   at   least   50   per   cent   of   real   freedom   (or
relative  entropy)   in  the   choice  of   letters  if   one  is  to  be  able   to
construct satisfactory  crossword  puzzles.   If it has  complete  free-
dom,   then  every  array  of   letters  is  a  crossword  puzzle.   If  it   has
only  20 per  cent of  freedom,   then  it would  be  impossible  to  con-
struct crossword  puzzles  in such  complexity  and number as would
make   the   game   popular.   Shannon   has   estimated   that   if   the
English   language  had  only   about   30  per   cent   redundancy,   then
it   would   be   possible   to   construct   three-dimensional   crossword
puzzles.
Before  closing  this  section  on  information,   it   should   be  noted
that the real reason that Level   A analysis deals  with  a  concept of
information   which   characterizes   the   whole  statistical   nature   of
the  information  source,   and  is  not   concerned  with  the   individual
messages   (and  not at all   directly  concerned  with  the  meaning  of
the  individual   messages)   is  that   from  the   point  of   view  of  engi-
neering,   a   communication   system  must   face   the   problem   of
handling  any  message  that   the  source   can  produce.   If  it   is  not
possible   or   practicable   to   design   a   system  which   can   handle
everything   perfectly,   then   the   system  should   be   designed   to
handle   well   the   jobs   it   is   most   likely   to   be   asked   to   do,   and
should  resign  itself to  be less efficient for  the  rare  task.   This sort
of   consideration  leads   at   once  to  the   necessity  of   characterizing
the   statistical   nature  of   the   whole  ensemble   of   messages   which
a   given  kind  of   source   can  and  will   produce.   And  information,
as   used  in  communication  theory,   does   just   this.
Although  it   is  not  at   all   the  purpose  of   this   paper   to  be  con-
cerned  ',vith mathematical   details,   it   nevertheless  seems  essential
to  have  as  good  an  understanding  as  possible  of the  entropy-like
expression  which   measures   information.   If   one  is   concerned,   as
in a simple  case, with a set of n independent symbols, or a set of n
independent  complete  messages  for that  matter,   whose probabili-
ties   of   choice  are   PI,   P2'   .   •   Pn,   then  the   actual   expression   for
the  information is
_____- - - - - " " H ~ _ - - - - - C [ Pi log Pi +  P2 log P2 +  .  .  .+  Pn log Pn ]f-1-'-----
or
H   ~ Pi log Pi.
Some  Recent  Contributions :   Weaver   15
\Vhere
5
the  symbol It indicates, as  is usual in  mathematics, that
one  is  to  sum  all   terms  like  the  typical   one,   Pi   log  Pi,   written  as
a  defining  sample.
This  looks   a  little  complicated;   but   let  us  see  how  this  expres-
sion  behaves  in  some  simple  cases.
Suppose  first   that   we  are   choosing  only  between  two   possible
messages,   whose  probabilities  are  then  PI   for   the  first   and  P2 =  1
-   PI   for   the   other. If   one   reckons,   for   this   case,   the   numerical
value   of   H,   it   turns   out   that   H   has   its   largest   value,   namely
one,   when  the  two  messages  are  equally  probable;   that  is  to  say
when  PI =  P2 = !;  that  is  to  say,   when  one  is  completely  free  to
choose  between  the   two  messages.   Just   as   soon   as   one   message
becomes   more   probable  than  the  other   (PI   greater  than  P2,   say),
the value of H  decreases.   And  when one  message  is very  probable
(PI   almost   one  and  P2   almost   zero,   say),   the  value  of   H  is  very
small   (almost zero).
In the  limiting  case  where  one  probability  is  unity   (certainty)
and  all   the  others  zero   (impossibility), then  H  is  zero   (no  uncer-
tainty at all- no  freedom  of   choice -   no  information).
Thus   H  is   largest   when  the   two   probabilities   are   equal   (i.e.,
when   one   is   completely   free   and   unbiased   in   the   choice),   and
reduces  to  zero  when  one's   freedom  of   choice   is   gone.
The   situation   just   described   is   in   fact   typical.   If   there   are
many,   rather than  two,   choices,   then  H  is  largest when  the  prob-
abilities   of   the   various   choices   are   as   nearly   equal   as   circum-
stances   permit -   when   one   has   as   much   freedom  as   possible   in
making  a   choice,   being  as   little   as   possible  driven  toward  some
certain  choices   which  have  more  than  their  share  of   probability.
Suppose,   on   the   other   hand,   that   one   choice   has   a   probability
near   one   so   that   all   the   other   choices   have   probabilities   near
zero.   This  is clearly a situation in  which one  is heavily  influenced
toward   one   particular   choice,   and   hence   has   little   freedom  of
choice.  And H  in  such  a  case  does  calculate  to  have  a  very  small
value -   the  informa.tion   (the  freedom  of  choice,   the  uncertainty)
is low.
'V'Vhen  the  number of  cases  is fixed,   we have  just seen  that  then
5 Do   not   WOrty   about   the   minus   sign.   Any   probability   is   a   number   less
than   or   equal   to   one,   and   the   logarithms   of   numbers   less   than   onc   arc
themselves   negative.   Thus   the  minus   sign   is   necessary  in   order  that   H   be
in  fact   positive.
16   The  Mathematical   Theory  of Communication
the   information   is   the   greater,   the   more   nearly   equal   are   the
probabilities of the various cases. There is another important way
of increasing H,   namely  by  increasing the number of  cases. More
accurately, if all   choices are equally likely, the more  choices there
are,   the   larger   H  will   bel.   There   is   more   "information"   if   you
select  freely  out  of  a  set of  fifty  standard  messages,   than  if  you
select   freely   out  of   a  set  of   twenty-five.
2.3.   Capacity  of   a   Communication  Channel
After the  discussion of  the preceding section,   one  is not surprised
that the  capacity  of  a  channel   is to  be  described  not  in  terms  of
the number of symbols it can  transmit,   but rather in terms of the
information  it   transmits.   Or   better,   since  this   last   phrase   lends
itself   particularly  well   to  a  misinterpretation  of   the   word  infor-
mation,   the  capacity  of   a  channel   is  to  be  described  in  terms  of
its   ability  to  transmit what   is  produced  out  of  source  of  a  given
information.
If the source is of  a  simple sort in  which  all  symbols  are  of the
same  time   duration   (which   is   the   case,   for   example,   with  tele-
type),   if   the  source  is   such  that   each   symbol   chosen  represents
s  bits  of   information   (being  freely   chosen  from  among   2
8
sym-
bbls) , and if  the  channel can transmit,   say ri symbols per  second,
then the  capacity  of  C of  the  channel  is defined  to  be ns  bits  per
second.
In a  more  generai  case,   one has  to  take  account of the  varying
lengths  of   the  various   symbols.   Thus  the  general   expression  for
capacity  of   a  channel   involves   the   logarithm  of   the  numbers  of
symbols   of   certain  time   duration   (which   introduces,   of   course,
the   idea   of   information   and   corresponds   to   the   factor   s   in  the
simple   case   of   the   precedIng   paragraph);   and   also   mvolves
the  number  of   such  symbols   handled   (which   corresponds  to  the
fact.or  n  of   the  preceding  paragraph).   Thus  in  the   general   case,
capacity   measures   not   the   number   of   symbols   transmitted   per
second,   but   rather   the   amount   of   information   transmitted   per
second,   using  bits  per  second  as  its   unit.
2.4.   Coding
At the  outset. it  was  pointed  out that  the   transmitter  accepts  the
message  and  turns  it  into  something  called  the   signal,   the   latter
being  what   actually  passes  over   the  channel   to  the  receiver.
Some  Recent   Contributions :   Weaver   17
The  transmitter,   in  such   a   case  as   telephony,   merely  changes
the   audible  voice  signal   over   into  something   (the  varying  elec-
trical   current   on   the   telephone   wire)   which   is   at   once   clearly
different   but   clearly  equivalent.   But   the   transmitter   may  carry
out   a  much  more   complex  operation  on  the  message  to   produce
the  signal.  It could,   for  example,  take  a  written  message   and  use
some   code   to   encipher   this   message   into,   say   a   sequence   of
numbers;   these  numbers then  being  sent over  the  channel   as  the
signal.
Thus one  says,   in  general,   that   the  function  of  the  transmitter
is to encode, and that of the  receiver to  decode,   the  message.   The
theory  provides  for  very  sophisticated  transmitters  and  receivers
-   such,   for   example,   as   possess   "memories,"   so  that   the   way
they  encode  a   certain  symbol   of   the   message   depends   not   only
upon   this   one   symbol,   but   also   upon   previous   symbols   of   the
message and the  way  they have been encoded.
Weare  now  in  a   position  to  state  the   fundamental   theorem,
produced   in   this   theory,   for   a   noiseless   channel   transmitting
discrete symbols.   This theorem  relates to  a  communication  chan-
nel   which  has   a  capacity  of  C  bits  per   second,   accepting  signals
from  a  source  of  entropy   (or  information)   of  H  bits  per  second.
The   theorem  states   that   by   devising   proper   coding   procedures
for   the   transmitter   it   is   possible   to   transmit   symbols   over   the
channel   at  an  average  rate"  which  is  nearly  C/H,   but  which,   no
matter how clever  the  coding,   can  never  be made  to  exceed  C/H.
The  significance  of   this   theorem  is  to   be  discussed  more   use-
fully   a   little   later,   when   we  have   the   more   general   case   when
noise is present. For the  moment, though, it is important to  notice
the   critical   role   which   coding  plays.
Remember   that   the   entropy   (or   information)   associated  with
the  process  which  generates  messages  or  signals  is  determined  by
the   statistical   character   of   the   process -   by   the   various   prob-
abilities for arriving at message situations and for  choosing, when
in  those   situations   the   next   symbols.   The   statistical   nature   of
messages   is  entirely  determined  by  the   character   of   the  source.
8 We  remember  that   the  capacity  C  involves  the  idea  of  information  trans-
mitted  per   second,   and  is  thus  measured  in  bits  per  second. The  entropy  H
here  measures information  per symbol, so that   the  ratio  of C to H  measures
symbols per second.
18   The  Mathematical   Theory  of  Communication
But the statistical  character of  the  signal  as  actually  transmitted
by   a   channel,   and   hence   the   entropy   in   the   channel,   is   deter-
mined  both  by  what   one   attempts   to   feed  into  the   channel   and
by   the   capabilities   of   the   channel   to   handle   different   signal
situations.   For   example,   in   telegraphy,   there   have   to   be   spaces
between   dots   and  dots,   between   dots   and   dashes,   and   between
dashes   and  dashes,   or   the   dots   and  dashes   would  not   be   recog-
nizable.
Now  it  turns  out   that  when   a   channel   does   have  certain  con-
straints   of   this  sort,   which   limit   complete  signal   freedom,   there
are  certain statistical signal   characteristics which  lead  to  a  signal
entropy  which  is  larger than it would  be  for  any  other statistical
signal   structure,   and   in  this   important   case,   the   signal   entropy
is  exactly  equal   to  the  channel   capacity.
In  terms   of   these   ideas,   it   is   now  possible   precisely  to   char-
acterize   the  most   efficient   kind  of   coding,   The   best   transmitter,
in  fact,   is  that   which   codes   the  message  in  such   a  way  that   the
signal   has   just   those   optimum  statistical   characteristics   which
are  best  suited  to  the   channel   to  be  used -   which  in  fact   maxi-
mize  the  signal   (or  one  may  say,   the   channel)   entropy  and make
it equal to the capacity C of the channel.
This  kind  of  coding  leads,   by  the  fundamental   theorem  above,
to  the  maximum  rate  C/ H  for   the  transmission  of  symbols.   But
for   this   gain  in   transmission  rate,   one   pays   a   price.   For   rather
perversely  it   happens   that   as   one   makes   the   coding  more   and
more   nearly  ideal,   one   is   forced   to   longer   and   longer   delays   in
the  process of coding. Part of this dilemma is met by  the  fact that
in  electronic  equipment   "long"   may  mean  an   exceedingly  small
fraction   of   a   second,   and   part   by   the   fact   that   one   makes   a
compromise,   balancing  the  gain  in  transmission  rate  against   loss
of  coding time.
2.5.   Noise
How  does   noise   affect   information?   Information   is,   we   must
steadily remember, a measure of one's freedom  of choice in select
iug  a  message.   The greater  this  freedom  of   choice,   and  hence  the
greater   the   InformatIOn,   the   greater   IS   the   uncertainty  that   the
message   actually   selected   is   some   particular   one.   Thus   greater
Some  Recent   Contributions :   Weaver   19
freedom  of   choice,   greater   uncertainty,   greater   information   go
hand  in  hand.
If   noise   is   mtroduced,   then   the   receIved   message   contains
certain   distortions,   certain   errors,   certain   extraneous   material,
that   would  certainly  lead  one   to  say  that   the   received  message
exhibits,   because  of   the  effects   of   the   noise,   an   increased  uncer-
tainty.   But   if   the   uncertainty   is   increased,   the   information   is
increased,   and  this  sounds  as   though  the  noise  were   beneficial!
It is generally  true that when  there  is noise,   the  received  signal
exhibits   greater   information -   or   better,   the   received   signal   is
selected  out   of   a  more  varied  set   than  is  the  transmitted  signal.
This is a  situation which  beautifully  illustrates  the  semantic trap
into  which  one   can  fall   if   he  does   not   remember  that   "informa-
tion"  is  used  here   with  a  special   meaning that  measures   freedom
of choice  and  hence  uncertainty  as  to  what choice  has  been  made.
It   is   therefore   possible   for   the   word  information  to   have   either
good  or   bad  connotations.   Uncertainty  which  arises  by  virtue  of
freedom  of   choice   on  the   part   of   the   sender   is   desirable   uncer-
tainty.   Uncertainty  which  arises  because  of   errors  or   because  of
the  influence  of  noise  is  undesirable  uncertainty.
It  is  thus   clear   where  the   joker   is  in  saying  that   the   received
signal  has  more  information.   Some  of  this  information  is  spurious
and  undesirable   and  has   been   introduced  via   the   noise.   To  get
the   useful   information   in   the   received   signal   we   must   subtract
out  this  spurious   portion.
Before  we  can  clear   up  this  point   we  have  to   stop  for   a   little
detour.   Suppose one  has   two  sets   of  symbols, such  as  the  message
symbols   generated   by   the   information   source,   and   the   signal
symbols   which   are   actually  received.   The   probabilities   of   these
two   sets   of   symbols   are  interrelated,   for   clearly  the   probability
of   receiving   a   certain   symbol   depends   upon   vlhat   symbol   was
sent.  With  no errors  from  noise   01'   from  other  causes,  the  received
sIgnals  would  correspond  preCIsely  to  the  message  symbols  sent;
and in the presence of possible error, the  probabilities for   received
symbols   would   obviously  be  loaded   heavily  on  those  which   cor-
respond, or  closely  correspond,   to  the  message  symbols sent.
Now  in  such   a   situation  one   can   calculate   what   is   called   the
entropy  of   one  set   of   symbols   relative  to   the  other.   Let   us,   for
example,   consider   the   entropy   of   the   message   relative   to   the
20   The   Mathematical   Theory  of Communication
signal.   It   is   unfortunate   that   we   cannot   understand   the   issues
involved   here   without   going   into   some   detail.   Suppose   for   the
moment that one knows that a  certain signal symbol has  actually
been   received.   Then   each   message   symbol   takes   on   a   certain
probability -   relatively large for the  symbol identical with  or the
symbols  similar  to  the  one  received,   and  relatively  small   for   all
others.   Using  this  set   of   probabilities,   one  calculates   a  tentative
entropy  value.   This   is   the   message   entropy  on  the   assumption
of   a  definite  known  received  or   signal   symbol.   Under   any  good
conditions   its   value   is   low,   since   the   probabilities   involved   are
not   spread   around  rather   evenly  on  the   various   cases,   but   are
heavily  loaded  on  one  or   a   few  cases.   Its   value   would   be   zero
(see  page   13)   in   any   case   where   noise   was   completely   absent,
for then, the signal symbol being  known,  all   message  probabilities
would  be  zero  except   for  one  symbol   (namely  the   one  received),
which  would  have  a  probability  of   unity.
For each  assumption  as  to  the  signal   symbol   received,   one  can
calculate  one  of   these  tentative  message   entropies.   Calculate  all
of them, and then average them, weighting each  one in accordance
with  the  probability  of the  signal   symbol   assumed  in  calculating
it.   Entropies   calculated  in  this  way,   when  there  are   two  sets   of
symbols  to  consider,   are   called  relative  entropies.   The  particular
one  just   described  is  the  entropy  of   the   message   relative  to  the
signal,   and  Shannon has  named  this  also  the  equivocation.
From the way  this  equivocation  is  calculated, we can  see what
its   significance   is.   It   measures   the   average   uncertainty   in   the
message   when  the   signal   is   known.   If  there  were  no  noise,   then
there   would   be   no   uncertainty   concerning   the   message   if   the
signal   is   known.   If   the   information   source   has   any   residual
uncertainty  after   the  signal   is   known,   then  this   must   be  unde-
sirable uncertainty  due  to  noise.
The  discussion  of   the  last   few  paragraphs   centers   around  the
quantity "the average               in  the  message source  when the
received signal is known." It can  equally well be phrased in terms
of  the  similar  quantity  "the  average  uncertainty  concerning  the
received   signal   when   the   message   sent   is   known."   This   latter
uncertainty  would,   of  course,   also  be  zero if  there  were  no  noise
As   to   the   interrelationship   of   these   quantities,   it   is   easy   to
prove  that
H(x)   Hy(x)   -   H(y)
Some Recent  Contributions:   Weaver   21
where  H (x)   is  the   entropy  or   information  of   the  source  of   mes-
sages;   H (y)   the   entropy   or   information   of   received   signals;
Hy(x)   the  equivocation, or the  uncertainty  in  the message source
if   the   signal   be   known;   Hit (y)   the   uncertainty   in   the   received
signals if  the messages sent be known,   or  the spurious part of the
received  signal   information  which  is  due  to  noise.   The  right  side
of this  equation is the  useful  information  which  is  transmitted  in
spite of the bad effect of the noise.
It  is  now  possible  to  explain  what   one  means   by  the  capacity
C  of   a   noisy   channel.   It   is,   in   fact,   defined  to   be   equal   to  the
maximum  rate   (in  bits  per  second)   at   which  useful   information
(i.e.,   total   uncertainty   minus   noise   uncertainty)   can   be   trans-
mitted  over  the  channel.
Why  does   one   speak,   here,   of   a   "maximum"  rate?  What   can
one  do,   that   is, to  make  this  rate  larger  or   smaller?  The  answer
is   that   one   can   affect   this   rate   by   choosing   a   source   whose
statistical   characteristics   are   suitably   related   to   the   restraints
imposed  by  the  nature  of  the  channel. That  is, one  can maximize
the rate of transmitting useful information by using proper coding
(see pages  16-17).
And  now,   finally,   let  us   consider  the  fundamental   theorem  for
a noisy  channel.  Suppose that this noisy  channel has,  in  the  sense
just   described,   a   capacity   C,   suppose   it   is   accepting   from  an
information   source   characterized   by   an   entropy   of   H (x)   bits
per   second,   the  entropy  of   the  received  signals   being  H (y)   bits
per   second.   If  the  channel   capacity  C  is  equal   to  or   larger  than
H (x),   then  by   devising  appropriate   coding  systems,   the  output
of   the   source  can  be  transmitted  over  the   channel   with  as   little
error   as   one   pleases.   However   small   a   frequency   of   error   you
specify,   there   is   a   code   which   meets   the   demand.   But   if   the
channel   capacity  C  is  less   than  H (x),   the  entropy  of   the  source
from  which   It   accepts   messages,   then  It   is   impossible  to   devise
codes which  reduce the  error frequency  as  low as  one may please.
However   clever   one  is  with  the   coding  process,   it  will   always
be  true  that   after   the  signal   is  received  there   remains   some   un
desirable   (noise)   uncertainty  about   what   the  message  was;   and
thIS  undeSIrable   uncertainty -   this   equivocation -   WIll   always
be   equal   to   or   greater   than   H (x)   -   C.   Furthermore,   there   is
always   at   least   one   code   which   is   capable   of   reducing   this
22   The  Mathematical   Theory  of  Communication
undesirable uncertainty,   concerning the message,   down  to  a value
which  exceeds H(x)   C by  an  arbitrarily small  amount.
The   most   Important   aspect,   of   course,   is   that   the   minimum
undesirable  or   spurious  uncertainties   cannot   be  reduced  further,
no   matter   how  complicated   or   appropriate   the   coding   process.
This   powerful   theorem  gives   a   precise   and   almost   startlingly
simple   description   of   the   utmost   dependability   one   can   ever
obtain   from   a   communication   channel   which   operates   in   the
presence  of  noise.
One  practical   consequence,   pointed  out   by  Shannon,   should  be
noted.   Since   English   is   about   50  per   cent   redundant,   it   would
be possible to  save about one-half the time of ordinary telegraphy
by a proper encoding process, provided one were going to transmit
over  a  noiseless  channel.   When  there  is  noise  on  a  channel,   how-
ever,   there  is  some  real   advantage  in  not   using  a  coding  process
that  eliminates  all   of  the  redundancy.   For  the  remaining  redun-
dancy  helps   combat   the  noise.   This  is  very  easy  to  see,   for   just
because  of   the  fact   that   the  redundancy  of   English  is  high,   one
has,   for  example,   little or  no  hesitation  about  correcting errors  in
spelling that  have  arisen  during  transmission.
2.6.   Continuous   Messages
Up  to  this   point   we  have  been   concerned  with  messages   formed
out of  discrete  symbols,   as  words   are  formed  of  letters,   sentences
of   words,   a   melody   of   notes,   or   a   halftone   picture   of   a   finite
number   of   discrete   spots.   What   happens   to   the   theory   if   one
considers continuous messages, such as  the speaking voice with its
continuous  variation  of   pitch  and  energy?
Very  roughly  one  may  say  that  the  extended  theory  is  some-
what   more   difficult.   and   complicated   mathematically,   but   not
essentially   different.   Many   of   the   above   statements   for   the
discrete   case   require   no   modification,   and   others   require   only
mmor  change.
One  circumstance  which  helps  a  good  deal   is  the  following.   As
a   practical   matter,   one   is   always   interested   in   a   continuous
signal  TNhich is built up  of simple  harmonic  constituents of not  all
frequencies,   but   rather   of   frequencies   which   lie   wholly   within
a  band  from  zero  frequency  to,   say,   a  frequency  of   W  cycles  per
second.   Thus  although  the  human  voice  does   contain  higher   fre-
Some  Recent   Contributions:   Weaver   23
quencies,   very  satisfactory  communication  can  be  achieved  over
a telephone  channel that  handles  frequencies  only up  to,   say  four
thousand.   WIth   frequencies   up  to   ten  or   twelve   thousand,   high
fidelity  radio  transmission  of symphonic  music  is possible,   etc.
There  is a very  convenient mathematical   theorem  which  states
that a  continuous  signal,   T  seconds  in  duration  and  band-limited
in  frequency  to  the  range  from  0  to  W,   can  be  completely  speci-
fied   by   stating   2TW  numbers.   This   is   really   a   remarkable
theorem.   Ordinarily   a   continuous   curve   can   be   only   approxi-
mately   characterized   by   stating   any   finite   number   of   points
through  which  it passes,   and  an  infinite  number  would  in  general
be required  for  complete  information  about  the  curve.   But  if  the
curve  is built up out of simple  harmonic  constituents of  a  limited
number   of   frequencies,   as   a   complex  sound   is   built   up   out   of   a
limited number of pure tones,  then  a  finite  number of  parameters
is   all   that   is   necessary.   This   has   the   powerful   advantage   of
reducing  the   character   of   the   communication   problem  for   con-
tinuous   signals   from  a   complicated   situation   where   one   would
have  to  deal   with  an  infinite  number  of   variables  to  a  consider-
ably   simpler   situation   where   one   deals   with   a   finite   (though
large)   number  of   variables.
In   the   theory   for   the   continuous   case   there   are   developed
formulas  which  describe the  maximum  capacity C of a  channel of
frequency  bandwidth  W,   when  the   average  power  used  in  trans-
mitting  is  P,   the   channel   being  subject   to   a   noise   of   power   N,
this   noise   being  "white   thermal   noise"   of   a   special   kind   which
Shannon  defines.   This  white   thermal   noise  is  itself   band  limited
in  frequency,   and  the   amplitudes   of   the   various   frequency   con-
stituents  are   subject   to  a   norma]   (Gaussian)   probability  distri-
bution.   Under these  circumstances, Shannon  obtains the  theorem,
again   really   quite   remarkable   in   its   simplicity   and   its   scope,
that It is possible,   by  the  best coding, to  transmit binary  digits at
the   rate  of
P+
  H
bits   per   second  and  have   an   arbitrarily   10'11   frequency  of   error.
But   this  rate  cannot  possibly   be  exceeded,   no  matter  how  clever
the   codmg,   WIthout  giving  rise  to  a  definIte   frequency  of   errors.
For   the   case  of   arbitrary  noise,   rather   than  the   special   "white
The  Mathematical   Theory  of   Communication
thennal"   noise   assumed   above,   Shannon   does   not   succeed   in
deriving  one  explicit   formula   for   channel   capacity,   but   does  ob-
tain  useful   upper  and  lower   limits  for   channel   capacity.   And  he
also   derives   limits   for   channel   capacity  when  one   specifies   not
the   average   power   of   the   transmitter,   but   rather   the   peak
instantaneous  power.
Finally it should  be stated that Shannon  obtains  results  which
are necessarily somewhat less specific, but which  are  of obviously
deep  and  sweeping  significance,   which,   for   a  general  sort  of  con-
tinuous message  or sigpal, characterize the fidelity of the received
message,   and  the   concepts   of   rate  at   which   a   source   generates
information,   rate   of   transmission,   and   channel   capacity,   all   of
these being relative to  certain fidelity  requirements.
3
The  Interrelationship  of  the  Three  Levels
of   Communication  Problems
3.1.   Introductory
In  the  first   section  of   this  paper it  was   suggested that  there  are
three levels at which one may consider the general communication
problem.   Namely,   one  may  ask:
LEVEL  A.  How  accurately   can  the  symbols  of   communication  be
transmined?
LEVEL  B.   How  precisely  do  the  transmitted  symbols   convey  the
desired  meaning?
LEVEl,   C   How  effectively  does  the  received   meaning  affect   con-
duct in the desired  ~ . v a y ?
It was  suggested  that  the  mathematical   theory  of  communica-
tion,   as  developed  by  Shannon,   Wiener,   and  others,   and  particu,
larly the  more  definitely  engineering  theory  treated  by  Shannon,
although  ostensibly   applicable  only   to  Level   A  problems,   actu-
ally  is  helpful   and  suggestive  for   the  level Band  C  problems.
Some  Recent  Contributions:   Weauer   25
'He  then  took  a   look,   in  section  2,   at   what   this  mathematical
theory  is, what concepts it develops,   what  results it has obtained.
It   is  the  purpose  of   this   concludmg  section  to  review  the  situa-
tion,   and   see   to   what   extent   and   in   what   terms   the   original
section  was justified in indicating that the  progress made at Level
A  is  capable  of   contributing  to  levels  Band  C,   was   justified  in
indicating  that   the   interrelation   of   the   three   levels   is   so   con-
siderable  that   one's   final   conclusion  may  be  that   the  separation
into  the  three  levels  is  really  artificial   and  undesirable.
3.2.   Generality  of   the  Theoty  at   Level   A
The obvious  first   remark,   and  indeed  the  remark that  carries  the
major  burden  of  the  argument,   is  that   the   mathematical   theory
is  exceedingly  general   in  its   scope,   fundamental   in  the  problems
it   treats,   and   of   classic   simplicity   and   power   in   the   results   it
reaches.
This  is a  theory so general that one  does  not need  to  say what
kinds  of  symbols  are   being  considered -   whether  written  letters
or words, or musical  notes, or spoken words,   or  symphonic  music,
or pictures. The theory  is deep  enough  so that the  relationships it
reveals  indiscriminately  apply  to  all   these  and  to  other  forms   of
communication.   This   means,   of   course,   that   the  theory  is   suffi-
ciently   imaginatively   motivated   so  that   it   is   dealing   with   the
real   inner core of the  communication  problem -   with  those  basic
relationships  which  hold  in  general,   no  matter  what special   form
the  actual case may take.
It is  an  evidence  of  this  generality  that  the  theory  contributes
importantly  to,   and  in  fact   is  really  the  basic  theory  of  cryptog-
raphy  which  is,   of   course,   a   form  of   coding.   In  a   similar   way,
the   theory   contributes   to   the   problem  of   translation   from  one
language   to   another,   although   the   complete   story   here   clearly
requires   consideration   of   meaning,   as   well   as   of   InformatIOn.
Similarly,   the   ideas   developed   in   this   work   connect   so   closely
with   the   problem  of   the   logical   design   of   great   computers   that
it   is  no  surprise  that   Shannon  has   just   v:ritten  a   paper   on  the
design   of   a   computer   which   would   be   capable   of   playing   a
skIllful   game   of   chess.   And   It   IS   of   further   dIrect   pertmence   to
the   present   contention  that   this   paper   closes   with   the   remark
that   either  one  must   say  that   such  a  computer   "thinks,"  or   one
26   The  Mathematical   Theory  of   Communication
must   substantially   modify   the   conventional   implication   of   the
verb  "to  think. "
As  a  second  point,   it   seems   clear   that   an   important   contribu-
tion has been  made to  any  possible general  theory  of  communica-
tion  by  the   formalization  on  which   the   present   theory  is   based.
It  seems  at  first   obvious  to  diagram  a  communication  system  as
it is done  at  the  outset of  this  theory;   but   this  breakdown  of  the
situation  must   be  very  deeply   sensible   and  appropriate,   as   one
becomes  convinced when  he sees  how smoothly  and  generally  this
viewpoint   leads  to  central   issues. It  is  almost   certainly  true  that
a  consideration  of  communication  on  levels  Band  C  will   require
additions   to   the   schematic   diagram  on   page   7,   but   it   seems
equally  likely  that   what   is  required  are  minor  additions,   and  no
real   revision.
Thus   when  one  moves   to  levels   Band  C,   it   may  prove  to  be
essential   to  take   account   of   the  statistical   characteristics  of   the
destination.   One   can   imagine,   as   an   addition   to   the   diagram,
another  box  labeled  "Semantic  Receiver"  interposed  between  the
engineering receiver   (which  changes  signals  to  messages)   and  the
destination.   This   semantic   receiver   subjects   the   message   to   a
second   decoding,   the   demand   on   this   one   being   that   it   must
match  the  statistical   semantic   characteristics   of   the   message   to
the  statistical   semantic  capacities  of   the  totality  of   receivers,   or
of   that   subset   of   receivers   which   constitute   the   audience   one
wishes  to  affect.
Similarly  one  can  imagine  another   box  in  the  diagram  which,
inserted   between   the   information   source   and   the   transmitter,
would  be  labeled  "semantic  noise,"  the  box  previously  labeled  as
simply  "noise"  now  being  labeled  "engineering  noise"  From  this
source  is  imposed  into  the  signal   the  perturbations  or   distortions
of   meaning   which   are   not   intended   by   the   source   but   which
inescapably  affect   the  destinatIOn.   And  the  problem  of   semantic
decoding  must   take   this   semantic  noise   into  account.   It   is   also
possible to  think  of  an  adjustment  of original   message so that the
sum  of   message   meaning   plus   semantic   noise   is   equal   to   the
desired  total   message  mcaning at  the  dcstination.
ThIrdly,  It seems  hIghly suggestIve  for  the  problem at all   levels
that   error   and   confusion   arise   and   fidelity   decreases,   when,   no
matter  how  good  the   coding,   one  tries  to  crowd  too  much  over  a
Some  Recent   Contributions:   Weaver   27
channel   (i.e.,   H >  C).   Here  again  a   general   theory  at   all   levels
will   surely  have  to  take  into  account not only  the  capacity  of  the
channel   but  also   (even  the  words   are  rIght!)   the  capacity  of   the
audience. If  one  tries  to  overcrowd  the  capacity  of   the  audience,
it   is   probably   true,   by   direct   analogy,   that   you   do   not,   so   to
speak,   fill  the  audience  up  and  then  waste  only  the  remainder  by
spilling.   More   likely,   and  again   by  direct   analogy,   if   you  over-
crowd   the   capacity   of   the   audience   you   force   a   general   and
inescapable error and  confusion.
Fourthly, it  is  hard  to  believe  that  levels Band  C  do  not   have
much   to   learn   from,   and   do   not   have   the   approach   to   their
problems   usefully   oriented   by,   the   development   in   this   theory
of the entropic ideas in  relation to  the concept of  information.
The   concept   of   information   developed   in   this   theory   at   first
seems   disappointing  and   bizarre -   disappointing  because   it   has
nothing   to   do   with   meaning,   and   bizarre   because   it   deals   not
with  a  single  message  but  rather  with  the  statistical   character  of
a   whole  ensemble  of   messages,   bizarre  also  because  in  these  sta-
tistical   terms   the   two   words   information   and   uncertainty   find
themselves   to   be  partners.
I   think,   however,   that   these   should   be   only   temporary   reac-
tions;   and that  one  should  say, at the  end,   that  this  analysis  has
so  penetratingly  cleared  the  air  that   one   is  now,   perhaps   for   the
first   time,   ready   for   a   real   theory   of   meaning.   An   engineering
communication  theory  is  just  like  a  very  proper  and  discreet  girl
accepting  your   telegram.   She  pays   no  attention  to   the   meaning,
whether   it   be  sad,   or   joyous,   or   embarrassing.   But   she   must   be
prepared  to  deal   with  all   that  come  to  her  desk.   This  idea  that  a
communication   system  ought   to   try   to   deal   with   all   possible
messages,   and that  the  intelligent  way  to  try  is  to  base  design  on
the   statistical   character   of   the   source,   is   surely   not   without
significance   for   communication   in   general.   Language   must   be
designed   (or   developed)   with  a  view  to  the  totality  of  things that
man  may  wish   to   say;   but   not   being  able  to   accomplish   every-
thing,   it   too   should  do   as   well   as   possible   as   often   as   possible.
That is  to  say, it too should deal   with  its  task  statistically.
The  concept   of   the  InformatIOn  to  be   associated  with  a  source
leads directly,   as  we  have  seen,   to  a  study  of  the  statistical struc-
ture  of   language;   and   this  study  reveals   about   the  English   Ian-
28   The  Mathematical   Theory  of   Communication
guage,   as  an  example,   information which  seems  surely  significant
to  students  of  every  phase  of   language  and  communication.   The
idea  of  utilizing the  powerful  body  of theory  concerning  Markoff
processes  seems  particularly promising  for  semantic studies, since
this  theory  is specifically  adapted  to  handle  one  of  the  most  sig-
nificant   but difficult   aspects  of  meaning,   namely  the  influence  of
context.   One has  the vague  feeling that  information  and  meaning
may  prove  to  be  something  like   a  pair  of   canonically  conjugate
variables   in   quantum  theory,   they   being   subject   to   some   joint
restriction  that   condemns  a  person  to  the   sacrifice  of  the  one  as
he  insists on having much  of the  other.
Or   perhaps  meaning  may  be  shown  to  be  analogous  to  one  of
the quantities on which the entropy of a thermodynamic ensemble
depends.   The   appearance   of   entropy   in   the   theory,   as   was   re-
marked earlier, is surely most interesting and significant. Edding-
ton   has   already   been   quoted   in   this   connection,   but   there   is
another   passage  in  "The  Nature  of   the   Physical   World"   which
seems  particularly  apt  and  suggestive:
Suppose  that   we were  asked  to  arrange  the  following  in  two  cate-
gories -   distance,   mass, electric  force,   entropy,   beauty,  melody.
I   think  there  are  the  strongest  grounds  for   placing  entropy  along-
side beauty and  melody,   and  not with  the  first  three.   Entropy is only
found  when  the  parts  are  viewed in  association,   and  it  is by  viewing
or   hearing  the   parts  in  association  that   beauty  and  melody  are   dis-
cerned.   All   three   are   features   of   arrangement.   It   is   a   pregnant
thought   that  one of these  three  associates should  be  able  to  figure as
a   commonplace   quantity  of   science.   The   reason   why   this   stranger
can  pass  itself  off among  the  aborigines  of the  physical   world is that
it is able  to  speak their  language,   viz., the  language  of arithmetic.
I   feel   snre  that  Eddington  would  have  been  willing  to  include
the  word  meaning  along  with  beauty  and  melody;   and  I   suspect
he  would   have  been  thrilled  to  see,   in   this   theory,   that   entropy
not   only  speaks   the   language   of   arIthmetIC;   It   also   speaks   the
language  of language.
THE  MATHEMATICAL  THEORY  OF  COMMUNICATION
By  Claude  E.   Shannon
Introduction
The  recent   development   of   various   methods   of   modulation  such
as   PCJ\t!  and  PPM  which  exchange  bandwidth  for   signal-to-noise
ratio  has  intensified  the  interest  in  a  general   theory  of   communi-
cation.   A  basis   for   such   a   theory   is   contained  in   the   important
papers   of   Nyquist.'   and  Hartley"   on  this   subject.   In  the   present
paper   we   will   extend   the   theory   to   include   a   number   of   new
factors,   in   particular   the   effect   of   noise   in  the   channel,   and  the
savings   possible   due   to   the   statistical   structure   of   the   original
message   and   due   to   the   nature   of   the   final   destination   of   the
information.
The  fundamental   problem  of   communication  is  that   of   repro-
ducing  at   one   point   either   exactly   or   approximately   a   message
selected at another point.  Frequently  the  messages  have meaning;
that   is   they  refer  to  or   are   correlated  according  to  some   system
with   certain   physical   or   conceptual   entities.   These   semantic
aspects  of   communication  are  irrelevant  to  the  engineering  prob-
lem.   The   significant   aspect   is   that   the   actual   message   is   one
selected   from  a  set   of   possible   messages.   The   system  must   be
designed  to   operate   for   each   possible   selection,   not   just   the   one
which  will   actually  be   chosen  since  this   is   unknown  at   the  time
of  design.
1 Nyquist,   H.,   "Certain   Factors   Affecting   Telegraph   Speed,"   Bell   System
Technical   Journal,   AprIl   1924, p.   324;   "Certain  Topics   in  Telegraph  Trans-
mission  Theory,"   A  l   l<;   E   Trans,   v   47,   April   1928, p   617
2 Hartley,   R.   V.   L., "Transmission   of   Information,"   Bell   System  Technical
Journal,   July  1928, p.   535.
32   The  Mathematical   Theory  of   Communication
If  the number of  messages   in  the  set is  finite  then  this number
or   any  monotonic  function  of   this   number   can  be  regarded  as  a
measure of the information produced when  one message  is chosen
from  the  set,   all   choices being equally  likely.   As was  pointed  out
by  Hartley  the  most   natural   choice  is  the  logarithmic  function.
Although  this   definition  must   be  generalized  considerably  when
we   consider   the   influence   of   the   statistics   of   the   message   and
when   we   have   a   continuous   range   of   messages,   we   will   in   all
cases  use  an  essentially  logarithmic  measure.
The   logarithmic   measure   is   more   convenient   for   various
reasons:
1.  It   is   practically   more   useful.   Parameters   of   engineering
importance such  as  time,   bandwidth,   number  of  relays,   etc.,   tend
to  vary linearly with  the  logarithm of the number of possibilities.
For example,   adding one  relay  to  a  group  doubles  the  number of
possible  states  of   the  relays.   It  adds   1  to  the   base   2  logarithm
of this number.   Doubling the time  roughly squares the  number of
possible  messages,   or   doubles   the   logarithm,   etc.
2.   It is nearer to  our intuitive feeling  as to  the proper measure.
This   is   closely   related   to   (l)   since   we   intuitively   measure
entities  by  linear  comparison  with  common  standards.   One  feels,
for   example,   that   two   punched   cards   should   have   twice   the
capacity  of  one  for   information  storage,   and  two  identical   chan-
nels  twice the  capacity  of one  for  transmitting  information.
3.   It   is   mathematically   more   suitable.   Many  of   the   limiting
operations are  simple in terms of the logarithm but would require
clumsy  restatement in  terms  of   the  number of  possibilities.
The   choice  of  a   logarithmic  base   corresponds  to  the   choice  of
a   unit   for   measuring   information.   If   the   base   2   is   used   the
resulting  units  may  be  called binary  digits,   or  more  briefly  bits,   a
word  suggested  by  J.   W.   Tukey.   A  deVIce  with  two  stable  posi-
tions,   such   as   a  relay  or   a  flip-flop  circuit,   can  store  one  bit   of
information   N  such   devices   can   store   N  bits,   since   the   total
number of possible  states  is 2
N
and  log2  2
N
-   N, If the  base  10 is
used  the  units  may  be  called-decirrral   digi ts:   Since
-   3.32 loglo M,
Introduction   33
a  decimal   digit   is   about   3 ~ bits.   A  digit   wheel   on  a   desk  com
puting   machine   has   ten   stable   positions   and   therefore   has   a
storage  capacity  of   one  decimal   digit.   In  analytical   work  where
integration  and  differentiation  are   involved  the  base  e  is   some-
times   useful.   The   resulting   units   of   information   will   be   called
natural   units.   Change  from  the  base  a  to  base  b  merely  requires
multiplication by  log,   a.
By   a   communication   system  we   will   mean   a   system  of   the
type  indicated  schematically  in  Fig.   1. It  consists   of   essentially
five parts:
1.   An information source which  produces a message or sequence
of   messages   to   be  communicated  to   the   receiving  terminal.   The
message   may  be  of   various  types:   (a)   A  sequence  of   letters   as
in  a  telegraph  or   teletype  system;   (b)   A  single  function  of   time
f (t)   as  in  radio  or   telephony;   (c)   A  function  of   time  and  other
variables   as   in   black   and   white   television -   here   the   message
may  be  thought  of  as  a  function f   (x,   Y,   t)   of  two  space  coordi-
nates and time,   the  light intensity at point   (x,   y)   and time t on  a
pickup   tube   plate;   (d)   Two   or   more   functions   of   time,   say
f(t),   g(t),   h(t)  -   this   is   the   case   in  "three-dimensional"  sound
transmission  or   if   the   system  is  intended  to  service  several   indi-
vidual   channels   in   multiplex;   (e)   Several   functions   of   several
variables -   in   color   television   the   message   consists   of   three
functions   f(x,   Y,   t),   g(x,   Y,   t),   h(x,   Y,   t)   defined   in   a   three-
dimensional   continuum -   we may  also  think  of  these three  func-
tions   as   components   of   a   vector   field   defined   in   the   region-
similarly,   several   black  and  white   television  sources  would  pro-
duce   "messages"   consisting   of   a   number   of   functions   of   three
variables;   (f)   Various   combinations   also   occur,   for   example   in
television with  an  associated audio  channel.
2.   A transmitter which operates on the message  in some way to
produce   a   signal   suitable   for   transmiss1ion   over   the   channel.   In
telephony  this  operation  consists  merely  of  changing  sound  pres
sure  into  a  proportional  electrical   current.   In  telegraphy   we have
an  encoding operation  which  produces  a  sequence  of  dots,   dashes
and  spaces   on   the   channel   corresponding   to   the   message.   In   a
multiplex   PCM  system  the   different   speech   functions   must   be
sampled,   compressed,   quantized  and  encoded,   and  finally   inter
34
INFORMATION
The  Mathematical   Theory  of   Communication
SOURCE   TRANSMITTER   RECEIVER   DESTINATION
~
11
~
SIGNAL   Lr-l   RECEIVED
SIGNAL
MESSAGE
  ~
MESSAGE
NOISE
SOURCE
Fig.   1. -   Schematic   diagram  of   a   general   communication   system.
leaved   properly   to   construct   the   signal.   Vocoder   systems,   tele-
vision  and  frequency  modulation  are  other   examples  of   complex
operations  applied  to  the  message  to  obtain the  signal.
3.   The   channel   is   merely   the   medium  used   to   transmit   the
signal   from  transmitter   to  receiver.   It may  be  a  pair  of   wires,   a
coaxial   cable,   a   band  of   radio   frequencies,   a   beam  of   light,   etc.
During  transmission,   or   at   one   of   the   terminals,   the   signal   may
be  perturbed  by  noise.   This   is   indicated  schematically  in  Fig.   1
by  the  noise  source   acting  on   the   transmitted  signal   to   produce
the  received  signal.
4.   The   receiver   ordinarily   performs   the   inverse   operation   of
that   done   by  the   transmitter,   reconstructing   the   message   from
the  signal.
5.   The  destination  is  the  person   (or   thing)   for   whom  the  mes-
sage is intended.
"'TT   •   I   .d   .   I   bl   .   I'
vv e   wIS  I   to  consl   er   certam  genera   pro   ems   lIIVO  vmg   com-
munlcatlOn  systems.   To  do  tIns   It   IS   first   necessary  to   represent
the  various   elements   involved  as   mathematical   entities,   suitably
idealized   from   their   physical   counterparts   We   may   roughly
classify   communication   systems   into   three   main   categories:
discrete,   continuous   and   mixed.   By   a   discrete   system  we   will
mean one  In which  both the message and the SIgnal are a sequence
of   discrete  symbols.   A  typical   case   is  telegraphy  where  the  mes-
sage   is   a   sequence   of   letters   and  the   signal   a   sequence   of   dots,
dashes   and   spaces.   A  continuous   system  is   one   in   which   the
Introduction   35
message   and   signal   are   both   treated   as   continuous   functions,
e.g.,   radio   or   television.   A  mixed   system  is   one   in   which   both
discrete  and  contmuous variables  appear,   e.g.,   PCM  transmission
of   speech.
We   first   consider  the  discrete   case.   This   case   has   applications
not   only   in   communication   theory,   but   also   in   the   theory   of
computing machines, the  design of  telephone  exchanges  and  other
fields.   In  addition  the   discrete   case   forms   a   foundation   for   the
continuous   and  mixed  cases   which   will   be  treated  in  the   second
half of the paper.
-Ir---------
1.   The   Discrete  Noiseless  Channel
Teletype  and  telegraphy  are  two   simple   examples   of   a   discrete
channel  for  transmitting information.   Generally,   a  discrete  chan-
nel   will   mean  a   system  whereby   a   sequence   of   choices   from  a
finite  set of  elementary  symbols  S1   .   .   .   Sn can  be  transmitted
from  one  point to  another.   Each  of the  symbols S,   is  assumed  to
have   a   certain  duration  in  time   i,   seconds   (not   necessarily  the
same  for   different   Si,   for   example  the  dots   and  dashes   in  teleg-
raphy). It is not required that all   possible  sequences of  the  S,   be
capable   of   transmission   on  the   system;   certain  sequences   only
may  be  allowed.   These  will   be  possible   signals   for   the   channel.
Thus  in  telegraphy  suppose the  symbols  are:   (1)   A dot,   consist-
ing of line  closure  for  a unit of time  and then  line open  for a unit
of   time;   (2)   A  dash,   consisting  of   three   time   units   of   closure
and  one   unit   open;   (3)   A  letter   space   consisting  of,   say,   three
units  of line open;   (4)   A word  space of six units of  line open. We
might place  the  restriction  on allowable  sequences that no spaces
follow  each  other   (for if  two  letter  spaces   are  adjacent,   they  are
identical   with   a   word   space).   The   question  we  now  consider   ilil
how  one  can  measure  the  capacity  of  such  a  channel  to  transmit
information.
In the teletype case where all  symbols are  of the  same  duration,
and any sequence of the  32 symbols is allowed,  the  answer is easy.
Each  symbol   represents   five   bits   of   information.   If   the   system
Discrete Noiseless Systems   37
transmits   n   symbols   per   second   it   is   natural   to   say   that   the
channel  has  a  capacity of  5n  bits  per second.   This  does  not mean
that the teletype channel will   always  be transmitting information
at   this  rate -   this  is  the  maximum  possible  rate  and  whether  or
not the  actual   rate  reaches  this  maximum  depends  on  the  source
of  information which  feeds  the  channel,   as  will   appear later.
In  the  more  general   case  with  different   lengths  of  symbols  and
constraints   on   the   allowed   sequences,   we   make   the   following
definition:   The capacity C of  a  discrete  channel  is given  by
C  =   tim  log  N(T)
T ~ r : D T
where  N (T)   is  the  number  of  allowed  signals  of  duration  T.
It   is   easily  seen  that   in   the   teletype   case   this   reduces   to   the
previous   result.   It   can  be  shown  that   the   limit   in  question  will
exist   as   a   finite   number   in   most   cases   of   interest.   Suppose   all
sequences   of   the   symbols   Sl'   .   .   .   ,   S;   are   allowed   and   these
symbols   have   durations   t
l
  ,   •   •   •   ,   t-:   What   is   the   channel
capacity? If N (t)   represents the number of sequences of duration
t  we have
N(t)   = N(t  -   t
l
  )  + N(t  -   t
2
 )  + ... + N(t  -   tn).
The total number is equal to  the sum of  the numbers of sequences
ending  in  Sl,   S2,   •   .   •  , S;   and  these   are  N (t  -   t
l
 ),   N (t   -   t
2
),
...  ,N(t -   t
n
 ) ,  respectively.   According to  a  well-known  result
in  finite   differences,   N (t)   is   the   asymptotic   for   large   t   to  AXJ
where   A  is   constant   and  X
o
  is   the   largest   real   solution  of   the
characteristic  equation:
and  therefore
log  Xl}.
T
log AX
o
Lim c
In  case  there are restrictions on  allowed  sequences we may  still
often  obtain  a  difference  equation  of   this   type  and  find  e  from
the   characteristic   equation.   In   the   telegraphy   case   mentioned
above
2)   + IV(t   4) +  ;.V(t   5)   +  ;.V(t
+N(t  -   8) + N(t  -   10)
7)
38   The  Mathematical   Theory  of   Communication
as   Yv'e   see   by   counting   sequences   of   symbols   according   to   the
last   or   next   to   the   last   symbol   occurring.   Hence   C  is   log  P.o
where   ).to   is   the   positive   root   of   1 =   p.2 + p.4 + p.5 + p.7 + p.8 +
p.lO.   Solving this  we  find  C =  0.539.
A  very   general   type   of   restriction   which   may   be   placed   on
allowed   sequences   is   the   following:   We   imagine   a   number   of
possible.   states   aI,   a
 2,
  .   .   .   ,   am.   For   each   state   only   certain
symbols  from  the  set  8
1
,   •   •   •   ,   S;   can  be  transmitted   (different
subsets   for   the   different   states).   When   one   of   these   has   been
transmitted  the  state  changes   to   a   new  state  depending  both  on
the   old   state   and   the   particular   symbol   transmitted.   The   tele-
graph   case   is   a   simple   example   of   this.   There   are   two   states
depending  on  whether   or   not   a  space  was   the  last  symbol   trans-
mitted. If  so,   then  only  a  dot   or   a  dash  can  be  sent  next  and  the
state  always  changes. If  not,   any  symbol   can  be  transmitted  and
the state changes if  a  space  is sent, otherwise it remains the same.
The   conditions   can   be   indicated   in   a   linear   graph   as   shown   in
Fig.   2.   The  junction  points  correspond  to  the  states  and  the  lines
indicate   the   symbols  possible   in   a  state   and  the   resulting  state.
In   Appendix   1   it   is   shown   that   if   the   conditions   on   allowed
sequences   can  be   described  in   this   form  C  will   exist   and  can  be
calculated  in   accordance  with  the  following  result:
Theorem  1:  Let        be  the  duration  of   the  sth  symbol  which is
allowable  in   state   i   and   leads   to  stage   j.   Then  the   channel   ca-
pacity  C is equal  to  log W  where  W  is  the  largest  real  root   of   the
determinantal   equation:
------------1b             1--=--.0+-------------
8
where      1 if i   j   and is  zero  otherwise.
DASH
WORD   SPACE
Fig.   2. -   Graphical   representation   of   1he   constraints   on   telegraph   symbols.
Discrete  Noiseless  Systems   39
For example,   in the  telegraph  case   (Fig.   2)   the  determinant is:
(W-
2
+ W-')   I
(W-
3
+ W-6)
I   -   1
On  expansion  this  leads  to  the  equation  given   above  for   this   set
of   constraints.
2.   The   Discrete  Source   of   Information
We  have  seen  that   under   very  general   conditions   the   logarithm
of  the  number  of  possible  signals   in  a  discrete  channel   increases
linearly  with  time.   The  capacity  to  transmit   information  can  be
specified  by  giving  this  rate  of   increase,   the  number   of   bits   per
second  required  to  specify  the  particular  signal   used.
We now consider the information source.   How is an  information
source  to  be  described  mathematically,   and  how  much  informa-
tion  in  bits  per   second  is  produced  in  a  given  source?  The  main
point   at   issue   is   the   effect   of   statistical   knowledge   about   the
source   in  reducing  the   required  capacity  of   the   channel,   by   the
use   of   proper   encoding   of   the   information.   In   telegraphy,   for
example,   the   messages   to  be  transmitted  consist   of   sequences   of
letters.   These   sequences,   however,   are   not   completely   random.
In general,  they  form  sentences  and  have  the  statistical  structure
of,   say,   English.   The   letter   E   occurs   more   frequently   than   Q,
the  sequence TH  more  frequently  than  XP,   etc.   The  existence  of
this   structure   allows   one  to  make   a   saving  in  time   (or   channel
capacity)   by  properly  encoding the  message sequences into  signal
sequences.   This  is  already  done  to  a  limited  extent   in  telegraphy
by  using the  shortest channel symbol, a dot,   for the most  common
English  letter  E;   while  the  infrequent   letters,   Q,   X,   Z  are  repre-
sented   by   longer   sequences   of   dots   and   dashes.   This   idea   is
carried  still   further   in  certain  commercial   codes   where   common
words   and   phrases   are   represented   by   four-   or   five-letter   code
groups   with   a   considerable   saving  in   average   time.   The   stand-
ardized   greeting   and   anniversary   telegrams   now  in   use   extend
this  to  the  point   of   encoding  a  sentence  or   two  into  a  relatively
short  sequence  of  numbers.
We  can  think  of   a  discrete  source   as   generating  the  message,
40
  The  Mathematical   Theory  of  Communication
symbol by  symbol. It will  choose successive  symbols  according to
certain  probabilities  depending,   in  general,   on  preceding  choices
as  well  as  the  particular symbols  in  question.   A physical  system,
or   a   mathematical   model   of   a   system  which   produces   such   a
sequence  of  symbols  governed  by  a  set   of  probabilities,   is known
as a stochastic process." We may consider a discrete source, there-
fore,   to  be  represented  by  a  stochastic  process.   Conversely,   any
stochastic process  which  produces  a  discrete  sequence  of  symbols
chosen   from  a   finite   set   may   be   considered   a   discrete   source.
This  will   include such  cases  as:
1.   Natural written  languages such  as  English, German,   Chinese.
2.   Continuous  information  sources  that  have  been  rendered  dis-
crete  by  some  quantizing  process.   For  example,   the  quantized
speech   from  a   PCM  transmitter,   or   a   quantized   television
signal.
3.   Mathematical   cases   where   we   merely   define   abstractly   a
stochastic process which  generates  a  sequence of  symbols.   The
following  are  examples  of  this   last   type  of   source.
(A)   Suppose we have five letters A, B, C, D, E  which  are  chosen
each  with  probability  .2, successive  choices  being  independ-
ent.   This  would   lead  to  a   sequence  of   which  the   following
is a typical example.
BDCBCECCCADCBDDAAECEEAABBD
A E  E  C ACE E  B A E  E  C B C E  A D.
This   was   constructed   with   the   use   of   a   table   of   random
numbers.'
(B)   Using  the  same  five  letters  let the  probabilities  be   4,   1,   2,
.2,   .1,   respectively,   with  successive   choices   independent.   J . A ~
typical message from  this source is  then:
AAACDCBDCEAADADACEDAEADCA
BED ADD C E C A A A A  A D.
(C)   A   more   complIcated   structure   is   obtamed   If   successive
symbols are  not chosen independently but their probabilities
8 See,   for   example,   S.   Chandrasekhar,   "Stochastic   Problems   in   Physics  and
Astronomy,"   Reviews   of   Modern  Physics,   v.   15, No.1,   January  1943, p.   1.
• Kendall   and   SmIth,   Tables   oj   Random  Samplmg   Numbers,   CambrIdge,
1939.
Discrete Noiseless  Systems   41
depend on preceding letters. In the simplest case of this type
a   choice  depends   only   on  the   preceding  letter   and   not   on
ones   before   that.   The  statistical   structure  can  then   be  de-
scribed  by  a  set  of   transition  probabilities   Pi (j),   the  prob-
ability that letter i   is  followed  by  letter  j.   The  indices i   and
j   range  over   all   the  possible  symbols.   A  second   equivalent
way   of   specifying   the   structure   is   to   give   the   "digram"
probabilities   P(i,j) ,   i.e.,   the   relative   frequency   of   the   di-
gram  i   j.   The   letter   frequencies   p(i),   (the   probability   of
letter   i),   the   transition  probabilities   Pi (j)   and  the   digram
probabilities  p (i,i)   are  related  by  the  following  formulas:
p(i)   =  ~ p ( i , j ) =  ~ p ( i , i)   =  ~ p ( i ) P i ( i )
1   1   1
p(i, j)   =  P(i)pi(j)
~ P i ( i ) =  ~ p ( i ) =  ~ p ( i , j ) =  1.
1   a   '.1
As a specific example suppose there are three  letters A, B,
C with  the  probability tabies:
Pi(i)
  J
  1-   p(i)   'P(i, j)
  J
A   B   C   A   B   C
A   0
  4   1
A
  9
A   0
  4   1
-   -   -
5   5   27   15   15
B
  1   1
0   B
  16
B
  8   8
0 1-
2   2   27
  1-
27   27
C
  1   2   1
C
  2
C
  1   4   1
-   --
2   5   10   27   27   135   135
A typical   message   from  this  source  is  the  following:
ABBABABABABABABBBABBBBBABA
BABABABBBACACABBABBBBABBAB
AC B  B B ABA.
The   next   increase   in   complexity   would   involve   trigram
frequencies   but   no  more.   The   choice  of   a   letter   would   de-
pend   on  the   preceding  two   letters   but   not   on  the   message
before  that  point.   A  set   of  trigram  frequencies   p (i,   j,   k)   or
equivalently  a   set   of   transition   probabilities   p"  (k)   would
be required.   Continuing in  this  way  one obtains successively
more complicated stochastic processes. In the general n-gram
42   The  Mathematical   Theory  of   Communication
case   a  set   of   n-gram  probabilities   p (i
l
 ,   i
2
 ,   •   •   •   ,   in)   or   of
transition  probabilities   Pi
l
 ,   i
2
 ,   •   •   •   ,   i
 n
 -
l
 (in)   is   required  to
specIfy  the  statistical   structure.
(D)   Stochastic   processes   can   also   be   defined   which   produce   a
text   consisting  of   a  sequence  of   "words."  Suppose  there  are
five   letters   A,   B,   C,   D,   E  and  16  "words"   in  the   language
with  associated  probabilities:
.10 A   .16 BEBE   .11 CABED   .04 DEB
.04 ADEB   .04 BED   .05 CEED   .15 DEED
.05 ADEE   .02 BEED   .08 DAB   .01 EAB
,
.01 BADD   .05 CA   .04 DAD   .05 EE
Suppose   successive   "words"   are   chosen   independently   and
are  separated  by  a  space.   A  typical   message  might  be:
DAB  EE  A  BEBE  DEED  DEB  ADEE  ADEE  EE  DEB
BEBE  BEBE  BEBE  ADEE  BED  DEED  DEED  CEED
ADEE A DEED DEED BEBE CABED BEBE BED DAB
DEEDADEB.
If all the  words are  of  finite  length this  process  is equivalent
to   one   of   the   preceding   type,   but   the   description   may   be
simpler in  terms  of  the  word  structure  and  probabilities.   We
may  also  generalize  here  and  introduce  transition  probabil-
ities  between  words,   etc.
These   artificial   languages   are   useful   in   constructing   simple
problems  and  examples  to  illustrate  various   possibilities.   We  can
also  approximate  to   a  natural   language  by  means   of   a   series   of
simple  artificial   languages.   The  zero-order   approximation  is   ob-
tained  by  choosing  all   letters  with  the  same  probability  and  in-
dependently.   The  first-order   approximation  is  obtained  by  choos-
ing   successive   letters   independently   but   each   letter   having   the
same   probability  that   it   has   in   the  natural   language."   Thus,   in
the  first-order   approximation  to  English,   E  is   chosen  with  prob-
ability  .12  (its  frequency  in  normal   English)   and  W  with  proba-
bility   .02,   but  there  is  no  influence  between  adjacent   letters   and
no  tendency  to  form  the  preferred  digrams  such  as  TH,   ED,   etc.
5 Letter,   digram  and  trigram  frequencies  are   given  in  Secret   and  Urgent   by
Fletcher   Pratt,   Blue   RIbbon   Books,   1939.   ~ y V O I d frequencies   arc   tabulated
in  Relative  Frequency   oj   English  Speech  Sounds,   G.   Dewey,   Harvard  Uni-
versity  Press,   1923.
Discrete  Noiseless  Systems
  43
In   the   second-order   approximation,   digram  structure   is   intro-
duced.   After   a  letter  is  chosen,   the  next   one  is  chosen  in  accord-
ance  with  the   frequencies   with   which  the   various   letters   follow
the  first   one.   This   requires   a   table   of   digram  frequencies   Pi (j) .
In the third-order approximation, trigram structure is introduced.
Each  letter  is  chosen  with  probabilities  which  depend  on  the  pre-
ceding two  letters.
3.   The  Series of   Approximations  to  English
To  give  a  visual   idea  of how  this  series  of  processes  approaches  a
language,   typical   sequences   in   the   approximations   to   English
have  been  constructed  and  are  given  below.   In  all   cases  we  have
assumed a  27-symbol "alphabet," the 26 letters and a  space.
1.   Zero-order   approximation   (symbols   independent   and   equi-
probable) .
XFOML  RXKHRJFFJUJ   ZLPWCFWKCYJ   FFJEYV-
KCQSGHYD  QPAAMKBZAACIBZLHJQD.
2.   First-order   approximation   (symbols   independent   but   with
frequencies  of English  text).
OCRO  HLI   RGWR  NMIELWIS  EU  LL  NBNESEBYA
TH EEl ALHENHTTPA  OOBTTVA  NAH  BRL.
3.   Second-order   approximation   (digram  structure   as   in   Eng-
lish) .
ON  IE  ANTSOUTINYS  ARE  T   INCTORE  ST  BE  S
DEAMY ACHIN D  ILONASIVE TUCOOWE AT TEA-
SONARE FUSO TIZIN ANDY TOBE SE.A.CE CTISBE.
4.   Third-order approximation  (trigram structure as  in English).
IN  NO  1ST  LAT  WHEY  CRATICT  FROURE  BIBS
GROCID   PONDENOME   OF   DEIVIONSTURES   OF
THE  REPTAOIN  IS  REOOACTIONA  OF  ORE.
5.   First-order  word  approximation.   Rather  than  continue  with
tetragram, ...  , n-gram  structure it  is  easier and  better  to
jump   at   this   point   to   word   units.   IIere   words   are   chosen
independently  but  with  their  appropriate  frequencies.
REPRESENTING AND SPEEDILY IS AN GOOD APT
OR  COME  CAN  DIFFERENT  NATURAL  HERE  HE
44
  The  Mathematical   Theory  of   Communication
THE  A  IN  CAME  THE  TO  OF  TO  EXPERT  GRAY
COME  TO  FURNISHES THE LINE  MESSAGE  HAD
BE THESE.
6.   Second-order word approximation. The word transition prob-
abilities are correct but no further structure is included.
THE  HEAD  AND  IN  FRONTAL  ATTACK  ON  AN
ENGLISH  WRITER  THAT  THE  CHARACTER  OF
THIS POINT IS  THEREFORE ANOTHER  METHOD
FOR  THE  LETTERS  THAT  THE  TIME   OF   WHO
EVER   TOLfr   THE   PROBLEM   FOR   AN   UNEX-
PECTED.
The  resemblance  to  ordinary  English  text   increases   quite  no-
ticeably at each  of the above  steps.   Note that these  samples have
reasonably  good  structure  out   to  about   twice   the   range  that   is
taken  into  account in  their  construction.   Thus  in   (3)   the   statis-
tical  process  insures reasonable  text   for  two-letter  sequences,   but
four-letter   sequences   from  the  sample  can  usually  be  fitted   into
good sentences.   In  (6)   sequences  of four or more words can easily
be  placed  in  sentences  without  unusual   or  strained  constructions.
The particular sequence of ten  words "attack on an English writer
that the  character of  this"  is  not  at   all   unreasonable.   It  appears
then   that   a   sufficiently   complex  stochastic   process   will   give   a
satisfactory representation of a discrete source.
The  first two  samples were constructed by  the use of a  book of
random  numbers  in  conjunction  with   (for   example  2)   a  table  of
letter   frequencies.   This   method  might   have   been  continued   for
(3),   (4)   and   (5),   smce   dIgram,   trigram  and   word   frequency
tables   are  available,   but   a  simpler  equivalent  method  was  used.
To  construct   (3)   for   example,   one  opens   a  book  at  random  and
selects   a   letter   at   random  on  the   page.   This   letter   is   recorded.
The book is then opened  to  another page  and  one reads  until  this
letter   IS   encountered.   The   succeedmg   letter   IS   then   recorded.
Turning to  another page  this second letter is searched for and  the
succeeding letter recorded,   etc.   A similar process  was  use  for   (4) ,
(5)   and   (6).   It   would   be   interesting  if   further   approximations
could  be  constructed,   but   the   labor   involved   becomes  enormous
at the next stage.
Discrete Noiseless  Systems
4.   Graphical   Representation  of  a  Markoff  Process
Stochastic processes  of the type described above are known math-
ematically  as   discrete   Markoff   processes   and   have   been   exten-
sively studied in the literature." The general case can be described
as  follows:   There  exist   a  finite   number  of   possible  "states"  of   a
system;   8
11
  S2,   .   .   .   , Sn.   In  addition  there  is  a  set  of  transition
probabilities,   Pi (j),   the  probability that  if  the  system  is  in  state
S, it will next go to state Sj. To make this Markoff process into an
information source we need only  assume that a  letter is produced
for each transition from one state to  another.   The states will  cor-
respond  to the "residue of influence"  from  preceding letters.
The situation  can  be  represented  graphically  as  shown  in  Figs.
3, 4 and 5. The "states"  are  the  junction  points  in  the  graph  and
the   probabilities   and  letters   produced  for   a  transition  are   given
beside  the   corresponding  line.   Figure  3  is   for   the  example  B  in
Section  2,   while  Fig.   4  corresponds   to  the  example  C.   In  Fig.   3
there is only  one state since successive  letters are independent. In
Fig,   4  there  are   as  many  states  as   letters. If  a  trigram  example
were  constructed  there  would  be  at  most n
2
states  corresponding
to   the   possible   pairs   of   letters   preceding  the  one   being   chosen.
Figure 5 is  a  graph  for  the  case  of  word  structure  in  example  D.
Here S corresponds to  the  "space" symbol.
s.   Ergodic  and  Mixed  Sources
As we have  indicated above a discrete source  for our purposes can
be considered  to  be represented  by  a  Markoff process.   Among  the
possible  discrete  Markoff  processes   there   is  a  group   with  special
properties  of   significance  in  communication  theory.   This   special
class   consists   of   the   "ergodic"   processes   and   we   shall   call   the
corresponding  sources   ergodic  sources.   Although  a   rigorous   defi-
nition  of   an   ergodic   process   is   somewhat   involved,   the   general
idea  is simple.   In  an  ergodic  process  every  sequence  produced  by
the   process   is  the   sallIe  in  statistical   properties.   Thus   the   letter
frequencies,   digram  frequencies,   etc"   obtained   from  particular
sequences, will, as the  lengths of the sequences increase, approach
6 For a detaIled treatment see  M.   Frechet,   Methods  des  fonctwns              
Theorie   des  enenements  en  chaine  dans  le  cas d'un  nombre  fini   d'etats  pos-
        Paris,   Gauthiel   Villars,   1938.
46   The  Mathematical   Theory  of   Communication
definite   limits   independent   of   the   particular   sequence.   Actually
this  is not true  of  every sequence  but  the  set for  which  it   is  false
has   probability   zero.   Roughly   the   ergodic   property  means   sta-
tistical  homogeneity.
E
.1
.2
C
Fig.   3. -   A  graph   corresponding  to   the   source   in   example  B.
B
.5
Fig.   4. -   A  graph   corresponding   to   the   source   in   example   C.
Fig.   5. -   A  graph  corresponding  to   the   source   in  example   D.
Discrete Noiseless  Systems   47
All  the examples of  artificial  languages given above are ergodic.
This   property   is   related   to   the   structure   of   the   corresponding
graph. If  the  graph  has   the   following  two  properbes
7
the   corre-
sponding process will   be  ergodic:
1.   The  graph  does   not   consist   of   two   isolated  parts   A  and  B
such that it is  impossible  to  go  from  junction  points  in  part
A to  junction points in part B  along  lines of the graph  in  the
direction  of  arrows  and  also  impossible  to  go  from  junctions
in  part B  to  junctions in part A.
2.   A  closed  series   of   lines   in  the  graph  with  all   arrows   on  the
lines  pointing  in  the  same  orientation  will   be   called  a   "cir-
cuit."   The  "length" of   a  circuit  is  the  number  of   lines  in  it.
Thus   in  Fig.   5  series   BEBES  is   a   circuit   of   length  5.   The
second property required is that the  greatest common divisor
of  the  lengths of  all   circuits  in  the  graph  be  one.
If the  first   condition  is satisfied but the  second  one  violated  by
having the greatest common divisor equal  to  d >  1, the sequences
have  a  certain  type  of   periodic  structure.   The  various   sequences
fall   into  d  different classes  which  are  statistically  the  same  apart
from  a   shift   of   the   origin   (i.e.,   which   letter   in   the   sequence   is
called  letter   1).   By  a  shift   of   from  0  up   to   d  -   1  any  sequence
can  be  made  statistically  equivalent   to   any  other.   A  simple  ex-
ample with d =  2 is the  following:   There are three possible letters
a,   b, c.   Letter  a  is   followed  with  either   b or   c  with  probabilities
!   and i   respectively.   Either  b or c is  always  followed  by  letter a.
Thus a typical  sequence is
a  b a cae a cab  a cab  a  b a cae.
ThIS type  of situation  IS  not of  much  Importance  for   our work.
If   the   first   condition  is   violated  the   graph  may  be   separated
into  a  set of  subgraphs  each  of   which  satisfies the  first   condition.
Vie   will   assume   that   the   second   condition   is   also   satisfied   for
each   subgraph.   We   have   in   this   case   what   may   be   called   a
"mixed"  source  made   up   of   a   number   of   pure   components.   The
components   correspond   to   the  various   subgraphs.   If  L
  1
 ,   L
2
 ,   L
3
 ,
.   .   .   are  the  component sources  we  may  write
1 These   are   restatements   in   terms   of   the   graph   of   conditions   gIven   III
Frechet.
48   The  Mathematical   Theory  of  Communication
where Pi   is the probability of the component source  L •.
Physically  the  situation  represented  is  this:   There  are  several
different sources L
1
 ,   L
2
,   L
3
,   •   •   •   which  are each  of  homogeneous
statistical   structure   (i.e.,   they   are   ergodic).   We   do   not   know
a  priori   which   is   to   be  used,   but   once  the  sequence  starts   in  a
given  pure  component   Li,   it   continues   indefinitely  according  to
the statistical structure of that component.
As an example one may take two of the  processes defined  above
and   assume   P1 =  .2   and   P2 =  .8.   A  sequence   from  the   mixed
source
L  =  .2L
1
 + .8L
2
would  be obtained by  choosing  first L
 1
 or L
 2
 with  probabilities  .2
and  .8  and  after   this   choice  generating  a  sequence   from  which-
ever was  chosen.
Except when  the contrary is stated we shall  assume a source  to
be   ergodic.   This   assumption   enables   one   to   identify   averages
along  a  sequence with  averages  over the  ensemble  of  possible  se-
quences   (the   probability  of   a   discrepancy   being   zero).   For   ex-
ample  the  relative  frequency  of  the  letter  A  in  a  particular  infi-
nite  sequence  will   be,   with  probability  one,   equal   to  its   relative
frequency  in  the ensemble of  sequences.
If Pi   is the probability of state i   and  Pi (j)   the transition prob-
ability  to  state  j,   then  for  the  process  to  be  stationary  it is  clear
that the Pi   must satisfy equilibrium  conditions:
Pi   =   ~ P i P i ( j ) .
i
In the ergodic  case it can  be shown that with  any  starting condi-
tions  the  probabilities r, (N)   of  being  in  state  j   after N  symbols,
approach  the equiUbrium values  as N  ~ 00.
6.   Choice,   Uncertainty  and  Entropy
We  have  represented  a  discrete  information  source  as  a  Markoff
process.   Can  we  define   a  quantity  which  will   measure,   in  some
sense,   how much  information  is "produced"  by  such  a  process, or
better, at what rate  information  is produced?
Discrete Noiseless  Systems
Suppose  we have  a set of  possible  events whose probabilities of
occurrence are PI'  P2, ..•  ,pn. These probabilities are known but
that   is  all   we  know  concernmg  which   event   wIll   occur.   Can  we
find a  measure  of  how much  "choice"  is  involved  in  the  selection
of the  event or of how uncertain  we are  of the  outcome?
If there is such  a measure, say H (PI,   P2, •.•  ,Pn), it is reason-
able to  require of it the  following  properties:
1.   H  should  be  continuous  in  the  Pi.
2. If  all   the  Pi   are   equal,   Pi =    '  then  H  should  be  a  mono-
tonic   increasing  function   of   n.   With   equally   likely   events
there   is  more   choice,   or   uncertainty,   when   there   are   more
possible  events.
3. If a  choice be  broken  down  into  two  successive  choices,   the
original   H   should   be   the   weighted   sum  of   the   individual
values of  H.   The  meaning  of this  is  illustrated  in  Fig.   6. At
the   left   we  have  three   possibilities   PI =!,   P2 = i,   P3 = 1.
On  the   right   we  first   choose  between  two  possibilities  each
with  probability  j ,   and  if   the   second  occurs   make   another
choice   with   probabilities   j,   i.   The   final   results   have   the
same  probabilities as  before. We  require, in this special case,
that
H(!, i, 1)   =   H(!, !) + !   H(j, i)·
The  coefficient!   is  the   weighting  factor   introduced  because  this
second  choice only  occurs  half  the  time.
Fig.   6. -   Decomposition   of   a   choice   from  three   possibilities.
In Appendix 2,   the  following  result  is  established:
Theorem 2:   The only  H  satisfY2:ng  the  three above                
is of the form:
50   The   Mathematical   Theory  of   Communication
n
where  K  is a  positive  constant.
This theorem,  and the assumptions required  for its proof, are in
no   way   necessary   for   the   present   theory.   It   is   given   chiefly   to
lend  a   certain  plausibility  to   some   of   our   later   definitions.   The
real   justification of  these  definitions,   however,   will   reside  in  their
implications.
Quantities: of   the   form   H = -   ~ Pi   log   Pi   (the   constant   K
merely  amounts  to  a  choice  of  a  unit  of  measure)   play  a  central
role  in  information theory  as  measures  of information,   choice  and
uncertainty.   The  form  of  H  will   be  recognized  as  that  of entropy
1.0
/
  ~
..........
r-,
.9
.8
  /   1'\
V
  \
.7
/
  1\
H   .6
BITS
/   \
.5
.4
  I   \
/   \
.3
/   \
.2
I   \
•
  I   \
., I
\
0  '
  !   I
  ,
  I
  ,   ,
  I   .   .   .
o   .1   .2   .3   .4   .5   .6   .7   .8   .9   1.0
p
Fig.   7.   Entropy   in   the   case   of   two   possibilities   with   probabilities   p   and  (1   pl.
as  defined  in  certain  formulations  of  statistical  mechanics"  where
Pi is the probability of  a  system  being in  cell i   of  its  phase space.
8 See,   for   example,   R.   C   Tolman,   Prindples   oj   Statistical   Mechanics,   Ox-
ford,   Clarendon,   1938.
Discrete Noiseless  Systems   51
H  is  then,   for   example,   the  H  in  Boltzmann's  famous  H  theorem.
We  shall   call   H  -    Pi   log  Pi  the  entropy  of   the  set of   proba-
bilities PI, ...  ,Pn. If x  is  a  chance  varIable  we  wIll   wflte  H (x)
for   its   entropy;   thus   x   is   not   an   argument   of   a   function  but   a
label   for   a  number,  to  differentiate it  from H(y)   say, the  entropy
of  the  chance variable  y.
The  entropy  in  the  case  of  two  possibilities with  probabilities  P
and  q =  1 -   P,  namely
H = -   (p  log P + q  log q)
is plotted in  Fig.   7 as  a  function  of  p.
The  quantity  H  has   a   number   of   interesting  properties   which
further   substantiate   it   as   a   reasonable   measure   of   choice   or
information.
1.   H  =   0 if   and  only   if   all   the   Pi   but   one   are   zero,   this   one
having the value unity. Thus only  when  we are certain of the out-
come  does  H  vanish.   Otherwise  H  is  positive.
2.   For  a  given  n,   H  is  a  maximum  and  equal  to  log  n  when  all
the Pi are equal, i.e.,     This is  also  intuitively the most uncertain
situation.
3.   Suppose  there  are  two   events,   x   and  y,   in  question,   with  m
possibilities  for   the  first   and  n  for   the  second.   Let   P(i, j)   be  the
probability  of   the  joint occurrence  of  i   for   the  first   and  j   for  the
second. The entropy of  the joint event is
H (x,  y)   -   -    p(i, j)  log  p(i, j)
  1
while
HEx)         j)            j)
Hey)   Ep(i,j)  logEp(i,j).
It is  easily  shown  that
H(x,   y)   <  H(x) + H(y)
with  equality  only  if   the  events   are   independent   (i.e.,   p(i, j)   =
p (£)   p (j) )   The uncertainty  of   a  joint event is  less than  or  equal
to  the sum of the  individual   uncertainties.
4.   Any  change  toward  equalization  of   the   probabilities  PI'   P2'
The  Mathematical   Theory  of   Communication
...  ,Pn increases H. Thus if Pl <  P2 and we increase PI'   deereas-
ing  P2  an  equal   amount  so that Pl   and  P2  are  more  nearly  equal,
then H  increases. More  generally,   if  we  perform  any  "averaging"
operation on the Pi of  the form
  =      Pi
1
where   aii       aii   =  1, and all aii >0, then H  increases (except
\   1
in  the  special   case   where   this   transformation   amounts   to   no
more than  a  permutation  of   the  Pi with  H  of   course  remaining
the  same).   .'
5.   Suppose   there   are  two  chance   events   x   and   y   as   in   3,   not
necessarily   independent,   For   any   particular   value   i   that   x   can
assume   there   is   a   conditional   probability   Pi (j)   that   y   has   the
value j. This is given by
(
 .)  _   p(i, j)
Pi J   -
      
1
We  define  the  conditional   entropy  of   y,   H:t(Y)   as  the  average  of
the  entropy  of   y   for   each  value  of   x,   weighted  according  to  the
probability  of   getting that particular x. That is
11:r:(Y)   =  -          log  Pi(j).
\.1
This quantity  measures how uncertain we are of   y  on  the average
when  we  know x.   Substituting the  value  of  Pi (j)   we  obtain
H:r:(Y)    p(i, j)  log  p(i,j) + p(i, j)  log   p(i, j)
= H(x,  y)   -   H(x)
or
H(x, y)   H(x) + H:t(Y).
The  uncertainty   (or entropy)   of  the  joint event x, y  is  the  uncer
tainty  of  x  plus   the  uncertainty  of  Y  when  x  is  known.
6   From  3  and  5  we  ha.ve
H(x) + H(y)   > H(x,y)   -   H(x) + H:t(Y).
Hence
H(y) >  HI/J(Y) ,
Discrete Noiseless  Systems   53
The uncertainty of  y  is never increased by knowledge of x. It will
be decreased unless x and yare independent events,  in  which  case
it is not changed.
7.   The  Entropy  of   an  Information  Source
Consider   a   discrete   source   of   the   finite   state   type   considered
above.   For each  possible state i   there will  be a set of  probabilities
Pi (j)   of   producing  the  various   possible  symbols   j.   Thus  there  is
an  entropy  Hi   for   each  state.   The  entropy  of   the  source  will   be
defined  as   the  average  of   these  Hi   weighted  in  accordance  with
the probability of occurrence of  the states in  question:
H  = LPiHi
i
=   -   ~ P i P i ( j ) log  Pi(j).
'.'
This   is   the   entropy   of   the   source   per   symbol   of   text.   If   the
Markoff process is  proceeding at  a  definite time  rate there  is  also
an  entropy per second.
were   Ii   is   the   average   frequency   (occurrences   per   second)   of
state i. Clearly
H'=mH
where  m  is  the  average  number  of  symbols  produced  per  second.
H  or   H'   measures   the   amount   of   information  generated  by  the
source   per   symbol   or   per   second.   If   the   logarithmic   base   is   2,
they  will  represent bits  per symbol or  per second.
If   successive   symbols   are   independent   then   H   is   simply
~ Pi log Pi where  Pi   is  the  probability  of   symbol i.   Suppose  in
this case we consider a long message of N  symbols. It will   contain
with  high  probability  about   P1N occurrences  of  the  first   symbol,
P2N occurrences  of  the  second,   etc.   Hence  the  probability  of  this
particular message  will   be  roughly
or
log  P  ~ N ~ P i log  Pi
•
54
  The  Mathematical   Theory  of  Communication
log  p   .
H
  log lip
N
H  is   thus   approximately   the   logarithm  of   the   reciprocal   prob-
ability   of   a   typical   long   sequence   divided   by   the   number   of
symbols   in  the   sequence.   The  same   result   holds   for   any  source.
Stated  more  precisely  we  have   (see  Appendix  3) :
Theorem  3:   Given  any   £ > 0  and  B> 0,   we   can   find   an   No
such   that   the   sequences   of   any   length   N > No  fall   into   two
classes:
1.   A  set whose  total probability is less  than  e.
2.   The  remainder,   all   of whose  members  have  probabilities  satis-
fying   the  inequality
log  p-l
N   -   H   <0.
log p-l
In  other words we  are  almost  certain to  have   N   very  close  to
H  when  N  is  large.
A  closely  related  result   deals  with  the  number   of   sequences  of
various   probabilities.   Consider   again  the   sequences   of   length  N
and  let  them  be  arranged  in  order  of   decreasing  probability.   We
define  n(q)   to  be  the  number  we  must take  from  this  set starting
with  the  most   probable  one  in  order   to  accumulate  a  total   prob-
ability  q  for   those taken.
Theorem  4:
L·   log  n(q)   H
when q  does  not  equal 0 or  1.
We  IIlay  interpret   log  n(q)   as   the   nUIIlber  of   bits   rcquircd   lo
specify   the   sequence   when   we   conSIder   only   the   most   probable
of bits per symbol  for  the specification. The theorem says that for
large  IV  this  will  be  independent of  q  and  equal   to H.  The  rale  of
growth   of   the   logarIthm  of   the   number   of   reasonably   probable
sequences   is   given   by   H,   regardless   of   our   interpretation   of
Discrete  Noiseless  Systems   55
"reasonably  probable."  Due  to  these  results,   which  are  proved  in
Appendix   3,   it   is   possible   for   most   purposes   to   treat   the   long
sequences   as   though   there   were   Just   2
R N
of   them,   each   wIth   a
probability  2-
H N
•
The  next  two  theorems  show  that Hand H'   can  be  determined
by  limiting  operations  directly  from  the  statistics  of   the  message
sequences,   without   reference   to   the   states   and   transition   prob-
abilities  between states.
Theorem  5:   Let   p(Bd   be   the  probability  of  a  sequence  B,   of
symbols  from  the  source. Let
GN   =   -   ~ ~ p(Bi )   log  p(Bi )
where   the   sum  is   over   all   sequences   B,   containing   N   symbols.
Then  G
N
  is  a  monotonic  decreasing  function  of Nand
Lim  G
N
  =  H.
N-+oo
Theorem  6:   Let   p(B
i
 ,   Sj)   be   the   probability   of   sequence s,
followed   by   symbol   S,   and   PB;(Sj)  = p(B
i
 ,   Sj)/p(Bd   be   the
conditional   probability  of  S,   after  B i.   Lei-
F N   =   -   ~ p ( B i , Sj)   log  PBi   (Sj)
t,1
where  the sum is  over all blocks  B i   of N  -   1 symbols and  over all
symbols  Sj.   Then  F
N
 is  a  monotonic  decreasing  function  of N,
F
N
  =   NG
N
  -   (N  -   1)  G
N
 -
  I
 ,
1   N
G.r.l   =   N   ~ F N ,
and  Lim  F
N
  =  H.
l'l   tee
These   results   are   derived   in   AppendIx   3.   They   show  that   a
series   of   approximations   to   H   can   be   obtained   by   considering
only  the  statistical  structure of  the  sequences  extending over  1, 2,
.   .   .   ,   J.V  symbols.   F
N
  is  the  better  approximation.   In  fact   F
N
  is
the  entropy  of   the  Nth  order   approximation  to  the  source  of   the
type  dIscussed  above. If there  are no  statIstIcal  influences  extend-
ing   over   more   than  N  symbols,   that   is   if   the   conditional   prob-
58   The  Mathematical   Theory  of   Communication
ability   of   the   next   8ymbol   knowing   the   preceding   (J.V   1)   i8
not   changed  by  a   knowledge   of   any   before  that,   then  P
N
  -   H.
FN  of   course  is  the  conditional   entropy  of  the  next   symbol   when
the   (N -   1)   preceding  ones  are  known,   while  G
N
  is  the  entropy
per symbol of blocks of N  symbols.
The  ratio  of  the  entropy  of   a  source  to  the   maximum  value  it
could   have   while   still   restricted   to   the   same   symbols   will   be
called its relative  entropy. This,   as will  appear later, is the  maxi-
mum  compression  possible  when  we  encode  into  the  same  alpha-
bet.   One   minus   the   relative   entropy   is   the   redundancy.   The
redundancy of ordinary English, not considering statistical struc-
ture   over   greater   distances   than  about   eight   letters,   is   roughly
50%.   This   means  that   when  we  write  English  half   of   what   we
write  is  determined  by  the  structure  of  the  language  and  half   is
chosen  freely.   The  figure  50%  was  found  by  several   independent
methods   which   all   gave   results   in  this   neighborhood.   One  is  by
calculation  of   the   entropy  of   the   approximations  to   English.   A
second method is to  delete a  certain fraction  of the  letters  from  a
sample  of   English  text   and  then  let   someone  attempt   to   restore
them.   If  they  can  be  restored  when  50%  are  deleted  the  redun-
dancy  must   be   greater   than  50%.   A  third   method   depends   on
certain  known  results  in  cryptography.
Two  extremes  of   redundancy  in  English  prose  are  represented
by  Basic  English  and  by   James   Joyce's   book  Finnegans   Wake.
The  Basic  English  vocabulary  is   limited  to   850  words   and  the
redundancy  is  very  high.   This  is  reflected  in  the  expansion  that
occurs   when   a   passage   is   translated   into   Basic   English.   Joyce
on   the   other   hand   enlarges   the   vocabulary   and   IS   alleged   to
achieve  a  compression  of  semantic  content.
The   redundancy   of   a   language   is   related   to  the   existence   of
cro8sword   puzzles.   If   the   redundancy   is   zero   any   sequence   of
letters  is  a  reasonable  text   in  the  language  and  any   two-dimen-
slOnal   array  of   letters   forms   a   crossword  puzzle.  If   the   redun-
dancy  is too  high  the  language  imposes  too  many  constraints  for
large  crossword  puzzles  to  be  possible   A  more  detailed  analysis
ShOW8 that if we assume  the  constraints  imposed by  the  language
are of a rather chaotic and random nature, large  crossword  puzzles
are just possible when  the  redundancy  is 50%. If the redundancy
Discrete Noiseless  Systems   57
is  33%,   three-dimensional   crossword   puzzles  should  be  possible,
etc.
8.   Representation  of   the   Encoding  and  Decoding  Operations
We   have   yet   to   represent   mathematically   the   operations   per-
formed  by  the  transmitter and  receiver in  encoding  and  decoding
the  information.   Either   of   these  will   be  called  a  discrete  trans-
ducer.   The  input   to  the  transducer   is  a  sequence  of   input   sym-
boIs and  its  output a  sequence of output symbols. The transducer
may  have   an  internal   memory   so  that   its   output   depends   not
only   on  the  present   input   symbol   but   also   on  the  past   history.
We  assume  that  the  internal   memory  is  finite,   i.e.,   there  exist   a
finite  number  m  of  possible  states  of   the  transducer  and  that  its
output   is   a   function  of   the  present   state  and  the   present   input
symbol.   The  next   state  will   be   a   second   function   of   these   two
quantities.   Thus  a  transducer  can  be  described  by  two  functions:
u; = f(xn, an)
an+1 =  g(Xn, an)
where:
X
n
 is the  nth input symbol.
an   is  the  state  of  the  transducer  when  the   nth  input   symbol   is
introduced,
u;  is  the  output  symbol   (or   sequence  of  output  symbols)   pro-
duced  when  X
n
 is introduced if the state is  an.
If the  output symbols  of  one  transducer   can  be  identified  with
the  input  symbols  of  a  second,   they  can  be  connected  in  tandem
and   the   result   is   also   a   transducer.   If   there   exists   a   second
transducer  which  operates  on the  output of the first  and  recovers
the  original  input, the  first   transducer will   be  called non-singular
and  the  second  will   be  called  its   inverse.
Theorem 7:   The  output of a finite  state  transducer driven  by a
finite   state   statistical   source   is   a   finite   .state  .statistical   source,
with   entropy   (per   unit   time)   less   than  or   equal   to   that   of   the
input. If  the  transducer is non-singular they  at e equal.
Let   a   represent   the  state  of   the  source,   which   produces   3.   se-
58   The  Mathematical   Theory  of  Communication
quence   of   symbols   Xl;   and  let   f3   be  the  state  of   the   transducer,
which produces, in its output, blocks of symbols Yj.   The  combined
system  can  be  represented  by  the  "product  state  space"  of   pairs
(a,   f3).   Two  points   in   the  space   (a1,   {31)   and   (a2,   f32),   are   con-
nected  by  a  line   if   a1   can  produce  an  x  which   changes   f31   to   f32'
and  this   line  is  given  the  probability  of   that   x   in  this   case.   The
line   is   Iabeled   with   the   block   of   Y1   symbols   produced   by   the
transducer.   The  entropy  of   the   output   can  be   calculated  as   the
weighted  sum  over  the  states. If we sum  first   on  f3   each  resulting
term  is  less  than  or   equal   to  the  corresponding  term  for   a,   hence
the   entropy   is   not   increased.   If   the   transducer   is   non-singular
let   its  output   be  connected  to   the   inverse   transducer.  If  Hi,   H ~
and   H ~ are   the   output   entropies   of   the   source,   the   first   and
second   transducers   respectively,   then   Hi >  H ~ >  H ~ =   Hi   and
therefore Hi =  H ~ .
Suppose  we have  a  system of  constraints  on  possible  sequences
of   the   type   which   can   be   represented   by   a   linear   graph   as   in
Fig.   2.   If   probabilities   p ~ s ~ were   assigned   to   the   various   lines
connecting state i   to  state  j   this would  become  a  source.   There  is
one  particular  assignment  which  maximizes  the  resulting  entropy
(see  Appendix 4) .
Theorem 8:   Let  the  system of constraints  considered as a chan-
nel   have  a  capacity  C =  log  W.   If   we  assign
(s)   B;   W-l(')
Pi; =IJ:   .,
where  lWis  the  duration  of  the  sth  symbol   leading  from  state i   to
state  j   and  the  B i   satisfy
then H  is maximized and  equal to  C.
By   proper   assignment   of   the   transition   probabilities   the
entropy   of   symbols   on   a   channel   can   be   maxImIzed   at   the
channel   capacity.
9.   The  Fundamental   Theorem  for   a   Noiseless  Channel
We   will   now  justify  our  interpretation   of   H  as   the  rate   of   gen-
Discrete  Noiseless  Systems   59
erating   information   by   proving   that   H  determines   the   channel
capacity  required  with   most   efficient   coding.
Theorem  9:   Let   a  source   have   entropy   H   (bits   per   symbol)
and  a   channel   have   a   capacity   C   (bits   per   second).   Then  it   is
possible   to   encode   the  output   of   the   source  in  such  a  way  as   to
C
transmit   at   the  average  rate   H   -   £   symbols   per  second  over  the
channel  where   £   is  arbitrarily  small. It  is  not   possible  to  transmit
h
  e
at   an  average  rate  greater  t   an Ii'
The  converse  part  of  the  tlieorem,   that Zcannot   be  exceeded,
may  be  proved  by  noting  that   the   entropy  of   the   channel   input
per   second  is   equal   to   that   of   the   source,   since   the   transmitter
must   be   non-singular,   and   also   this   entropy   cannot   exceed   the
channel   capacity.   Hence  H' < C  and  the  number  of   symbols  per
second  =  H'jH < CjH.
The  first   part   of   the   theorem  will   be   proved  in   two   different
ways.   The  first   method  is  to   consider  the  set   of   all   sequences  of
N  symbols   produced  by  the  source.   For   N  large   we   can   divide
these  into  two   groups,   one   containing  less   than  2
U1
+1/ ) N   members
and  the second  containing less than 2
R N
members   (where R  is the
logarithm  of  the  number of  different symbols)   and  having  a  total
probability  less   than   fL.   As   N  increases   1]   and   J.L   approach   zero.
The   number   of   signals   of   duration   T   in   the   channel   is   greater
than  2(C-O)T  with  0 small when  T  is  large. If  we  choose
then   there   will   be   a   sufficient   number   of   seqllcnp,es  of   channel
symbols   for   the  high  probability  group  when   ... -"'1   and  Tare  suffi
cicntly   large   (however   small   A)   and   also   some   additional   ones.
The  high  probabIlIty  group   IS   coded  In  an   arbItrary   one-to-one
way   into  this   set.   The   remaining   sequences   are   represented   by
larger  seqllenccs,   starting  and   ending  with   one   of   the   sequcnces
not   used   for   the   high   probability   group.   This   special   sequence
acts  as  a  slart and  stop  signal   for   a  different   code.   In  between  a
suffiCIent   tnne   IS   allowed   to   give   enough   dIfferent   sequences   for
all   the  low  probability  messages.   This   will   require
60   The  Mathematical   Theory  of   Communication
where   ep   is small.  The  mean  rate  of  transmission  in  message  sym-
bols  per second will   then  be  greater than
[<1-0)   ~ +0   ~ J - l = [(1-0)(  ~ +A)+O(   ~ -or
As N  iribreases 0, Aand  ~ approach zero and the rate approaches
C
H'
Another method  of  performing  this  coding and  thereby prov-
ing  the  theorem  can  be  described  as   follows:   Arrange  the  mes-
sages of length N  in  order of decreasing probability and suppose
s-1
their   probabili ties   are  PI  >  P2  >  pa'   .   . >  pn.   Let   Pa   = 2:Pi;
I
that is P,   is the cumulative probability up to,   but not including,
pa.   We  first   encode  into  a   binary  system.   The  binary  code  for
message s is obtained by expanding P,   as  a  binary number.   The
expansion  is   carried  out   to   m,   places,   where   m,   is   the  integer
sa tisfying :
1   1
log2 -   <  m,   <  1 + log, -.
pa   ps
Thus  the  messages  of  high  probability are  represented  by short
codes  and  those  of   low  probability  by  long  codes.   From  these
inequalities  we  have
1   1
2m,   <  ps <  2m , -   I   '
The  code  for   Fa   will   differ   from  all   succeeding  ones   in  one  or
more of its m, places, since all   the remaining Pi   are at  least   2 ~ n ,
larger   and  theIr   bInary  expansions   therefore  dIffer   In  the   first
m,   places.   Consequently  all   the   codes   are   different   and  it   is
possible  to  recover   the   message  from  its   code.   If   the   channel
sequences  are  not   already  sequences  of  binary  digits,   they  can
be   ascribed   binary   numbers   in   an   arbitrary   fashion   and   the
bInary code thus translated Into signals sUItable for  the channel.
The  average  number  HI   of   binary  digits   used  per  symbol   of
origins,]   message  is  easily  estima.ted.   We  ha.ve
Discrete Noiseless Systems   61
1
HI   = ~ ~ m , p , .
But,
1   (   1)   1   1   (   1)
N   ~ log2 -:p:   p,  <   N   ~ m 8 P , <   N   ~ 1 + log2  ---:;;:   p,
and  therefore,
1
GN  < HI   <  GN +  N'
As N  increases GN  approaches H,   the entropy  of the  source and
HI   approaches  H.
We   see  from  this  that   the  inefficiency  in   coding,   when  only
a   finite  delay  of   N  symbols   is   used,   need  not   be   greater   than
~ plus   the   difference   between   the   true   entropy  H  and   the
entropy  G
N
  calculated  for   sequences  of  length  N.   The  per  cent
excess  time  needed  over  the  ideal   is  therefore  less  than
G
N
+   1   _   1.
H   HN
This   method   of   encoding   is   substantially   the   same   as   one
found   independently  by   R.   M.   Fano."  His  method  is   to   arrange
the   messages   of   length   N   in   order   of   decreasing   probability.
Divide  this  series   into  two  groups  of   as   nearly  equal   probability
as   possible.   If   the   message   is   in   the   first   group   its   first   binary
digit   will   be   0,   otherwise   1.   The   groups   are   similarly   divided
into  subsets  of   nearly  equal   probability  and  the   particular   sub-
set  determines  the  second  binary  digit.   This  process  is   continued
until   each   subset   contains   only   one   message.   It   is   easily   seen
that   apart   from  minor   differences   (generally   in   the   last   digit)
thIs  amounts to the same thing as the arIthmetic process described
above.
] O.   Discussion  and  Examples
In  order to  obtain  the  maximum  power transfer  from  a  generator
to   a   load,   a   transformer   must   in   general   be   introduced   80  that
9 Technical   Report   No.   65, The  Research  Laboratory  of   Electronics,   M.l.T.,
March  17, 1949
62   The it!athematical   Theory  of   Communication
the  generator   as   seen   from  the  load  has   the  load  resistance.   The
situation   here   is   roughly   analogous.   The   transducer   which   does
the  encoding  should  match  the  source  to  the  channel   in  a  statis-
tical   sense.   The   source   as   seen   from   the   channel   through   the
transducer   should   have   the   same   statistical   structure   as   the
source  which  maximizes  the  entropy  in  the  channel.   The  content
of   Theorem  9  is  that,   although  an   exact   match  is  not   in  general
possible,   we  can  approximate   it   as   closely  as   desired.   The  ratio
of  the actual rate of  transmission  to  the capacity C may  be  called
the  efficiency  of  the  coding  system.   This  is  of   course  equal   to  the
ratio  of   the   actual   entropy  of   the  channel   symbols   to   the  maxi-
mum  possible  entropy.
In  general,   ideal  or nearly  ideal   encoding  requires  a  long  delay
in   the   transmitter   and  receiver.   In  the   noiseless   case   which   we
have  been  considering,  the  main  function  of  this delay  is to  allow
reasonably   good   matching   of   probabilities   to   corresponding
lengths   of   sequences.   With   a   good   code   the   logarithm  of   the
reciprocal   probability  of   a  long  message  must   be  proportional   to
the  duration  of   the  corresponding  signal,   in  fact
log  p-l   I
T   -   C
must  be  small   for   all   but a  small   fraction  of  the  long  messages.
If a source  can produce only one  particular message its  entropy
is   zero,   and   no   channel   is   required.   For   example,   a   computing
machine   set   up   to   calculate   the   successive   digits   of   7r   produces
a definite sequence with no chance clement. No  channel is required
to  "transmit" this  to  another  point.   One  could  construct  a  second
machine   to   compute   the   same   sequence   at   the   point.   However,
this  may  be  imprnctica.l   In  such   a   case   we  c:m  choose   to  ignore
some   or   all   of   the   statistical   knowledge   'I.e  have   of   the   source.
that   we   construct   a   system  capable   of   sendmg  any  sequence  of
digits.   In   a   similar   way   we   may   choose   to   use   some   of   our
statistical   knowledge   of   English   in   constructing  a   code,   but   not
all   of  it.   In such  a  ease  vie consider  the  source  ',.'lith  the  maximum
entropy   subj ect   to   the   statistical   conditions   we   wish   to   retain.
The   entropy   of   tIllS   source   determmes   the   channel   capacity
which  is necessary and sufficient.   In  the  tr  example  the only  infor-
Discrete  Noiseless  Systems   63
Illation   retained   is   that   all   the   digits   are   chosen   from  the   set
0,   1,   .   .   .   , 9.   In  the   case  of   English  one  might   wish  to   use  the
statistIcal   savmg  possIble   due   to   letter   frequencIes,   but   nothIng
else. The maximum entropy source is then the  first approximation
to   English   and   its   entropy   determines   the   required   channel
capacity.
As  a  simple  example  of  some  of  these  results   consider  a  source
which  produces  a  sequence  of  letters  chosen  from  among  A, B,   C,
D  with  probabilities !,  !,   1,   1,   successive   symbols   being  chosen
independently.   We  have
H  =  -   (!   log ! + !   log ! +  ~ log  1)
= t   bits per  symbol.
Thus   we  can   approximate   a   coding   system  to   encode   messages
from  this  source  into  binary  digits   with  an   average   of i   binary
digit per symbol. In this case  we can actually  achieve  the  limiting
value   by   the   following   code   (obtained   by   the   method   of   the
second   proof   of   Theorem  9) :
A
B
C
D
o
10
110
111
The  average  number of  binary digits  used  in  encoding a  sequence
of N  symbols  will  be
N (!   X  1 + !   X  2 + i   X  3)   =   i   N.
It  is   easily  seen  that   the   binary  digits   0,   1  have   probabilities
~ , !   so  the   H  for   the   coded  sequences   is   one   bit   per   symbol.
Since,   on   the  average,   we   have  i   bInary  symbols   per   orIginal
letter,   the  entropies   on   a   time   basis   are   the  same.   The   maxi-
mum  possible entropy for   the  original  set is log  4 -   2,  occurring
when A,   B,   0,   D have probabilities L  ~ , L  ~ . Hence the relative
entropy  is   ~ . 'Vve   can  translate   the   binary  sequences   into  the
orIgmal   set   of   symbols  on  a   two-to-one   basIs   by  the  following
table:
00
01
10
11
A'
B'
0'
D'
64   The  Mathematical   Theory  of  Communication
This   double  process   then  encodes   the  original   message   into   the
same  symbols  but with  an  average  compression  ratio  :.
As   a   second  example  consider   a   source  which   produces   a   se-
quence   of   A's   and   B's   with   probability   p   for   A   and   q   for   B.
If  p < <  q  we have
H = -   log pP(l   -   p)1-P
= -   p log p(1  -   p) (l-p)/p
..:.   p log  ~ .
In  such   a   case   one   can   construct   a   fairly   good  coding   of   the
message   on   a   0,   1  channel   by   sending  a   special   sequence,   say
0000, for the infrequent symbol  A  and then a  sequence indicating
the  number  of   B's   following   it.   This   could   be  indicated  by   the
binary   representation   with   all   numbers   containing   the   special
sequence  deleted.   All  numbers up  to  16 are  represented  as  usual;
16 is represented  by  the  next binary  number  after  16 which  does
not contain four  zeros, namely  17 =  10001, etc.
I t   can   be   shown   that   as   p ~ 0   the   coding   approaches   ideal
provided  the  length  of   the  special   sequence  is  properly  adjusted.
-Hr---------
11.   Representation  of   a  Noisy  Discrete  Channel
We  now  consider the  case  where  the  signal   is  perturbed  by  noise
during  transmission  or  at  one  or  the  other  of  the  terminals.   This
means that the received  signal is not necessarily the same  as that
sent out   by  the  transmitter.   Two  cases  may  be  distinguished. If
a   particular   transmitted   signal   always   produces   the   same   re-
ceived  signal,   i.e.,   the   received   signal   is   a   definite   function   of
the  transmitted  signal,   then  the  effect   may  be  called  distortion.
If   this   function   has   an   inverse -   no   two   transmitted   signals
producing   the   same   received   signal -   distortion   may   be   cor-
rected,   at   least   in   principle,   by   merely   performing   the   inverse
functional   operation  on the  received  signal.
The  case  of   interest   here   is  that   in  which  the  signal   does  not
always undergo the  same  change  in  transmission.   In  this  case  we
may  assume  the  received  signal  E  to  be  a  function  of   the   trans-
mItted  sIgnal   S and  a  second  varIable,   the  noise  N.
,.L , E  -   f(S  V)
The   noise  is  considered  to  be  a  chance  variable  just  as  the  mes-
sage  \-vas   above.   In  general   it   may  be  represented  by   a  suitable
stochastic  process.   The  most general   type  of  noisy  discrete  chan-
nel  we shall   consider  is  a  generalIzation  of   the  finIte  state  noise-
free  channel   described  previously.   We  assume  a  finite  number of
states  and  a  set of  probabilities
66   The  Mathematical   Theory  of   Communication
Pa,i ({1,j).
This  is  the  probability,   if   the   channel   is  in  state   a   and  symbol  i
is   transmitted,   that   symbol   j   will   be   received   and   the   channel
left in  state  {3.  Thus  a   and  {3  range  over   the  possible  states, i   over
the  possible  transmitted  signals   and  j   over   the  possible   received
signals.   In  the  case   where   successive  symbols   are   independently
perturbed  by  the  noise  there  is only  one  state,   and  the  channel   is
described  by   the   set   of   transition  probabilities   Pi (j),   the   prob-
ability  of  transmitted  symbol   i   being  received  as   j.
If  a  noisy  channel   is   fed  by  a  source   there  are   two  statistical
processes   at   work:   the   source   and  the   noise.   Thus   there   are   a
number   of   entropies   that   can   be   calculated.   First   there   is   the
entropy  H (x)   of  the  source  or  of  the   input to  the  channel   (these
will   be  equal   if  the  transmitter  is  non-singular).   The  entropy  of
the  output of the  channel, i.e., the  received signal, will  be denoted
by  H  (y).   In  the  noiseless   case  H (y)   =  H  (x).   The  joint  entropy
of  input   and output  will   be  H (x,y).   Finally  there  are  two  condi-
tional   entropies   Hx(Y)   and   Hy(x),   the   entropy   of   the   output
when  the  input is  known  and  conversely.   Among  these  quantities
we have  the  relations
H(x,y)   = H(x) + Hx(Y)   = H(y) + Hy(x).
All   of  these  entropies  can  be  measured  on  a  per-second  or   a  per-
symbol   basis.
1-2.   Equivocation  and  Channel   Capacity
If  the  channel   is  noisy  it is  not  in  general   possible  to  reconstruct
the  original   message  or   the  transmitted  signal   }'!lith  certainty  by
any  operation  on  the  receivcd  signal E.   Therc  arc,   however,   ways
of   transmitting  the  information  which  are   optimal   in  combating
noise.   This is the  problem  which  we now  consider.
Suppose   there   are   two  possible  symbols   0   and   1,   and   we  are
transmitting at a  rate  of  1000 symbols  per second  with  probabili
ties  po   PI   ~ . Thus our source  is  producing  infOIIllation at  the
rate of  1000 bits  per second.   During transmission  the  noise  intro-
duces   errors  so  that,   on  the  average,   1  in  100 is   received   incor-
rectly   (a   0  as   1, or   1 as  0).   What   is  the  rate  of   transmission  of
The  Discrete  Channel   with  Noise   67
information?  Certainly  less  than  1000 bits  per second since  about
1%of  the  received  symbols  are  incorrect.   Our first  impulse  might
be  to  say  the  rate  is  990  bits  per   second,   merely  subtracting  the
expected  number   of   errors.   This   is  not   satisfactory  since  it   fails
to  take  into  account   the   recipient's   lack  of   knowledge   of   where
the errors occur. We  may  carry it to  an  extreme  case  and  suppose
the   noise   so  great   that   the   received   symbols   are   entirely   inde-
pendent of  the  transmitted  symbols.   The  probability  of   receiving
1 is !  whatever  was   transmitted  and  similarly  for   O.   Then  about
half  of  the  received  symbols  are   correct  due  to  chance  alone,   and
we  would  be   giving  the   system  credit   for   transmitting  500   bits
per  second  while   actually  no  information  is  being  transmitted  at
all.   Equally "good" transmission  would  be obtained  by  dispensing
with   the   channel   entirely   and   flipping   a   coin   at   the   receiving
point.
Evidently  the  proper   correction  to  apply  to  the  amount   of   in-
formation  transmitted  is the  amount of  this  information  which  is
missing   in   the   received   signal,   or   alternatively   the   uncertainty
when  we have  received  a  signal   of   what   was   actually  sent.   From
our previous  discussion  of  entropy  as  a  measure  of  uncertainty  it
seems  reasonable  to  use   the   conditional   entropy  of   the   message,
knowing  the  received  signal,   as   a  measure  of   this   missing  infor-
mation.   This  is indeed the  proper  definition,   as  we shall   see  later.
Following  this   idea  the  rate  of   actual   transmission,   R,   would  be
obtained   by   subtracting   from   the   rate   of   production   (i.e.,   the
entropy  of  the  source)   the  average  rate  of   conditional   entropy.
R  =   H(x)   -   Hy(x)
The  conditional   entropy  Hy(x)   will,   for   convenience,   be   called
the   equivocation   It   measures   the   average   ambiguity   of   the   re-
eeived  signal.
In  the  example  considered  above,   if  a  0  is  rcccived  the  a  poste-
non probability that a  0 was  transmitted  IS  .99, and that  a  1 was
transmitted  is   .01.   These  figures   are   reversed  if   a   1  is   received.
Hence
                                    log  .99 + 0.01  log 0.01]1--------
=  .081  bits/symbol
or 81 bits  per second.   We may say that the system  IS  transmitting
88   The  Mathematical   Theory  of  Communication
at   a   rate  1000   81   919  bits  per  second.   In  the  extreme   case
where a 0 is equally likely to  be received as a 0 or  1 and similarly
for  1, the a postertori probabilities are !, !   and
H
1I
(x)   =   -   alog! +!   log  !]
=  1 bit per symbol
or   1000  bits  per  second.   The  rate  of  transmission  is  then  0  as  it
should be.
The  following  theorem  gives  a direct  intuitive  interpretation of
the equivocation and  also  serves to  justify it as the unique appro-
priate   measure.   We   consider   a   communication   system  and   an
observer   (or auxiliary device)   who  can  see both what is sent and
what is  recovered   (with  errors  due  to  noise).   This  observer notes
the   errors   in   the   recovered   message   and   transmits   data   to   the
receiving  point over a  "correction channel" to  enable the  receiver
to  correct the  errors.   The  situation  is  indicated  schematically  in
Fig. 8.
CORRECTION  DATA
I
OBSERVER
~
M
  M'
SOURCE   TRANSMITTER   RECEIVER   CORRECTING
DEVICE
Fig.   8.   Schematic   diagram  of   a   correction   system.
Theorem  10:   If  the  correction  channel  has  a  capacity  equal   to
Hy(x)   it is  possible to so encode the  correction data  as to 8end it
over this  channel and  correct  all   but  an  arbitrarily  small  fraction
e of the  errors.   This  is  not  possible if the  channel  capacity  is less
than Hy(x).
Roughly  then,  IIy (x)   is  the  amount   of   additional   information
The  Discrete  Channel  with  Noise   69
that must be supplied per  second  at the  receiving point to  correct
the  received  message.
To   prove   the   first   part,   consider   long   sequences   of   received
message M'   and  corresponding original message  M.  There will  be
logarithmically  THy   (x)   of  the M's  which  could  reasonably  have
produced  each  M'.   Thus  we  have   THy (x)   binary  digits  to  send
each  T  seconds.   This can  be done  with  f   frequency  of errors on  a
channel of capacity Hy(x).
The  second  part   can  be  proved  by   noting,   first,   that   for   any
discrete  chance variables  x,   y,   Z
,
Hy(x,z)   > Hy(x).
The left-hand side  can  be expanded to give
Hy(z) + Hyz(x)  > Hy(x)
Hyz (x)   > HfAx )  -   Hy(z)   >H1/ (x)   -   H (z ) .
If   we  identify  x   as   the  output   of   the   source,   y   as  the  received
signal   and  z  as  the  signal   sent  over   the  correction  channel,   then
the  right-hand  side  is the  equivocation  less the  rate  of  transmis-
sion  over   the  correction  channel.   If  the  capacity  of   this   channel
is  less than  the  equivocation  the   right-hand  side  will   be  greater
than  zero  and  H1/z(x)   >  o.   But   this   is  the   uncertainty  of   what
was   sent,   'knowing   both   the   received   signal   and   the   correction
signal.   If this  is greater than  zero  the  frequency  of  errors  cannot
be arbitrarily small.
Example:
Suppose  the  errors  occur  at   random  in  a  sequence  of  binary  digits:
probability  p   that   a  digit   is  wrong  and   q =  1 -   P that   it   is  right.
These  errors   can  be  corrected   if   their   position   is   known.   Thus   the
correction  channel   need  only  send  information  as  to  these   positions.
This   amounts  to  transmitting  from  a  source  which  produces  binary
digits with  probability  p  for  1  (incorrect)   and  q  for  0  (correct). This
requires a channel  of capacity
[p log p +  q log q]
which is the  equivocation  of the  original system.
The  rate  of   transmission  R  can  be  written  in  two  other  forms
due to  the  Identities noted above.   We have
70   The  Mathematical   Theory  of Communication
R   H(x)   Hy(x)
= H (y)   -   Hx(Y)
=H(x)  +H(y)  -H(x,y).
The  first   defining  expression  has   already  been  interpreted  as  the
amount of information sent less the uncertainty of what was sent.
The   second   measures   the   amount   received  less  the   part   of   this
which  is  due   to  noise.   The  third  is  the  sum  of  the  two  amounts
less  the  joint   entropy  and  therefore  in  a  sense  is  the  number   of
bits   per   second   common   to   the   two.   Thus   all   three   expressions
have  a  certain  intuitive  significance.
The capacity C of a noisy  channel should be the  maximum pos-
sible rate of transmission, i.e., the  rate when the source is properly
matched   to   the   channel.   We   therefore   define   the   channel   ca-
pacity  by
C =   Max  (H (x)   -   Hy(x))
where   the   maximum  is   with   respect   to   all   possible   information
sources  used  as   input  to  the  channel. If  the  channel   is  noiseless,
Hy(x)   =   O.   The   definition   is   then   equivalent   to   that   already
given  for   a  noiseless   channel  since  the  maximum  entropy  for  the
channel is its capacity  by  Theorem 8.
13.   The   Fundamental   Theorem  for   a   Discrete  Channel
with  Noise
It   may   seem  surprising   that   we   should   define   a   definite   ca-
pacity   C  for   a   noisy   channel   since   we   can   never   send   certain
information  in  such  a  case.  It  is  clear,   however,   that   by  sending
the  information in  a  redundant form  the  probability of errors can
be  reduced.   For   example,   by   repeating  the  message   many  times
and  by  a statistical study of  the  different received versions of the
message  the  probability  of  errors  could  be  made  very  small.   One
would   expect,   however,   that   to   make   this   probability  of   errors
approach   zero,   the   redundancy   of   the   encoding   must   increase
indefinitely,  and the  rate of  transmission  therefore  approach  zero.
This   is  by  no  means  true.   I f   it   were,   there  would  not   be  a  very
well   defined  capacity,   but  only  a  capacity  for   a  given  frequency
of errors, or  a given  equivocation;   the  capacity  going down  as the
The  Discrete  Channel   with  Noise   71
error   requirements   are   made   more   stringent.   Actually   the   ca-
pacity  C  defined  above  has  a  very  definite  significance. It  is  pos-
sible  to  send  information  at  the  rate  C  through  the  channel   with
as   small   a   frequency   of   errors   or   equivocation   as   desired   by
proper   encoding.   This  statement   is   not   true   for   any  rate  greater
than C. If an attempt is made to  transmit at a  higher rate than C,
say  C + R
 l
  ,   then  there  will   necessarily  be  an  equivocation  equal
to   or   greater   than  the   excess   R,   Nature  takes   payment   by  re-
quiring  just that   much   uncertainty,   so  that   we   are  not   actually
getting  any  more than  C through  correctly.
The situation is indicated in  Fig. 9. The rate of  information into
the   channel   is   plotted   horizontally   and   the   equivocation   verti-
cally.   Any  point   above   the   heavy  line   in   the  shaded  region   can
be attained and those  below  cannot. The  points on  the  line  cannot
in  general  be  attained, but there  will  usually  be  two  points  on  the
line that can.
These  results  are  the  main  justification  for   the  definition  of   C
and  will   now  be  proved.
Theorem  11:   Let   a  discrete   channel   have   the  capacity  C  and
a discrete  source  the  entropy  per  second  H.   If  H  <  C  there  exists
a  coding  system  such  that   the  output  of   the  source  can  be  trans-
mitted  over   the   channel   with  an   arbitrarily   small   frequency   of
errors   (or   an  arbitrarily  small  equivocation).  If  H  >  C it  is  pos-
sible   to   encode   the   source   so   that   the   equivocation  is   less   than
H  -   C + f.   where   f.   is   arbitrarily   small.   There   is   no   method  of
encoding  which  gives an equivocation  less  than H  -   C.
The  method  of   proving  the  first   part  of   this  theorem  is  not  by
exhibiting  a  coding  method  having  the  desired  propertIes,   but   by
showing that  such  a   code   must   exist   in   a   certain  group  of   codes.
Fig.   9. -   The   equivocation   possible   for   a   given   input   entropy   to   a   channel.
72   The  Mathematical   Theory  of  Communication
In  fact   we  will   average  the  frequency  of   errors  over   this   group
and show that this average can  be made less than  e. If the average
of  a  set of  numbers  is  less than  £   there  must   exist at  least one  in
the  set which  is less  than  e.  This  will establish  the  desired  result.
The  capacity C of  a  noisy  channel has  been  defined  as
C = Max  (H(x)   -   Hy(x))
where  x  is  the  input and  y   the  output.   The  maximization  is  over
all  sources which  might be used  as input to  the channel.
Let   So   be   a  source  which   achieves   the  maximum  capacity  C.
If   this   maximum  is   not   actually   achieved   by   any  source   (but
only approached as a limit)   let So be a source which  approximates
to   giving  the  maximum  rate.   Suppose  So  is  used  as   input  to  the
channel.   We   consider   the  possible   transmitted  and   received  se-
quences of a long duration T. The following  will be true:
1.   The transmitted sequences fall   into two  classes,   a  high  prob-
ability  group  with  about   2
TH
( Z)   members   and  the  remaining  se-
quences of small total  probability.
2.   Similarly  the  received  sequences have  a  high  probability set
of   about   2
T H
( y )   members   and  a  low  probability  set  of   remaining
sequences.
3.   Each  high   probability  output   could  be   produced  by   about
2
T H
.   ( Z )   inputs.  The total probability  of all   other cases  is small.
4.   Each   high   probability   input   could   result   in   about   2
TH
.  ( Z )
outputs. The total probability of all other results is small.
All the  e's and 8's implied by  the words  "small" and "about" in
these  statements  approach  zero  as  we allow  T  to  increase  and  So
to  approach  the maximizing source.
The   SItuation   IS   summaflzed   In  FIg.   10  where   the   input   se-
quences  are points on  the  left and output sequences points on the
right   The upper fan  of cross lines  represents the range of possible
causes for  a typical output. The  lovler fan  represents the range of
possible   results   from  a   typical   input.   In  boLh   cases   the   "small
probabilIty" sets are ignored.
Now  suppose  we have  another source S, producing  information
at  rate  R  with  R < C  In  the  period  T  this  source  will   have  2
TR
high   probability   messages.   Vle   wish   to   associate   these   with   a
The  Discrete  Channel  with  Noise   73
f
•
II.
•
•
•
•
•
•
2B  ( .   ) 2'
•   HIGH   PROBABILITY
•   RECEIVED   SIGNALS
•
•
•
2H,(lf)2'
REASONABLE  CAUSES
FOR  EACH  f
•
•
2 H(lf)2'
HIGH                         
MESSAGES
•
•   REASONABLE   EFFECTS   •
FROM  EACH  II.
Fig.   10. -   Schematic   representation   of   the   relations  'between   inputs   and   outputs
in  a   channel.
selection of the possible  channel inputs in  such  a  way  as  to  get a
small   frequency  of   errors.   We  will   set   up  this   association  in  all
possible  ways   (using,   however,   only  the high  probability group of
inputs as determined by  the source So)   and average the frequency
of errors for this large class of possible coding systems. This is the
saIIle as  calculating the frequency  of errors  for  a  random  associa-
tlOn of the messages  and  channel Inputs of duration  '1'. Suppose a
particular output Yl is observed. What is the probability  of  more
than  one   message   from  S,   in   the   set   of   possible   canses   of   Yl?
There   are   2
TR
messages   distributed  at   random  in   2
TH
( Z )   points.
The   probability  of   a   particular   point   being   a   message   is   thus
2T(R-H(z» •
The  probability  that   none  of   the  points  in  the  fan   is   a  message
(apart from  the  actual   originating message)   is
                          -                                _
74   The  Mathematical   Theory  of Communication
NovV'   R < H (x)   H
1I
(x)   80   R   H (x)
positive.   Consequently
approaches   (as  T  ~ 00)
1J   with   1J
1 -   2-
T
'7.
Hence  the   probability  of   an  error   approaches   zero   and  the   first
part of  the  theorem  is  proved.
The  second  part  of  the  theorem  is  easily  shown  by  noting that
we   could  merely  send   C  bits   per   second   from  the   source,   com-
pletely  neglecting  the  remainder of  the  information  generated.   At
the  receiver   the   neglected  part   gives   an  equivocation  H(x)   -   C
and  the  part transmitted  need  only  add  e.   This  limit   can  also  be
attained  in  many  other  ways,   as  will   be  shown  when  we  consider
the  continuous  case.
The  last   statement   of   the  theorem  is   a   simple   consequence  of
our definition of  C. Suppose  we can encode  a  source  with H(x)  =
C + a  in  such  a  way  as  to  obtain  an  equivocation  Hy(x)  = a  -   £
with  £   positive.   Then
H(x)   -   Hy(x)   =  C +  e
with  e  positive.   This  contradicts  the  definition  of   C  as  the  maxi-
mum  of  H  (x)   -   Hy(x).
Actually  more  has  been  proved than  was stated in  the  theorem.
If  the  average  of   a  set  of   positive  numbers  is  within  e  of   zero,   a
fraction  of at most y'--;   can have values greater than V;.   Since
£   is  arbitrarily  small   we  can  say  that   almost   all   the  systems   are
arbitrarily  close to  the  ideal.
14.   Discussion
The   demonstration   of   Theorem  11,   while   not   a   pure   existence
proof,   has  some  of   the  deficIencIes  of   such  proofs.   An  attempt  to
obtain   a   good   approximation   to   ideal   coding   by   following   the
method  of  the  proof  is  generally  impractical   In  fact,   apart   from
some  rather   trivial   eases   and  certain  limiting  situations,   no  ex
plicit   description  of   a   series   of   approximation   to   the   ideal   has
been   found.   Probably   thIS   IS   no   aCCIdent   but   IS   related   to   the
The   Discrete  Channel  with  Noise   75
difficulty of giving  an  explicit construction  for  a  good approxima
tion  to  a  random sequence.
An  approximation  to  the  ideal   would  have  the  property  that  if
the  signal   is altered in a reasonable way  by  the  noise, the  original
can  still   be  recovered.   In  other   words   the   alteration  will   not   in
general   bring it closer to  another  reasonable  signal than  the  orig-
inal.   This   is   accomplished   at   the   cost   of   a   certain   amount   of
redundancy  in  the   coding.   The   redundancy  must   be   introduced
in  the   proper   way   to   combat   the   particular   noise   structure   in-
volved.   However,   any  redundancy  in  the  source  will   usually  help
if it is utilized  at   the  receiving  point.   In  particular,   if  the  source
already   has   a   certain   redundancy   and   no   attempt   is   made   to
eliminate it in matching to  the  channel, this redundancy  will help
combat   noise.   For  example,   in  a  noiseless   telegraph  channel   one
could  save   about   50%  in  time   by   proper   encoding  of   the   mes-
sages.   This   is  not   done  and  most   of   the   redundancy  of   English
remains   in  the   channel   symbols.   This   has   the   advantage,   how-
ever,   of   allowing   considerable   noise   in   the   channel.   A  sizable
fraction  of the  letters  can  be  received  incorrectly  and  still   recon-
structed by  the  context. In fact this is probably not a bad approx-
imation  to  the  ideal   in  many  cases,   since  the  statistical   structure
of   English   is   rather   involved   and   the   reasonable   English   se-
quences  are  not too  far   (in  the  sense  required  for  theorem)   from
a  random  selection.
As   in   the   noiseless   case   a   delay   is   generally   required   to   ap-
proach  the   ideal   encoding.   It  now  has  the   additional   function  of
allowing  a  large   sample  of   noise  to  affect   the  signal   before  any
judgment   is  made  at   the   receiving  point   as  to  the  original   mes-
sage.   Increasmg   the   sample   SIze   always   sharpens   the   possible
statistical assertions.
The  content of Theorem  11 and  its  proof  can  be  formulated  in
a  somewhat different   'vVay  which  exhibits  the   connection  with  the
noiseless case more  clearly.   Consider the  possible  signals of dura-
tion  T  and  suppose   a  subset  of   them  is  selected  to  be  used.   Let
those  in the subset all be used  with  equal probability, and suppose
the   receiver   is   constructed   to   select,   as   the   original   signal,   the
most  probable  cause  from  the  subset,   when  a  perturbed  signal   is
received.   We  define  Iv(T, q)   to  be  the  maximum  number  of   sig-
76   The  Mathematical   Theory  of   Communication
nah:;  we can choose  for the subset such that  the probability  of  an
incorrect interpretation  is less than or  equal to  q.
Theorem 12: Lim  log NiT, q)   =   C,   where C is   the channel   ca-
T--+oo
pacity,   provided that  q does not  equal 0 or 1.
In  other words,   no  matter  how  we set our  limits  of  reliability,
we  can  distinguish  reliably  in  time  T  enough  messages  to  corre-
spond  to  about CT  bits, when  T  is sufficiently  large.   Theorem  12
can  be compared with the definition of the  capacity of a noiseless
channel given  in section 1.
1S.   Example  of  a   Discrete  Channel   and  Its   Capacity
A  simple  example  of   a   discrete   channel   is   indicated  in  Fig.   11.
There  are  three  possible  symbols.   The  first   is  never   affected   by
noise.   The  second   and  third  each   have  probability   p  of   coming
through undisturbed, and  q of being changed into the  other of the
•
TRANSMITTED
SYMBOLS
p
Fig.   11. -   Example   of   a   discrete   channel.
•
RECEIVED
SYMBOLS
pair.   Let   ex = -   [p  log  p +  q  log  q]   and  let   P,   Q  and  Q  be  the
probabilities  of  using  the  first,   second  and  third  symbols  respec
tively  (the last two being equal from  consideration of symmetry).
We  have:
H(x)   -   -P log P  -   2Q log Q
Hy(x)   -   2Qa.
We   wish   to   choose   P   and   Q  in   such   a   way   as   to   maximize
H (x)   Hy(x),   subject   to  the  constraint  P + 2Q -   1. Hence  we
consider
The  Discrete  Channel  with  Noise   77
u   P  log P   2Q log Q   2Qa   t   A(P + 2Q)
log  P + X -   0 1
au
ap
au
7iQ =  -   2  -   2 log  Q -   2a + 2X  =  o.
Eliminating  X
log  P  =  log  Q + a
P  =  Qe"   =  Q ~
~
P  =   ~ +2
  1
Q =   ~ +2·
The  channel  capacity is then
~ + 2
C  =   log   ~ .
Note how this checks  the  obvious values in the  cases  p =  1 and
p = !.   In  the   first,   f3 = 1 and  C =  log  3,   which   is  correct   since
the   channel   is then  noiseless  with  three  possible  symbols. If  p =
!,   f3  =   2 and C =   log 2. Here the  second  and  third  symbols  can-
not be distinguished at all   and  act  together  like  one symbol.   The
first   symbol   is  used  with  probability  P = !   and  the  second  and
third  together   with   probability  j .   This   may  be   distributed   be-
tween   them  in  any  desired  way  and  still   achieve  the  maximum
capacity.
For  intermediate  values  of   p  the  channel   capacity  will   lie  be-
tween   log  2  and  log  3.   The  distinction  between  the  second   and
third  symbols   conveys   some  information  but   not   as   much   as   in
the  noiseless   case.   The  first   symbol   is  used  somewhat   more   fre-
quently than the  other two  because of its freedom  from  noise.
16.   The  Channel   Capacity  in  Certain   Special   Cases
If  the   noise  affects   successive   channel   symbols   independently  it
can  be  described  by  a  set  of   transition  probabilities   pij.   This  is
the   probabihty,   if   symbol   t   is  sent,   that   j   Will   be  received.   The
channel  capacity is then  given  by  the  maximum of
78   The  Mathematical   Theory  of   Communication
where   we  vary   the   P
l
  subject   to   zP
l
method of Lagrange to  the equations,
~ psi log   psi   =   }J.
1   LPiPii
i
1.   This   leads   by   the
s =  1   2   " ,   ,
Multiplying 'by  P,   and  summing  on  s  shows  that.   u  «   -C.   Let
the  inverse  of  psi   (if it exists)   be  hst so that LhstPsi  =  Oti.   Then:
s
~ hstpsi log  psi  -   log ~ PiPit   =   -   CL   i.:
S.l   l   s
Hence:
or,
This is the  system of equations  for determining the  maximizing
values of Pi, with  C to  be determined so that   ~ P i =  1. When this
is   done   C  will   be  the   channel   capacity,   and  the  Pi   the   proper
probabilities  for  the  channel symbols to  achieve  this  capacity.
If  each  input   symbol   has   the   same   set   of   probabilities  on  the
lines  emerging  from  it,   and  the  same  is  true  of  each  output sym-
a   b
  c
Fig.   12. -   Examples   of   discrete   channels   with   the   same   transition   probabilities
for   each  input   and  for   each  output.
bol, the  capacity  can  be easily  calculated,  Examples  are  shown in
Fig.   12. In such  a  case Hx(Y)   is independent of the  distribution of
The  Discrete  Channel   with  Noise   79
probabilities on  the  input symbols,   and  is given  by   ~ Pl   log  P ~
where   the   Pi   are   the  values   of   the   transition  probabilities   from
any  input symbol.  The  channel   capacity  is
Max  [H (y)   -   Hx(Y)]
=  Max H(y) + ~ Pi log Pi.
The maximum of H (y)   is clearly  log m where  m is the  number of
output   symbols,   since   it   is   possible   to   make   them  all   equally
probable   by   making   the   input   symbols   equally   probable.   The
channel capacity  is therefore
C =  log m + ~ Pi log Pi.
In Fig.   12a it would  be
C =  log 4  -   log 2 =  log 2.
This  could  be achieved  by  using  only  the   1st and  3d  symbols.   In
Fig.   12b
C  =  log  4  -   flog  3  -   ~ log  6
=  log  4  -   log  3  -   ~ log  2
I
  1   2
5
=   og  3   3'
In Fig.   12c we have
C  =  log  3  -   ~ log  2  -   ~ log  3  -   i   log  6
3
=  log   2!  31  6
t
  .
Suppose  the  symbols fall   into several groups such  t.hat t.he noise
never  causes   a  symbol   in  one  group  to  be  mistaken  for   a  symbol
in   another   group.   Let   the   capacity   for   the   nth  group  be   en   (in
bits   per   second)   when   we  use   only  the   symbols   In  thIS  group.
Then  it   is  easily  shown  that,   for   best   use   of   the  entire  set,   the
total   probability P
n
  of all   symbols in  the  nth  group  should  be
\Vithin  a  group  the  probability   is  distributed  just  as  it  would  be
if these  were the only  symbols being used. The channel  capacity is
80   The  Mathematical   Theory  of   Communication
17.   An  Example  of   Efficient  Coding
The  following  example,   although  somewhat artificial,   is  a  case in
which  exact   matching  to  a  noisy   channel   is  possible.   There  are
two  channel symbols, 0 and 1, and the noise affects them in blocks
of  seven  symbols.   A block  of  seven  is  either  transmitted  without
error, or exactly one symbol of the seven  is incorrect.  These eight
possibilities  are  equally  likely.   We  have
C  =   Max  [H(y)   -   H;z;(Y)]
= f  [7 +~ log  ~ ]
= f  bits/symbol.
An   efficient   code,   allowing   complete   correction   of   errors   and
transmitting  at  the  rate  0,   is  the  following   (found  by  a  method
due  to  R.   Hamming) :
Let  a  block  of  seven  symbols  be  Xl'   X
2
 ,  •••  ,X
7
•   Of  these
Xg ,   X
  5
,   X
6
  and  X
7
  are  message   symbols   and   chosen  arbitrarily
by   the  source.   The  other  three  are   redundant   and  calculated  as
follows:
X
4
 is chosen to make  ex =   X
4
 + X
5
 + X
6
 + X
7
 even
X 2   "   "   "   "   f3 = X2 + Xa + X6 + X7   "
Xl "   "   "   "   Y  = Xl + X
g
 + X5 + X7   "
When  a  block of  seven  is  received  ex,   f3   and  yare  calculated  and
if  even  called  zero,   if   odd  called  one.   The  binary  number   ex   f3   y
then  gives   the   subscript   of   the  Xi   that   is   incorrect   (if   0  there
was  no error) .10
10 For   some   further   examples   of   self-correcting   codes   see   M.   J.   E.   Golay,
"Notes on  DIgItal Codmg," Proceedm(Js  of  the  lnst'ttute  of Radw En(Jmeers,
v. 37, No. 6, June,   1949, p. 637.
-wr---------
We  now  consider   the  case  where   the  signals   or   the  messages   or
both   are   continuously   variable,   in   contrast   with   the   discrete
nature   assumed   heretofore.   To   a   considerable   extent   the   con-
tinuous  case  can  be obtained  through  a  limiting  process  from  the
discrete  case  by  dividing  the  continuum  of   messages  and  signals
into   a   large   but   finite   number   of   small   regions   and  calculating
the  various  parameters  involved  on  a  discrete  basis.   As  the  size
of the  regions  is  decreased  these  parameters  in  general   approach
as   limits   the  proper   values   for   the   continuous   case.   There   are,
however,   a  few new effects that appear and  also  a  general  change
of emphasis in the direction of specialization of the general results
to  particular cases.
We   will   not   attempt,   in   the   continuous   case,   to   obtain   our
results  with  the  greatest   generality,   or  with  the  extreme  rigor  of
pure  mathematics,   since  this   \vould  involve   a   great   deal   of   ab-
stract measure  theory  and  would  obscure  the  main  thread  of  the
analysis.  A preliminary study,   however,   indicates that the theory
can be formulated in a completely axiomatic and rigorous manner
which  includes   both  the  continuous  and  discrete  cases  and  many
others   The  occasional   liberties  taken  with  limiting  processes  in
the   present   analysis   can   be   justified   in   all   eases   of   practical
interest.
18.   Sets  and  Ensembles   of   Functions
We  shall   have  to  deal   in  the  continuous  case  with  sets   of   func-
82   The  Mathematical   Theory  of   Communication
tiofis  and  ensembles  of   functions.   A  set  of   functions,   as  the  name
implies,   is   merely  a   class   or   collection  of   functions,   generally  of
one  variable,   time.  It   can  be  specified  by  giving  an  explicit   rep-
resentation  of   the  various   functions   in   the  set,   or   implicitly  by
giving  a   property  which   functions   in   the  set   possess   and  others
do  not.   Some  examples  are:
1.   The  set   of   functions:
fe(t)   =   sin  (t + 8).
Each  particular  value  of   8 determines  a  particular  function  in
the   set.
2.   The set of   all   functions  of  time  containing no  frequencies  over
W  cycles  per second.
3.   The set of  all   functions  limited  in  band  to  Wand  in  amplitude
to  A.
4.   The  set of   all   English  speech  signals  as   functions  of   time.
An  ensemble  of   functions   is   a  set  of   functions   together   with  a
probability  measure  whereby  we  may  determine   the   probability
of   a   function  in  the  set   having  certain  properties.'   For  example
with  the set,
fe (t)   = sin  (t + 8),
we  may  give  a  probability  distribution  for   8,   say  P(8).   The  set
then  becomes  an  ensemble.
Some   further   examples  of   ensembles  of   functions   are:
1.   A  finite   set   of   functions   !k(t)   (k = 1,   2, ...   ,   n)   with  the
probability  of   fk being  Pk.
2.   A  finite  dimensional   family  of   functions
.
with  a  probability distribution  for   the  parameters  l¥i:
For example  we could  consider the ensemble  defined by
n
n=l
1 In   mathematical   terminology   the   functions   belong   to   a   measure   space
whose  total   measure  is  unity.
Continuous  Information   83
with   the   amplitudes      distributed   normally   and   independ
ently,   and  the  phases   (}i distributed  uniformly   (from  0  to   271'")
and  independently.
3.   The ensemble
+00
""   sin  7r(2Wt   -   n)
f( ai, t)   = LJ an -----"-------'--
n=-oo   7r(2Wt   -   n)
with  the ai  normal and independent all   with the  same standard
deviation y'N.  This  is a  representation of "white" noise,   band
limited  to  the  band  from  0  to  lV  cycles   per   second  and  with
average  power  N.2
4.   Let  points  be  distributed  on  the  t   axis   according  to  a  Poisson
distribution.   At  each  selected  point  the  function I (t)   is  placed
and  the  different   functions   added,   giving  the   ensemble
where   the   tk  are   the   points   of   the   Poisson  distribution.   This
ensemble  can  be  considered  as  a  type  of  impulse  or  shot  noise
where   all   the  impulses  are  identical.
5.   The set of English speech  functions  with the  probability  meas-
ure  given  by  the  frequency  of   occurrence  in  ordinary  use.
An   ensemble   of   functions   la(t)   is   stationary   if   the   same   en-
semble   results   when   all   functions   are   shifted  any   fixed   amount
in time.  The  ensemble
fe(t)   =  sin  (t + 8)
is   stationary  if   8   is   distributed   uniformly   from  0   to   271".   If   we
shift each  function  by  tx  we obtain
le(t + ttl =  sin  (t + t, + 8)
-   sin  (t + If)
2 This                  can  be  used  as  a  definition  of band  limited white  noise.
It  has  certain  advantages  in  that   it   involves  fewer   limiting  operations  than
do   definitions   that   have   been   used   in   the   past.   The   name   "white   noise,"
already   firmly   intrenched   in   the   literature,   is   perhaps   somewhat   unfortu-
nate.   In  optics   white   light   means   either   any  continuous   spectrum  as   con-
trasted  WIth  a  pomt   spectrum,   or   a  spectrum  whiCh  IS  fiat   WIth wavelength
(whiCh  IS not  the  same  as  a  spectrum  flat   WIth frequency).
84
  The  Mathematical   Theory  of  Communication
with   If   distributed   uniformly   from  0   to   271".   Each   function   has
changed but the ensemble as a whole is invariant under the  trans-
lation. The other examples  given  above  are  also  stationary.
An   ensemble   is   ergodic   if   it   is   stationary,   and   there   is   no
subset   of   the   functions   in   the   set   with   a   probability   different
from  0 and  1 which  is stationary.   The  ensemble
sin  (t + 0)
is  ergodic.   No  subset  of  these  functions   of   probability   =1= 0,   1  is
transformed into  itself  under   all   time  translations.   On  the   other
hand  the  ensemble
a sin  (t + 0)
with  a  distributed  normally  and  0  uniform  is  stationary  but  not
ergodic.  The subset of these  functions  with  a between 0 and  1, for
example,   is stationary,   and  has  a  probability  not equal   to  0 or   1.
Of the  examples given,  3 and 4 are ergodic,   and 5 may  perhaps
be  considered  so. If  an  ensemble  is  ergodic   we  may  say  roughly
that   each   function   in  the   set   is   typical   of   the   ensemble.   More
precisely  it   is  known  that   with  an   ergodic  ensemble  an  average
of   any  statistic  over   the  ensemble  is  equal   (with  probability  1)
to  an  average  over   all   the  time  translations  of  a  particular  func-
tion in the set." Roughly speaking, each  function  can  be expected,
as  time  progresses,   to  go through,   with  the  proper   frequency,   all
the  convolutions of  any  of  the  functions  in  the  set.
Just   as   we   may   perform  various   operations   on   numbers   or
functions   to  obtain  new  numbers   or   functions,   we  can   perform
operations   on  ensembles   to   obtain  new  ensembles.   Suppose,   for
example, we have  an  ensemble of functions  faU)   and  an  operator
T  which  gives   for   each  function  fa( t)   a  resulting  function   ga (t) :
3 This   is   the   famous   ergodic   theorem  or   rather   one   aspect   of   this   theorem
whICh  was   proved   In   somewhat   different   formulatIOns   by   Blrkhoff,   von
Neumann,   and  Koopman,   and  subsequently  generalized  by  Wiener,   Hopf,
IImewicz   and  others:   The   literature   on   elgodlC  theOlY  is   quite   extensive
and  the   reader   is   referred   to   the   papers   of   these   WrIters   for   precise   and
general   formulations;   e.g., E.   Hopf                      Ergebnisse  der   Math-
ematic   und  ihrer   Grenzgebiete,   v   5;   "On   Causalit.y   Statistics   and   Proha-
bility,"   Journal   of   Mathematics   and   Physics.   v.   XIII,   No.   I,   1934;   N.
Wiener "The  Ergodic  Theorem,"  Duke  Mathematical   Journal,   v.   5,  1939.
Continuous  Information   85
Probability measure is defined  for  the  set   ga (t)   by means  of that
for the  set   ta{t).   The  probability  of  a  certain  subset  of  the  ga(t)
functions   is   equal   to   that   of   the   subset   of   the   fa(t)   functions
which  produce  members  of  the  given  subset  of   g functions  under
the   operation   T.   Physically   this   corresponds   to   passing   the
ensemble   through   some  device,   for   example,   a   filter,   a   rectifier
or   a   modulator.   The   output   functions   of   the   device   form  the
ensemble  ga(t).
A  device  or   operator   T  will   be  called  invariant  if   shifting  the
input merely  shifts  the  output,  i.e., if
ga(t)   =   Tfa(t).
implies
ga(t +  t l   )   =  Tfa(t + td
for   all   fa(t)   and  all   t
i
  •  It  is  easily  shown   (see  Appendix  5)   that
if   T  is  invariant   and  the  input   ensemble   is   stationary  then  the
output ensemble  is stationary. Likewise if the input is ergodic  the
output will also be ergodic.
A  filter   or   a  rectifier   is  invariant   under   all   time  translations.
The  operation  of modulation  is  not,   since  the  carrier phase  gives
a  certain  time structure.   However,   modulation  is  invariant under
all  translations  which  are  multiples of the period of the  carrier.
Wiener   has   pointed  out   the  intimate  relation  between  the  in-
variance  of  physical   devices  under  time  translations  and  Fourier
theory."   He   has   shown,   in  fact,   that. if   a  device  is  linear  as  well
as   invariant   Fourier   analysis   is   then   the   appropriate   mathe-
matical tool   for dealing with  the  problem.
An   ensemble   of   functions   is   the   appropriate   mathematical
representation  of  the   messages   produced  by  a   continuous  source
(for   example,   speech),   of  the  signals  produced  by  a  transmitter,
and  of   the   perturbing  noise.   Communication  theory   is   properly
4 Communication   theory   is   heavily   indebted   to   Wiener   for   much   of   its
baSIC  phIlosophy  and  theory.   HIS  claSSIC  NDRC  report,   The   Interpolatwn,
Extrapolation,   and   Smoothing   of   Stationary   Time   Series   (Wiley,   1949),
contains the  first   clear-cut   fOlllIulation  of  communication  theory   as  a  statis-
tical   problem,   the  study  of   operations  on   time  series.   This   work,   although
chiefly   concerned   with   the   linear   prediction   and   filtering   problem,   is   an
important.   collateral   reference   in   connection   with   the   present   paper.   We
may  also  refer  here  to  Wiener's Cybernetics   (Wiley,   1948),   dealing with the
general   problems  of  communication  and  control.
86   The  Mathematical   Theory  of Communication
concerned,   as   has   been   emphasized   by   Vlicner,   not   with  opera-
tions   on  particular   functions,   but   with  operations   on  ensembles
of   functIOns. A  communication  system  is  designed  not   for   a  par-
ticular  speech   function  and  still   less  for   a  sine  wave,   but   for  the
ensemble  of   speech   functions.
19.   Band   Limited  Ensembles   of   Functions
If   a   function   of   time   f(t)   is   limited  to   the   band   from  0  to   W
cycles   per  second  it  is  completely  determined  by  giving  its   ordi-
nates at a series  of discrete points spaced  2 ~ seconds  apart in the
manner  indicated  by  the  following  result."
Theorem  13:   Let   f(t)   contain no  frequencies  over W.
Then
~ sin  7l'"(2Wt   -   n)
j(t)   =  LJXn   (   )
-00   7l'"   2Wt   -   n
where
In  this   expansion  f (t)   is   represented  as   a   sum  of   orthogonal
functions.   The   coefficients  Xn   of   the   various   terms   can   be   con-
sidered as  coordinates in an infinite dimensional "function space."
In  this   space   each   function   corresponds   to   precisely   one   point
and  each  point   to  one  function.
A  function  can  be   considered  to  be  substantially  limited  to  a
time   T   if   all   the   ordinates   X;   outside   this   interval   of   time   are
zero.   In  this   case   all   but   2TJV  of   the   coordinates   will   be   zero.
Thus   functions   limited  to  a  band  Wand  duration  T   correspond
to  points in a space of 2TlV dimensions.
A  subset   of   the   functions   of   band  Wand  duration   T   corre-
sponds   to   a   region   in   this   space.   For   example,   the   functions
whose   total   energy   is   less   than   or   equal   to   E   correspond   to
points  in  a  2TH'   dimensional   sphere   \-)lith   radius   r   vi 2lVE.
An  ensemble  of  furrotitrrrs  of  limited  duration  and  band  will  be
5 Fm  a   proof   of   this   themem  and  fUl ther   discussion  see   the   author's   paper
"Communication  in  the  Presence  of   Noise"   publi:::Jh£'d  in  the  Proceedings   oj
the   Institute  of   Radio  Engineers,   v,   37,   No.   1, Jan.,   1949, pp.   10 21.
Continuous  Information   87
represented  by  a  probability  distribution  P(Xl'   ", x
n
)   in  the
corresponding n dimensional  space.   If the  ensemble  is not limited
in  time  we  can  consider  the  2TW  coordinates  in  a  given  interval
T   to   represent   substantially   the   part   of   the   function   in   the
interval   T  and  the   probability  distribution  P(Xl'   •   •   •   ,   X
n
 )   to
give  the  statistical  structure of  the  ensemble  for   intervals  of that
duration.
20.   Entropy  of   a   Continuous   Distribution
The  entropy  of   a   discrete  set   of   probabilities  PI'   .   .   .   ,   P»   has
been  defined as:
H  =  -   L Pi log  Pi.
In  an   analogous   manner   we  define   the   entropy  of   a   continuous
distribution  with  the  density  distribution  function  P(x ~ by:
H  =  - L: p(x)   log  p(x)   dx.
With  an  n  dimensional  distribution  P(Xl, '.'   •   , x
n
 )   we  have
H  =   -   J.   0   ojP(XI   0   0   0   x
n
)   log  P(Xl,   0   0   0   ,   x
n
)   dx,   0   0   0   dx.;
If   we  have   two   arguments   X   and   y   (which  may  themselves   be
multidimensional)   the  joint  and  conditional   entropies  of   P(x,   y)
are   given  by
H(x,  y)   =  - JJp(x, y)   log  p(x, y)   dx  dy
and
where
TT   E  ~ .u. x   y
H
,I
(x)
ifp(x,   y ~ log
Ifp(x, y)   log
r
J
p(x,  y)
p(x)
p(x,  y)
p(y)
dx  dy
dx  dy
The  entropies  of   continuous   distributions   have   most   (but  not
88   The   Mathematical   Theory  of Communication
all)   of  the  properties  of  the  discrete  ca8e. In  particular  we  have
the  following:
1. If x  is limited to  a  certain volume  v  in  its  space,   then  H(x)   is
a   maximum  and   equal   to   log  v   when   p(x)   is  constant   ( ~ )
in the volume.
2.   With  any two  variables  x,   y  we have
H(x, y)   < H(x) + H(y)
with  equality  if   (and  only   if)   x  and  yare  independent,   i.e.,
p(x,y)   =   p(x)   p(y'Y   (apart   possibly   from  a   set   of   points   of
probability  zero).
3.   Consider   a   generalized   averaging   operation  of   the   following
~
type:
p' (y)   = Ja(x, y)   p(x)   dx
with
fa(x, y)   dx  = fa(x,  y)   dy   =  1,
  a(x, y)   > o.
Then  the  entropy  of   the  averaged  distribution  p'(y)   is  equal
to  or  greater than that of the original  distribution p(x).
4.   We  have
H(x,  y)   = H(x) + H:c(y)   = H(y) +H
1I(x)
and
H:c(y)   < H (y).
5.   Let p(x)   be  a  one-dimensional   distribution.   The  form  of  p(x)
giving  a  maximum  entropy  subject   to  the  condition  that   the
standard deviation of x be fixed at  u  is Gaussian.   To  show this
we must maximize
H(x)   fp(x)   log  p(x)   dx
f p(x)   dx
1 and
(
1p(x)x
2
dx
with
as   constraints.   ThIS  requires,   by   the   calculus   of   varIations,
maXimizing
______p- p(x)   log  p(x) + Ap(X)X
2
+I-Lp(x)]   dx.
Continuous  Information
The  condition  for   this  is
-1  -   log p(x) + AX
2
+ M =  0
89
and  consequently   (adjusting  the  constants  to  satisfy  the  con-
straints)
122
p(X)   = ---- e-(x/2f1).
y!2;  a
Similarly  in  n  dimensions,   suppose  the  second  order   moments
of P(Xl'   •   •   •  ,X
n
 )   are  fixed at  A
i j
  :
A
i i
  = J...JXiXiP(Xi,  ...  ,   X
 n
 )   dx, ...  dx.;
Then  the  maximum  entropy  occurs   (by  a  similar  calculation)
when  P(Xl,   •   •   •   , x
n
 )   is the  n  dimensional   Gaussian  distribu-
tion with the second  order moments  Aij.
6.   The entropy of a  one-dimensional Gaussian distribution whose
standard deviation is   (T is given  by
H(x)   =logy!2;eO'.
This is calculated as  follows:
(   )
  1   e-(X2/2f12)
P  X   =   y21r  U
-   x
2
-   log  p(x)   =   log y21r  U   +  2u
2
H(x)   =  - Jp(x)   log  p(x)   dx
fp(x)   log v'27f   (1   dx +fp(x)
-   log v'21r  U   +   2(12
=  log vZ; U   + log ve
-   log V21fi U  •
Similarly  the   n  dimensional   Gaussian  distribution  with   asso-
ciated  quadratic  form  ai,   is given  by
~ I i (
(2   )   '2   exp
90   The  Mathematical   Theory  of  Communication
and the entropy can be calculated as
H  =  log  (21re)n/2  ~ _ - _ i _
where Iaij I is the  determinant whose elements are aij.
7.   If x  is limited to  a half line   (p(x)   =  0 for  x < 0)   and the first
moment of  x  is fixed at a:
a  =1
00
p(x)x  dx,
then  the maximum entropy occurs  when
p(x)
  1
=   -   e-(z/a)
a
and is equal to  log ea.
8.   There  is  one  important difference   between  the  continuous  and
discrete entropies. In the discrete case the entropy measures in
an  absolute way  the randomness of the  chance  variable. In the
continuous  case  the  measurement  is  relative  to  the  coordinate
system.  If  we  change  coordinates   the  entropy  will   in  general
change.   In fact if we change to  coordinates YI ...  Yn the new
entropy is given  by
H(y)   = J...JP(Xl ...  xn )  J(1-)
log  P(Xl ...  x
n
 )  J(   ~ ) dYl ...  dYn
where  J (:)  is the  Jacobian  of the  coordinate  transformation.
On expanding the  logarithm  and  changing variables to  Xl •••
X
n
 ,  we obtain:
H(y)   -   H(x)   -   f·   . ·fp(Xl' ...  ,   x
n)
  log J(   ~ ) dXl ...  dx
n.
Thus  the  new  entropy is the  old  entropy  less the  expected  log-
arithm  of  the Jacobian   In  the  continuous case the entropy  can
be  considered  a  measure  of  randomness relative  to  an assumed
standard,   namely   the   coordinate   system  chosen   with   each
small  volume  element  dX
I
  •   •   •   dX
n
  given  equal   weight.   When
we  change  the  coordinate  system  the   entropy  in  the  new  sys-
tern   mea,sures   the   randomness   when   equal   volume   elements
dYI ...  dYn in the  ne')."': system are given  equal  weight.
In  spite   of   this   dependence   on   the   coordinate   system  the
Continuous  Information   91
entropy  concept  is  as   important  in  the  continuous  case  as  the
discrete  case.   This  is  due  to  the  fact that the  derived  concepts
of  information rate and  channel  capacity depend on  the differ-
ence  of   two  entropies   and  this   difference   does   not   depend  on
the  coordinate  frame,   each  of  the  two  terms  being  changed  by
the same amount.
The  entropy  of   a   continuous   distribution   can  be   negative.
The  scale  of   measurements  sets  an  arbitrary  zero  correspond-
ing  to   a  uniform  distribution  over   a  unit  volume.   A  distribu-
tion which is more confined than this  has  less  entropy and  will
be  negative.   The  rates  and  capacities  will,   however,   always  be
non-negative.
9.   A  particular  case  of   changing  coordinates  is  the  linear  trans-
formation
In this case the Jacobian is simply the determinant I ai,   1-
1
and
H(y)  = H(x) + log   1 aij   I .
.
In  the  case  of   a  rotation  of   coordinates  {or  any  measure  pre-
serving transformation)   J =  1 and H (y)   = H (x).
21.   Entropy  of   an   Ensemble  of   Functions
Consider   an  ergodic   ensemble   of   functions   limited  to   a   certain
band of width W  cycles per second. Let
P(Xl'   .   •   .   ,X
n
 )
be  the density  distribution  function  for   amplitudes  Xl  •••  X
n
 at
n  successive sample points. We  define  the entropy of the ensemble
per degree of  freedom  by
H'
  1   (   (
log  P(Xl, ...  ,  x
n
)   dXI ...  dx
n
•
We  may  also  define  an  entropy  H  per second  by  dividing,  not by
n, but   by  the  time  T  in  seconds   for   n  samples.   Since  n   2TW,
H  -   2WH'.
92   The  Mathematical   Theory  of   Communication
Vlith white thermal noise  p is Gaussian and we have
H' =  log ,; 27reN,
H  =  W  log 2rreN.
For   a   given  average  power  N,   white  noise   has  the  maximum
possible  entropy.   This  follows  from  the  maximizing  properties  of
the Gaussian distribution noted  above.
The   entropy   for   a   continuous   stochastic   process   has   many
properties analogous to that for  discrete processes. In the discrete
case  the  entropy  was  related  to  the  logarithm  of   the  probability
of   long  sequences,   and  to  the  number  of  reasonably  probable  se-
quences  of   long  length.   In  the   continuous   case  it   is  related  in  a
similar  fashion  to  the  logarithm  of   the  probability  density   for   a
long  series of  samples,   and  the  volume  of  reasonably  high  proba-
bility in  the  function  space.
More  precisely,   if  we assume  p (Xl' ••.  ,x
n
 )   continuous in  all
the  Xi   for  all n, then  for  sufficiently  large n
1
10
:   p   -   H' I <  E
for   all   choices   of   (Xl,   •   .   .   ,   x
n
 )   apart   from  a   set   whose  total
probability  is  less than 8, with  8 and  £  arbitrarily  small. This fol-
lows from  the ergodic property if we divide the space into  a  large
number of small cells.
The  relation  of   H  to  volume  can  be  stated  as   follows:   Under
the   same   assumptions   consider   the   n   dimensional   space   corre-
sponding to  P(Xl'  •••  ,x
n
) . Let Vn(q)   be the smallest volume in
this   space   which   includes   in   its   interior   a   total   probability   q.
Then
Lim   log  Vn(q)   H'
n-+oo   n
provided  q does  not equal 0 or  1.
These results show that for  large n there is a rather well-defined
volume   (at   least   in  the   logarithmic   sense)   of   high   probability,
and  that within  this  volume  the  probability  density  is  relatively
unifonn   (again  in  the  logarithmic  sense).
In the 'Nhite noise  case  the distribution  function  is given  by
1
(27rN)n/2   exp
Continuous  Information   93
Since  this  depends  only  on      the   surfaces  of  equal   probability
density are spheres and  the entire distribution has spherical sym-
metry.   The   region   of   high   probability   is   a   sphere   of   radIUS
vr;N.   As   n   00   the   probability  of   being  outside   a   sphere   of
radius V n (N + f)   approaches  zero  however  small   e and! times
n
  
the  logarithm of the  volume of the  sphere approaches log V 27reN.
In  the   continuous   case   it   is   convenient   to   work  not   with  the
entropy  H  of  an  ensemble  but with  a  derived  quantity  which  we
will   call   the   entropy   power.   This   is   defined   as   the   power   in   a
white   noise   limited  to  the   same   band   as   the   original   ensemble
and having the same  entropy.   In other words  if H'  is the  entropy
of an  ensemble  its  entropy  power  is
N
1
 = _1_ exp  2H'.
21re
In  the   geometrical   picture   this   amounts   to   measuring  the   high
probability volume  by  the  squared  radius  of  a  sphere  having  the
same  volume.   Since  white   noise  has  the  maximum  entropy  for   a
given power, the entropy  power of  any noise  is  less than  or  equal
to  its  actual power.
22.   Entropy  Loss  in  Linear   Filters
Theorem  14:   If   an  ensemble   having  an   entropy  HI   per   degree
of   freedom  in  band  W  is   passed  through  a  filter  with  character..
istic  Y (f)   the  output  ensemble  has  an  entropy
H
2
  =  HI + --=!;- [   log   I                            d"'ol--f.   _
WJw     
The operation of the  filter  is essentially a linear transformation of
coordinates. If  we think of  the  different frequency  components  as
the   onginal   coordmate   system,   the   new  frequency   components
are   merely  the   old   ones   multiplied   by   factors.   The   coordinate
transformation  matrix is thus essentially diagona.lized in  terms of
these   coordinates.   The  Jacobian  of   the  transformation  is   (for  n
sine  and n  cosine components)
94
  The  Mathematical   Theory  of   Communication
where   the  f,   are  equally  spaced  through   the   band  W.   This   be-
comes  in  the  limit
Since   J  is   constant   its   average  value   is   the   same   quantity  and
applying  the  theorem  on  the  change  of  entropy  with  a  change  of
TABLE  I
ENTROPY   ENTROPY
GAIN   POWER   POWER  GAIN   IMPULSE   RESPONSE
FACTOR   IN   DECIBELS
1
~
1
-8.68
  SIN
 2
/Tt
l-fA/ -----
e
2
  (ITt)2
0
  fA/   I
I
~
l-G12___ ~
(:t
  -5.32
  2  [   SIN  t   _   COS t   ]
t
  3
t
  2
0
fA/   I
I - M . - - - ~ · I \
0.384   -4.15
  6  [   COS  t   -   I   _   COS  t   +  SIN t J
t
  4
2t
2
t
  3
\
-   fA/   I
1--......
<,
il- fA/2---.
  \
  (-!f   -2.66
  ~
.... , '''I
\
  ..
n
  \
w   1
1   ,
:\
:\
  1
_A  ..A"
  I
I r-nc: (,-n\t- r-nc;. t I
~ , . . ~
e:l
Cf
at"  l   J
:
"
0   ,.
•
-
Continuous  Information   95
coordinate8,   the  re8ult   follews.   Vie  may  also  phrase  it   in  terms
of   the   entropy   power.   Thus   if   the   entropy   power   of   the   first
ensemble is N1 that of the second  is
N1 exp   ~ fw log   I Y(f)   1
2
df.
The  final   entropy  power   is   the  initial   entropy  power   multiplied
by  the  geometric  mean  gain  of  the  filter.   If  the  gain  is  measured
in   db,   then  the   output   entropy  power   will   be   increased  by   the
arithmetic mean db gain over W.
In  Table   I   the   entropy  power   loss   has   been   calculated   (and
also  expressed  in  db)   for   a  number  of   ideal   gain  characteristics.
The   impulsive   responses   of   these   filters   are   also   given   for
W =  271",   with  phase assumed  to  be o.
The  entropy  loss  for   many  other   cases   can   be  obtained   from
1
these   results.   For   example   the   entropy   power   factor   2"   for   the
e
first   case   also   applies   to   any  gain   characteristic   obtained  from
1 -   w  by  a  measure  preserving  transformation  of   the   w  axis.   In
particular a  linearly  increasing  gain  G(w) =  w,   or   a  "saw  tooth"
characteristic  between  0  and  1  have  the  same  entropy  loss.   The
reciprocal gain has  the reciprocal  factor.   Thus l   has  the  factor e
2
•
w
Raising the gain to  any power raises the  factor to  this power.
23.   Entropy  of   the  Sum  of   Two   Ensembles
If   we  have  two   ensembles   of   functions   fa(t)   and   gfJ (t)   we   can
form  a  new  ensemble   by  "addItIOn."  Suppose  the  first   ensemble
has   the   probability   density   function   p (X
t
 ,   •   •   •   ,   Xn)   and   the
second  q (Xl'   .   .   • ,x
n
 )  .  Then  the  density  function  for   the  sum is
given  by  the  convolution:
Physically this  corresponds  to  adding the  noises  or  signals  repre-
sented  by  the  original   ensembles  of  functions.
The  following  result   is  derived  in  Appendix  6.
Theorem  15:   Let   the   average   power  of   two   ensembles   be  N1
96   The  Mathematical   Theory  of  Communication
and   .J.V
2
  and   let   their   entropy   powers   be   .J.V
I
  and  ll2'   Then   the
entropy  power of  the  sum, n; is bounded  by
N  I + N 2 < s, < N  I + N 2 •
White   Gaussian   noise   has   the   peculiar   property   that   it   can
absorb any other noise  or signal ensemble which may be added to
it with a resultant entropy power approximately equal to the sum
of   the  white  noise  power   and  the  signal   power   (measured  from
the  average  signal   value,   which   is  normally  zero),   provided  the
signal power is small, in  a  certain sense,  compared to  the  noise.
Consider   the   function   space   associated   with   these   ensembles
having n dimensions. The white noise  corresponds to the spherical
Gaussian  distribution  in  this   space.   The  signal   ensemble   corre-
sponds  to  another probability  distribution,   not necessarily  Gaus-
sian   or   spherical.   Let   the   second   moments   of   this   distribution
about  its  center of   gravity  be  au,   That  is,   if   p (Xl'   •   •   •   , x
n
 )   is
the  density  distribution  function
aii   = J.. -JP(Xi   -   ai)   (Xi   -   ai)   dx, ...  dx;
where the  (Xi   are the  coordinates of the  center of gravity.   Now u.,
is   a   positive   definite   quadratic   form,   and   we   can   rotate   our
coordinate system to  align  it  with  the  principal   directions  of this
form.   aij   is  then  reduced  to  diagonal   form  b
ii
.   We  require  that
each   b
ii
  be   small   compared   to   N,   the   squared   radius   of   the
spherical   distribution.
In  this   case   the   convolution  of   the   noise   and   signal   produce
approximately   a   Gaussian   distribution   whose   corresponding
quadratic form  is
The  entropy  power   of  this  distribution  is
___________[II(N + bid le- 1/_n   _
or  approximately
The   last   term  IS   the   signal   power,   while   the   first   IS   the   nmse
power.
-1\'----
24.   The  Capacity  of   a  Continuous  Channel
In  a  continuous  channel   the  input or   transmitted  signals  will   be
continuous  functions  of time f (t)   belonging to  a  certain  set,   and
the output or received signals will be perturbed versions of  these.
We  will   consider  only   the   case  where   both  transmitted  and   re-
ceived  signals  are  limited to  a  certain band W.   They  can  then  be
specified,   for   a   time   T,   by  2TW  numbers,   and  their   statistical
structure  by   finite   dimensional   distribution  functions.   Thus   the
statistics of the transmitted signal will be determined by
P(Xl' •..  ,x
n
 )   = P(x)
and those  of the  noise  by  the  conditional probability  distribution
P
XlI
  •••   ,   x,,(Yl, ...  ,  Yn)   =  Px(Y)·
The rate  of  tra,nsmission  of  information  for   a  continuous  chan-
nei   is defined  in  a  way  analogous  to  that   for   a  discrete  channel,
namely
R  -   H(x)   -   Hy(x)
where  H (x)   is the entropy  of  the  input  and H; (x)   the  equivoca-
tion.   The   channel   capacity  C  is   defined   as   the   maximum  of   R
when  we vary  the   input   over   all   possible  ensembles.   This  means
that in  a  finite  dimensional  approximation  we must vary  P (x)   -
P (Xl,   .   .   .   , x
n
 )   and  maximize
98   The  Mathematical   Theory  of   Communication
This  can  be  written
rr   P(x,y)
] } P(x, y)   log   P(x)P(y)   dx  dy
using the fact thatJJP(x,  y) log P(x) dx dy  = JP(x) log P(x) dx.
The  channel   capacity  is  thus  expressed  as  follows:
C  =              JP(x, y)   log           dx  dy.
I t   is obvious  in  this  form  that Rand  C  are  independent of  the
coordinate   system  since   the   numerator   and  denominator   in   log
        will   be  multiplied  by  the  same  factors   when  x  and  y
are   transformed  in  any  one-to-one  way.   This   integral   expression
for   C  is  more   general   than  H(x)   -   Hy(x).   Properly  interpreted
(see  Appendix  7)   it  will   always  exist   while  H (x)   -   Hy(x)   may
assume an  indeterminate form  00   -   00   in some cases. This occurs,
for  example,   if  x  is  limited  to  a  surface  of  fewer  dimensions  than
n  in  its n  dimensional   approximation.
If  the  logarithmic  base  used  in  computing  H (x)   and  Hy(x)   is
two  thenC  is the  maximum  number  of  binary  digits that  can  be
sent per second  over the  channel with  arbitrarily  small  equivoca-
tion,   just  as   in  the  discrete  case.   This  can  be  seen  physically  by
dividing  the  space  of   signals  into  a  large  number   of   small   cells,
sufficiently  small   so that   the  probability  density  P; (y)   of   signal
x  being  perturbed  to  point   y  is  substantially  constant  over   a  cell
(either   of   x  or   y).  If   the  cells   are   considered  as   distinct   points
the  situation is essentially the  same  as  a  discrete  channel  and the
proofs  used  there  will   apply.   But   it   is  clear  physically  that   this
quantizmg  of   the   volume   Into   indIvidual   points   cannot   in   any
practical   situation  alter   the   final   answer   significantly,   provided
the   regions   are   sufficiently  small.   Thus   the   capacity  will   be  the
limit   of   the   capacities   for   the   discrete   subdivisions   and   this   is
just  the  continuous  capacity  defined  above.
On the mathematical side it can  be shown first  (see Appendix 7)
that   if   u  is  the   message,   x  is  the  signal,   y   is  the  received  signal
(perturbed  by  noise)   and  v  the  recovered  message  then
H(x)   -Hy(x)  >H(u)  -Hv(u)
The  Continuous   Channel   99
regardless  of  what   operations  are  performed  on  u  to  obtain  x  or
on  y  to obtain v. Thus no matter how we encode the binary digits
to   obtain   the   signal,   or   how  we   decode   the   received   signal   to
recover  the   message,   the  discrete  rate   for   the   binary  digits   does
not   exceed  the   channel   capacity  we  have   defined.   On  the   other
hand,   it is possible  under very  general   conditions  to  find  a  coding
system  for  transmitting  binary  digits  at the  rate  C  with  as  small
an   equivocation  or   frequency   of   errors   as   desired.   This   is   true,
for  example,   if,  when  we take  a  finite   dimensional   approximating
space  for the signal functions,  P (x, y)   is continuous in  both  x  and
y except at a set of points of probability zero.
An  important   special   case   occurs   when   the   noise   is   added  to
the   signal   and   is   independent   of   it   (in   the   probability   sense).
Then   Px(y)   is   a   function   only   of   the   (vector)   difference   n  =
(y  -   x),
Px(y)   =  Q(y  -   x)
and  we  can   assign  a   definite  entropy  to   the   noise   (independent
of   the   statistics   of   the   signal),   namely   the   entropy   of   the   dis-
tribution Q(n).   This  entropy  will   be  denoted  by  H (n).
Theorem  16:   If   the   signal   and  noise  are  independent   and  the
received  signal is  the  sum of  the  transmitted  signal  and  the  noise
then  the  rate  of  transmission  is
R=H(y)  -H(n),
i.e.,   the   entropy   of   the   received   signal   less   the   entropy   of   the
noise.   The  channel capacity is
C  =   Max H(y)   -   H(n).
P(x)
x+  n:
H(x, y)   -   H(x, n)
Expanding   the   left   side   and   using   the   fact   that   x   and   n   are
independent
H(y)   + Hy(x)   -   H(x)   + H(n).
Hence
R  -   H(x)   Hy(x)   -   H(y)   H(n).
Since   II (n)   is   independent   of   P (x),   maximizing   R   requires
maximizing H(y),  the  entropy  of the received  signal. If there  are
100   The  Mathematical   Theory  of Communication
certain   constraints   on   the   em5emble   of   transmitted   signals,   the
entropy of the received signal must be maximized subject to  these
constraints.
25.   Channel   Capacity  with  an  Average  Power   Limitation
A  simple  application  of   Theorem  16  occurs   when  the   noise  is  a
white  thermal   noise  and  the  transmitted  signals  are  limited  to  a
certain average power P.   Then the  received  signals have an  aver-
age  power P + N  where  N  is  the  average  noise  power.  The  max-
imum entropy  for the received signals occurs  when they also  form
a  white  noise  ensemble  since  this  is  the  greatest  possible  entropy
for   a  power P + N  and  can  be  obtained  by  a  suitable  choice  of
the ensemble of transmitted  signals,   namely  if  they  form  a white
noise   ensemble   of   power   P.   The   entropy   (per   second)   of   the
received  ensemble is then
H(y)   =   W  log  27re(P + N),
and the noise  entropy  is
H(n)  =  W  log 27reN.
The channel  capacity  is
C  =  H(y)   -   H(n)   =  W  log   P t N
Summarizing we have  the  following:
Theorem  17:   The  capacity  of   a  channel   of   band  W  perturbed
by  white  thermal  noise  of  power N  when  the  average  transmitter
power is  limited  to P is given  by
C   WI   P+N
This   means   that   by  sufficiently   involved   encoding  systems
we  can  transmit  binary  digits at  the  rate  W  log2   P t   N   bits
per  second,   with  arbitrarily  small  frequency  of errors. It is  not
possible   to   transmit   at   a  higher   rate   by  any  encoding  system
without a  definite  positive frequency  of  errors.
To   approximate   this   limiting  rate   of   transmission  the   trans-
mitted signals must approximate, in statistical properties, a white
The  Continuous  Channel   101
noise."   A  !5ystem  which   approaches   the   ideal   rate   may   be   de-
scribed  as   follows:   Let   lvI   -   2
8
samples   of   white  noise   be   con-
structed  each  of  duration  T.   These  are   assigned  binary  numbers
from  0 to  (M  -   1). At the  transmitter the message  sequences are
broken  up  into  groups  of s and  for  each  group  the   corresponding
noise  sample  is  transmitted  as  the  signal.   At   the  receiver  the  M
samples  are  known  and  the  actual   received  signal   (perturbed  by
noise)   is  compared with  each  of  them.   The  sample which  has the
least   R.M.S.   discrepancy   from  the   received   signal   is   chosen   as
the   transmitted  signal   and  the   corresponding  binary  number  re-
constructed.   This  process  amounts  to  choosing  the most probable
(a  posteriori)   signal.   The  number   M  of   noise  samples  used  will
depend  on  the  tolerable  frequency   e  of  errors,   but   for   almost   all
selections  of samples we have
L
·   L'   log  M(E, T)   -   WI   P +N
im   im   T   -   og   N   '
f-+O   T...... a>
so  that   no  matter   how  small   E  is  chosen,   we  can,   by  taking  T
P+N
sufficiently large, transmit as near as we wish to  TW log   N
binary digits  in  the  time  T.
Formulas similar  to  C  =   W  log   P t N   for   the  white  noise
case   have   been   developed   independently   by   several   other
writers,   although  with  somewhat   different   interpretations.   We
may  mention  the  work  of   N.   Wiener,"   W.   G.   Tuller,"  and  H.
Sullivan  in  this  connection.
In  the   case  of   an   arbitrary  perturbing  noise   (not   necessarily
white   thermal   noise)   it   does   not   appear   that   the   maximizing
problem  involved  in  determining  the  channel   capacity  C  can  be
solved   explicitly.   However,   upper   and   lower   bounds   can   be   set
for C in terms of the  average noise power  1',[ and the noise entropy
power   N\.   These   bounds   are   sufficiently   close  together   in   most
• This  and  other   properties   of   the  white   noise   case   are   discussed  from  the
geometrical   point   of   view  in   "Communication  in   the   Presence   of   Noise,"
loe. cit.
7 Cybernetics,   loco  cit.
8 "Theoretical   Limitations   on   the   R.ate   of   Transmission   of   Information,"
Proceedings   of   the   Institute   of   Radio   Engineers,   v.   37,   No.   5,   May,   1949,
pp.468-78.
102   The  Mathematical   Theory  of  Communication
practical   cases  to  furni8h  a  satisfactory  solution  to  the  problem.
Theorem  18:   The   capacity  of   a  channel   of   band  W  perturbed
by  an  arbitrary  noise  is  bounded  by  the  inequalities
W  I   P + Nt   < C <  W  log   P +  N
og   N
l
  N
1
where
P   =  average  transmitter  power
N   =  average noise  power
N  1 =  entropy  power of  the  noise.
Here  again  the  average  power   of  the   perturbed  signals  will   be
P + N.   The  maximum  entropy  for  this  power  would  occur  if  the
received signal were white noise and would  be W  log 27re (P + N).
It may  not be possible  to  achieve  this;   i.e., there  may not be  any
ensemble  of   transmitted  signals   which,   added  to   the   perturbing
noise,   produce  a  white  thermal   noise  at  the   receiver,   but at  least
this sets an  upper bound to  H (y). We  have,  therefore
C  =   Max H(y)   -   H(n)
< W  log 27re (P + N)   -   W  log 27reN
l
  •
This  is the  upper  limit given  in  the  theorem.   The  lower  limit can
be  obtained  by  considering  the  rate  if   we  make  the  transmitted
signal   a  white  noise,   of  power   P.   In  this  case  the  entropy  power
of the received signal must be at least as  great as  that of  a white
noise  of  power P + N1  since  we have  shown  in  Theorem  15 that
the  entropy  power  of the sum  of two ensembles  is greater than  or
equal to  the sum  of the  individual entropy  powers.   Hence
and
P-   I   M
IV1
As P  increases,   the  upper and  lower  bounds  in  Theorem  18 ap-
proach each  other, so we have  as  an  asymptotic  rate
WI   P+N
If the  noise  is  itself  white,   N =  N1   and  the   result   reduces  to  the
formula  proved  previously:
The  Continuous  Channel   103
C  =  W  log (1 + -;-)"------.   _
If the  noise  is Gaussian  but  with  a  spectrum  which  is  not  nec-
essarily flat,   N
 1
 is the  geometric mean of the noise power  over the
various  frequencies  in  the  band  W.   Thus
N  I   =  exp   ~ fw log  N(f)   df
where  N (f)   is the  noise  power at frequency  f.
Theorem  19:   If   we   set   the   capacity   for   a   qiven   transmitter
power P  equal to
C  =  W  log   P + N   -   11
N
1
then  1] is   monotonic  decreasing  as  P  increases   and  approaches  0
as a  limit.
Suppose that for a given  power P
1
 the  channel capacity is
W  I
  PI + N   -   111
og   N
1
•
This   means   that   the   best   signal   distribution,   say   p (x),   when
added to  the  noise distribution  q (x),   gives  a  received  distribution
r(y)   whose entropy  power  is   (P
1
 + N  -   1]1).   Let  us  increase  the
power   to  P1 +  ~ P by   adding  a  white   noise  of   power   ~ P to   the
signal.   The entropy of the  received  signal is now at least
H (y)   = W  log  271'e (P
1
 + N  -   1]1 +  ~ P )
by  application  of the  theorem  on  the  minimum  entropy  power of
a sum. Hence,   since we can  attain the  H  indicated, the entropy of
the  maximizing distribution  must be at  least as  great and  1] must
be monotonic  decreasing.   To show that   'YJ  ~ 0  as  P  ~ 00   consider
a  signal   which  is a  JNhite  noise with  a  large  P.   Whatever the  per
turbing  noise,   the  received   signal   will   be  approximately  a   white
noise, if  P  is  sufficiently  large,   in  the  sense  of  having  an  entropy
power approaching P + N.
26.   The  Channel   Capacity  with  a   Peak  Power   Limitation
In some applications the  transmitter is limited  not by the  average
104   The  Mathematical   Theory  of  Communication
~ S + N
1rC
N
power output but by  the peak instantaneous power.   The  problem
of   calculating  the   channel   capacity  is   then  that   of   maximizing
(by variation of the ensemble of transmitted symbols)
H(y)   -   H(n)
subject   to   the  constraint   that   all   the  functions  f(t)   in  the  en-
semble  be  less  than  or  equal to VB,  say, for  all t. A constraint
of   this   type  does  not   work  out   as   well   mathematically  as   the
average  power  limitation.   The  most   we  have  obtained  for   this
case is  a  lower bound  valid for   all t, an  "asymptotic"  upper
bound (valid for   large t) and  an  asymptotic  value  of  C for
S
N   small.
Theorem  20:   The  channel  capacity  C  for  a  band W  perturbed
by  white  thermal noise  of power N  is  bounded  by
2   S
C >  W  log   1re3   N'
where   S  is  the   peak   allowed   transmitter   power.   For   sufficiently
S
large   N
where  E is   arbitrarily  small.   As   ~ ~ 0 (and provided the band W
starts  at  0)
------------+-'-C/rIJwV-t}f\1 0gf}"-( 1 +  ~ ) ~ ~ . . . - + 1 - . ---------
We   wish  to   maximize  the  entropy  of   the  received  signal.  If
~ is  large  this  will   occur  very  nearly  when  we  maximize  the
entropy  of the  transmitted  ensemble.
The  asymptotic upper bound is obtained by  relaxing the  condi-
tions  on  the  ensemble.   Let   us suppose   that   the  power   is  limited
to  S not at every  instant  of  time,   but  only  at  the   sample  points.
The  maximum  entropy  of   the   transmitted  ensemble  under   these
The  Continuous  Channel   105
weakened   condition5   i5   certainly  greater   than   or   equal   to   that
under the  original  conditions.   This  altered  problem  can  be  solved
easily.   The  maximum  entropy  occurs   if  the  different   samples  are
independent   and  have  a   distribution  function  which   is   constant
from  -   -vsto +-vs.   The  entropy  can  be  calculated  as
W log 48.
The received signal will then have an  entropy  less than
lV log  (48 + 27reN) (1 + £)
with  e  0 as     ex>   and the  channel capacity is obtained by
subtracting the  entropy  of  the  white  noise,   W  log  27reN:
W  log  (48 + 27reN)   (1 + e) -   W  log  (27reN)
    
7re
=  W  log   N   (1 +e).
This is the desired upper bound to the  channel  capacity.
To  obtain  a  lower   bound  consider  the  same  ensemble  of   func-
tions.   Let these  functions  be passed through  an  ideal  filter  with  a
triangular  transfer  characteristic.   The  gain  is to  be  unity  at  fre-
quency  0  and  decline   linearly  down  to   gain   0  at   frequency  W.
We first   show that  the  output  functions  of  the   filter  have  a  peak
power limitation 8 at all  times   (not just the sample points). First
we note  that a  pulse          going  into  the  filter  produces
1   sin"  7r Wt
In the  output. This  function  is never negative.   The  input function
(in  the  general   case)   can  be  thought  of  as  the  sum  of  a  series  of
shifted  functions
sin  27rWt
a
where  u,   the   amplitude  of   the   sample,   is  not   greater   than yg.
Hence   the   output   is   the   sum  of   shIfted   functIOns   of   the   non-
negative   form  above   with  the  same  coefficients.   These   functions
being  non-negative,   the   greatest   positive  value   for   any  t   is  ob-
106   The  Mathematical   Theory  of  Communication
tained   when   all   the   coefficients   a  have   their   maximum  positive
values,   i.e.,  VB.   In  thIS  case  the  mput   functIOn  was  a   constant
of   amplitude VB  and  since  the  filter   has  unit  gain  for   D.C.,   the
output   is   the   same.   Hence   the   output   ensemble   has   a   peak
power 8.
The entropy of the output ensemble can be  calculated from  that
of   the  input   ensemble  by  using  the  theorem  dealing  with  such  a
situation.   The  output  entropy  is  equal   to  the  input   entropy  plus
the  geometrical   mean  gain  of   the  filter:
W   W   (W   f)2
i   log  G2  df   = i   log   ;   df   =   -   2W.
Hence  the  output  entropy  is
W  log  48   -   2W
  48
-   W  log -2-
e
and  the  channel   capacity  is  greater  than
2   S
W  log --3  N'
7re
We  now  wish  to  show  that,   for  small   ~ (peak  signal   power
over   average   white   noise   power),   the   channel   capacity  is   ap-
proximately
lV
Therefore,   if   we   can   find   an  ensemble   of   functions   such  that
they   correspond   to   a   rate   nearly   W  log   (1 +-i-)  and   are
limited to band Wand peak S  the result will be  proved. Consider
the   ensemble   of   functIOns   of   the  follOWIng   type.   A  series   of   t
samples   have   the   same   value,   either   +VS   or  -VS  !   then
the  next   t   sa.mples   ha.ve   the  same  value,   etc   The   value  for   a
The   Continuous  Channel   107
series  is  chosen  at   random,   probability   ~ for   I vS   and   ~ for
y'S  . If this ensemble be passed through a filter  with triangu-
Iar   gain  characterIstIc   (UnIt   gam  at   D.C.),   the  output   IS   peak
limited to   +  S. Furthermore the average power is  nearly Sand
can   be   made   to   approach   this   by   taking   t   sufficiently   large.
The  entropy  of   the  sum  of   this   and  the   thermal   noise  can  be
found  by applying the theorem on the sum of a noise and a small
signal.   This  theorem  will   apply  if
~ rt:   S
V   t   N
is   sufficiently  small.   This   can   be   insured   by  taking   ~ small
enough  (after  t is  chosen).   The  entropy  power  will   be S+N to
as   close   an   approximation   as   desired,   and   hence   the   rate   of
transmission  as  near  as   we  wish  to
(
 S +N )
Wlog   -N-  .
-¥----
27.   Fidelity  Evaluation   Functions
In  the  case  of   a   discrete  source  of   information  we  were   able   to
determine  a  definite  rate  of   generating  information,   namely  the
entropy  of   the  underlying  stochastic  process.   With  a   continuous
source   the   situation  is   considerably   more   involved.   In  the   first
place   a   continuously   variable   quantity   can   assume   an   infinite
number   of   values   and  requires,   therefore,   an   infinite  number   of
binary digits  for  exact specification.   This  means that to  transmit
the  output of  a  continuous  source  with  exact   recovery  at  the   re-
ceiving  point   requires,   in  general,   a   channel   of   infinite  capacity
(in   bits   per   second).   Since,   ordinarily,   channels   have   a   certain
amount  of  noise,   and  therefore  a  finite   capacity,   exact transmis-
sion is impossible.
This,   however,   evades   the   real   issue.   Practically,   we  are   not
Interested   III   exact   transmission   when   we   have   a   continuous
source, but only  in transmission to within a  certain tolerance. The
question  is,   can  we  assign   a  definite  rate  to  a  continuous  source
when  we require  only  a  certain  fidelity  of  recovery,   measured  in
a   suitable   way.   Of   course,   as   the   fidelity   requirements   are   in-
creased  the  rate  will   increase.   It   will   be  shown  that   we  can,   in
very  general   cases,   define  such   a  rate,   having  the  property  that
it is possible, by  properly encoding the  information, to  transmit it
over   a   channel   whose  capacity  is  equal   to  the   rate   in  question,
and  satisfy  the   fidelity   requirements.   A  channel   of   smaller   ca-
pacity is insufficient.
The  Rate  for  a Continuous Source   109
It is first necessary to  give a  general mathematical  formulation
of  the  Idea  of   fidelity  of   transmission.   Consider  the  set   of   mes-
sages  of  a  long  duration,   say  T  seconds.   The  source   is  described
by  giving  the  probability  density,   P(x),   in  the  associated  space,
that  the  source will  select the message  in  question.   A  given  com-
munication system is described  (from the external point of view)
by  giving  the  conditional   probability  P:c(y)   that  if   message  x   is
produced  by   the   source   the   recovered   message   at   the   receiving
point   will   be   y.   The   system  as   a   whole   (including   source   and
transmission   system)   is   described   by   the   probability   function
P(x, y)   of  having  message   x  and  final   output   y.  If  this  function
is   known,   the   complete   characteristics   of   the   system  from  the
point   of   view  of   fidelity   are   known.   Any   evaluation  of   fidelity
must   correspond   mathematically   to   an   operation   applied   to
P(x, y).   This   operation  must   at   least   have   the   properties   of   a
simple  ordering of systems;   i.e., it must be possible to  say  of  two
systems   represented  by  P
1
 (x , y)   and  P
2
 (x , y)   that,   according  to
our   fidelity   criterion,   either   (1)   the   first   has   higher   fidelity,
(2)  the second has  higher fidelity,   or   (3)   they have equal fidelity.
This  means  that   a   criterion  of   fidelity   can   be  represented  by   a
numerically valued evaluation function:
v(P(x,y))
whose   argument   ranges   over   possible   probability   functions
P(x, y).   The  function  v (P(x, y))   orders   communication  systems
according to fidelity,   and  for  convenience we take  lower values  of
v to  correspond to "higher fidelity."
We  vlill   now  show that  under  very  general   and  reasonable  as-
sumptions the  function  v (P(x, y))   can  be  written  in  a  seemingly
much  more  specialIzed  form,   namely  as  an  average  of  a  function
p(x,   y)   over  the  set of possible  values of  x  and  y:
jJ
v(P(x,  y))   =   { {P(x,  y)   p(x, y)   dx  dy.
To  obtain this  we need  only  assume   (1)   that the  source  and  sys-
tern  are  ergodic  so that   a  very  long  sample  will   be,   with  proba-
bility nearly  1, typical   of the  ensemble,   and   (2)   that the  evalua-
tion  is "reasonable"  in  the  sense that  it  is  possible,   by  observing
a  typical input and output  Xl   and  yl,   to  form  a tentative evalua-
110   The  Mathematical   Theory  of  Communication
tion  on  the  basis   of   these  samples;   and  if   these   samples  are   in-
creased in duration the tentative evaluation will, with  probability
1,   approach   the   exact   evaluation  based  on  a   full   knowledge   of
P(x, y).   Let   the  tentative  evaluation  be  p(x, y).   Then  the   func-
tion   p(x, y)   approaches   (as   T    00)   a   constant   for   almost   all
(z, y)   which   are   in  the  high  probability  region  corresponding  to
the system:
p(x, y)    v(P(x, y))
and we may also  write
p(x,  y)  -4ffP(x,  y)   p(x,  y)   dx  dy
SInce
ffP (x,   y)   dx  dy  =  1.
This establishes the  desired  result.
The   function   p(x, y)   has   the   general   nature   of   a   "distance"
between x  and  y.9 It measures  how undesirable it is   (according to
our   fidelity   criterion)   to   receive   y   when   x   is   transmitted.   The
general   result given  above  can  be  restated  as  follows:   Any  reas-
onable  evaluation  can  be  represented  as  an  average  of  a  distance
function  over the set of messages and  recovered messages  x and  y
weighted  according  to  the  probability  P(x, y)   of  getting  the  pair
in   question,   provided  the  duration   T   of   the   messages   be  taken
sufficiently large.
The  following  are  simple examples of evaluation  functions:
1.   R.M.S.   criterion.
v=  (x(t)   _y(t))2
In  this   very   commonly  used   measure  of   fidelity   the   distance
function   p(x, y)   is   (apart   from  a  constant   factor)   the  square
of  the  ordinary  euclidean  distance  between  the  points  x  and  y
in the  associated function  space
1   r
T
p(x, Y)   -   T  1
0
                       dt
2.   Frequency  weighted  R.M.S.   criterion.   More  generally  one  can
apply  different   weights  to  the   different   frequency  components
9 It   IS  not   a   "metriC"  in   the   strict   sense,   however,   since   in   general   it   does
not   satIsfy  eIther   p(x, y)   -   p(y, x)   or   p(x, y)   + p(y, z) >  p(x, z).
The  Rate  for   a  Continuous  Source   111
before  using  an  R.M.S.   measure  of  fidelity.   This  is  equivalent
to  passing  the  difference   x (t)   Y(t)   through  a  shaping  filter
and then determining the average power in the output. Thus let
e(t)  = x(t)   -   y(t)
and
J(t)   = L:e(r)k(t   -   r)   dr
then
1   r
T
p(x, Y)   = T   1
0
J(t)2 dt.
3.   Absolute error criterion.
1   T
p(x, Y)   = Til x(t)   -   yet) I dt
4.   The structure of the  ear and brain determine implicitly a num-
ber  of  evaluations,   appropriate  in  the  case  of  speech  or   music
transmission.   There  is,   for   example,   an  "intelligibility"   crite-
rion  in  which  p(x, y)   is  equal   to  the  relative  frequency  of   in-
correctly  interpreted  words   when  message  x (t)   is  received  as
y (t).   Although   we   cannot   give   an   explicit   representation   of
p(x,  y)   in  these   cases  it   could,   in  principle,   be  determined  by
sufficient   experimentation.   Some  of   its   properties   follow  from
well-known   experimental   results   in   hearing,   e.g.,   the   ear   is
relatively  insensitive  to  phase  and  the sensitivity  to  amplitude
and frequency  is roughly  logarithmic.
5.   The discrete case can be considered as a specialization in which
we have tacitly  assumed  an  evaluation  based on the  frequency
of   errors.   The  function   p(x, y)   is  then  defined   as  the  number
of  symbols  in  the  sequence  y   differing  from  the  corresponding
symbols in x divided by  the  total number of symbols in  x.
28.   The  Rate   for   a   Source  Relative  to  a   Fidelity  Evaluation
We  are  now  in  a  position  to  define  a  rate  of  generating  informa-
tion  for   a   continuous   source.   Vie  are   given  P(x)   for   the  source
and  an   evaluation  v   determined   by   a   distance   function   p(x,   y)
which  will   be  assumed  continuous  in  both  x  and  y.   With  a  par-
ticular system P (x,  y)   the  quality  is  measured  by
112   The  Mathematical   Theory  of Communication
v   !fp(x, y)   P(x, y)   dx  dy.
Furthermore  the   rate   of   flow  of   binary  digits   corresponding  to
P(x, y)   is
If
  P(x, y)
R  =   P(x, y)   log   P(x)P(y)   dx  dy.
We  define the rate R
1
 of generating information for  a  given qual-
ity  VI   of  reproduction  to  be  the  minimum  of  R  when  we keep   V
fixed at VI and vary Px(y). That is:
If
  P(x, y)
R
1
  =  ~ ( ~ ~ P(x,  y)   log   P(x)P(y)   dx  dy
subject to  the  constraint:
Vi   = ffP(x, y)p(x, y)   dx  dy.
This  means  that   we  consider,   in  effect,   all   the  communication
systems  that  might   be  used  and  that  transmit   with  the  required
fidelity.   The  rate  of  transmission  in  bits  per second  is  calculated
for  each  one  and we choose that having the  least rate.  This  latter
rate is the rate we assign the source  for the  fidelity  in question.
The justification of this definition lies in the  following result:
Theorem  21:   If  a source  has  a  rate  R
l
  for  a  valuation  VI  it  is
possible  to  encode  the  output of the  source and  transmit it  over  a
channel of capacity  C with  fidelity  as near  VI   as desired  provided
R
1
 < C.   This is not  possible if R
1
 >  C.
The   last   statement   in  the   theorem  follows   immediately  from
the  definition  of   R
1
  and  previous  results.   If  it   were  not   true  we
could   transmit   more   than   C  bits   per   second   over   a   channel   of
capacity  C. The  first  part of  the  theorem  is  proved  by  a  method
analogous to that used  for Theorem  11. We may, in the first  place,
divide   the   (x,  y)   space   into   a   large   number   of   small   cells   and
represent the situation as  a disClete case. This will not change  the
evaluation   function   by   more   than   an   arbitrarily   small   amount
(when  the cells are  very small)   because of the continuity assumed
for   p (x,   y)   Suppose that PI (x,  y)   is the  particular system  which
minimizes the rate  and  gives  RIo We  choose from  the  high  proba
bilitJy  y's  a  set at random  containing
The  Rate  for  a  Continuous  Source   113
member8  where   E   0 a8 T    00. "lith  large  T  each  chosen  point
will be connected by  high probability lines   (as in Fig.   10)  to  a set
of  x's.   A  calculation  similar  to  that  used  in  proving  Theorem  11
shows  that   with   large   T   almost   all   x's   are   covered  by   the   fans
from  the  chosen  y   points   for   almost   all   choices   of   the   y's.   The
communication  system  to   be  used  operates   as   follows:   The  se-
lected  points  are  assigned  binary  numbers.   When  a  message   x  is
originated it will   (with  probability  approaching  1 as  T   00)   lie
within at least one of the fans.   The  corresponding binary number
is transmitted  (or one of them  chosen arbitrarily if there  are sev-
eral)   over   the  channel   by  suitable  coding  means  to  give  a  small
probability  of  error.   Since R, <  C  this  is possible.   At the  receiv-
ing  point   the   corresponding   y   is   reconstructed  and  used   as   the
recovered message.
The evaluation     for this system can  be made arbitrarily  close
to  V
 l
  by taking T sufficiently large.   This is due  to  the Iact that for
each  long sample of message  x (t)   and recovered message  y (t)   the
evaluation approaches  VI   (with probability  1).
It   is  interesting  to   note  that,   in  this   system,   the  noise   in  the
recovered   message   is   actually   produced   by   a   kind   of   general
quantizing at the  transmitter  and  is not produced  by  the  noise  in
the   channel.  It  is  more  or   less  analogous  to  the  quantizing  noise
in PCM.
29.   The  Calculation  of   Rates
The  definition of the  rate  is similar in  many  respects  to  the  defi-
nition of channel capacity. In the former
                                      y)   log           dx  dy
,. ,.
J  J
c
with  Px(y)   fixed and possibly one or more  other constraInts   (e.g.,
an  average power  limitation)   of the  form  K = .ff  P (z, y)   A(z, y)
dx  dy
1'.....   partial   solution  of   the  general   maximizing  problem  for   de
114   The  Mathematical   Theory  of  Communication
tennining   the   rate   of   a   source   can   be   gIven.   Using  Lagrange's
method  we consider
fJ[
  P(x,  y)
P(x, y)   log   P(x)P(y)   +  J.I.   P(x,  y)p(x,  y)
+v(x)P(x,  y) ]   dx  dy.
The   variational   equation   (when   we  take   the   first   variation  on
P(x, y))   leads to
Py(x)   =  B(x)   e-Xp(x,y)
where   A is  determined  to  give   the   required  fidelity   and  B (x)   IS
chosen to  satisfy
JB(x)   e-Xp(x.y)   dx   =   1.
This shows that, with best encoding, the  conditional probability
of   a  certain  cause   for   various   received  y,   Py(x)   will   decline   ex-
ponentially  with  the  distance  function  p(x, y)   between  the  x  and
y  in  question.
In  the special  case  where  the  distance  function  p(z, y)   depends
only on the  (vector)   difference between x  and  y,
p(x, y)   =   p(x  -   y)
we have
JB(x)   e-Xp(x-y)   dx  =  1.
Hence  B (x)   is constant, say  «,   and
Unfortunately  these   formal   solutions   are   difficult   to  evaluate  in
particular cases  and  seem to  be of  little  value.   In  fact,   the  actual
calculation of rates has  been  carried out in only  a  few very simple
cases.
If the  distance  function  p(x, y)   is the  mean  square  discrepancy
between  x   and   y   and  the   message   ensemble   is   white   noise,   the
rate  can  be determined.   In  tha.t   case  we have
R  -   Min   [H (x)   -   Hy(x)]   -   H (x)   -   Max  Hy(x)
with  N =  (x  -   y)2.   But  the  Max  Hy(x)   occurs   when  y  -   x  is  a
The  Rate  for  a  Continuous  Source   115
white  noise,   and  is equal   to  H'l   log  27fel'l where  H'l   is  the   band-
width of the  message ensemble.   Therefore
R  =  WI   log  21reQ  -   WI   log  21reN
Q
=   WI   log   N
where Qis the  average message  power. This proves the  following:
Theorem  22:   The  rate  for  a  white  noise  source  of power Qand
band W  1 relative  to an  R.M.S. measure of fidelity  is
Q
R   =   WI   log   N
where   N  is   the   allowed  mean  square  error   between  original   and
recovered  messages.
More  generally with  any message  source we can obtain inequal-
ities   bounding  the  rate  relative  to  a  mean  square  error   criterion.
Theorem 23:   The rate for any source  of band W  1 is bounded by
WI   log   ~ < R <  WI   log   ~
where  Q is  the  average  power of   the  source,   Ql   its  entropy  power
and N  the  allowed mean square  error.
The lower bound  follows from  the  fact that the  Max  HII(x)   for
a  given   (x  -   y)  2  =   N  occurs  in  the  white  noise  case.   The  upper
bound results if we place  the  points  (used  in the proof of Theorem
21)   not   in   the   best   way   but   at   random  in   a   sphere   of   radius
vQ-N.
Acknowledgments
The  writer is indebted  to  his  colleagues  at  the  Laboratories,   par-
ticularly  to  Dr.   H.   W.   Bode,   Dr.   J.   R.   Pierce,   Dr.   B.   McMillan,
and Dr. B. M.  Oliver for  many  helpful  suggestions  and  criticisms
during  the   course   of   this   work.   Credit   should   also   be   given   to
Professor   N.   Wiener,   whose  elegant   solution  of   the   problems   of
filtering  and  prediction  of  stationary  ensembles  has   considerably
influenced the writer's  thinking in this field.
116   The  Mathematical   Theory  of  Communication
Appendix 1.   The  Growth  of   a  Number  of   Blocks  of   Symbols
with  a   Finite  State  Condition
Let N
i
 (L)   be  the  number of  blocks  of  symbols   of  length  L  end-
ing in state i. Then we have
Ni(L)   =        -   bW)
\8
where       bt, ...  , bij  are  the length  of the  symbols  which may
be  chosen  in  state  i   and  lead  to  state  j.   These   are  linear  differ-
ence equations  and  the behavior as L -:,   00   must be of the type
N, =  AjWL.
Substituting in the difference equation
AiWL  =   LAiWL-b!;>
i,   S
or
A·   =           
1   is
For this to  be possible  the  determinant
D(W)   = I a« I = IL        -   Oii I
S
must   vanish   and   this   determines   W,   which   is,   of   course,   the
largest real root  0 f D  =   o.
The quantity  C is then  given by
C   L
·   log        I   W
=   1m   =   og
L ...... a>   L
and  we  also  note   that   the   same   growth   properties   result   if   we
require that all blocks start in the same  (arbitrarily chosen)   state.
Appendix S.   Derivation  of   H
   PI   log PI
__------CL=-e-'---'t'----H'---'----(-tt, -tt, ... ,*) =  A  (n).   From  condition   (3)   we   can
decompose   a   choice   from  8
m
equally  likely  possibilities  into  a
series  of   m  choices  each  from  8   equally  likely  possibilities  and
obta.in
Appendixes
Similarly
A(t") -nA(t).
We can choose n arbitrarily large and find  an m to satisfy
sm < t
n
< s(m+l).
Thus, taking logarithms and dividing by n  log s,
.!!!:- <   log  t   < ~ +_1_ or   .!!!:- _   log  t   <  E
n   -   log  s   -   n   n   n   log  s
117
where   f   is arbitrarily small.   Now from  the monotonic property  of
A (n),
A(sm)   < A(t
n
)   < A (sm+l)
m A (s) < nA (t)   <  (m + 1)  A (s) .
Hence, dividing by nA (s),
~ <   A (t)   < ~ +_1_  or
n   A(s)   -   n   n
m
--
n
A(t)
A(s)
  <E
A(t)
A(s)
log  t
log  s
  A (t)   =  -   K  log t
where  K  must be positive to  satisfy  (2).
Now  suppose  we  have  a  choice  from  n  possibilities  with  com-
measurable  probabilities  Pi =  "<;!n
i
where  the  ni   are  integers.   We
~ n i
can break down  a  choice  from  ~ n i possibilities into  a  choice  from
n  possibilities with  probabilities Pl' ...  ,Pn and  then,   if  the ith
was  chosen,   a  choice  from  ni   with  equal   probabilities.  Using  con-
clItion 3  again,   we  equate  the  total   chOIce  from  ~ n i as   computed
by  two  methods
Hence
If   the   Pi   are   incommeasurable,   they   may   be   approximated   by
rationals   and  the  same   expression  must   hold   by   our   continuity
assumption.   Thus  the  expression  holds   in  general.   The  choice  of
118   The  Mathematical   Theory  of   Communication
coefficient If:   i15   a matter of  convenience and  amount15 to  the  choice
of  a unit of measure.
Appendix 3.   Theorems  on  Ergodic   Sources
We  assume the source to  be ergodic so that the strong law of  large
numbers  can  be  applied.   Thus  the  number  of   times  a  given  path
Pij   in  the  network  is  traversed  in  a  long  sequence  of   length  N  is
about   proportional   to  the  probability  of   being  at   i,   say  Pi,   and
then  choosing  this  path,   PiPijN. If  N  is  large  enough  the  proba-
bility  of  percentage  error   -t-   0 in  this  is  less than  e  so that  for   all
but   a  set   of   small   probability  the  actual   numbers   lie  within  the
limits
(PiPij   -t- o)N.
Hence nearly  all  sequences  have  a  probability P given  by
_ n   (PiPij ±fJ)N
P  -   Pii
log  P
and   N   is  limited  by
log  P
N   =  !-(PiPii +  0)  log  Pii
or
log  P
N   -   !-PiPii  log  Pii   <  'YJ.
This  proves  Theorem  3.
Theorem  4  follows   immediately  from  this  on  calculating  upper
and  lovler   bounds  for   n(q)   based  on  the  possible  range  of   values
of   p in  Theorem 3.
In the mIxed   ~ not ergodIC)   case  If
and  the  entropies  of   the  components  are   Hi >  H
2
 > ... >  H
R
we have the
{   ~ . d   .   1·
({)   q   1,8 uecreustny  step J unction,
N ~ o o
.   log n  (q)
Theorem:   Lun   N
8-1   8
1   1
'(J(q)   =  H
8
 in  the  internal L  ai   <  q < L  ai-
Appendixes   119
To  prove  theorems  5 and  6  first   note  that F
N
  is  monotonic  de
creasing  because  increasing  N  adds   a   subscript   to  a   conditional
entropy.   A  simple substitution  for   PBi (8;)   in  the definItion  of   F'N
shows that
F
N
 = N G
N
  -   (N -   1)  G
N
-
 1
and   summmg   this   for   all   N   gives   G
N
  =   ~ ~ FN  •   Hence  GN
>  F
N
  and  G
N
  monotonic  decreasing.   Also  they  must   approach
the  same  limit.   By  using  Theorem  3  we  see  that   Lim GN   =   H.
N-+a:>
Appendix 4.   Maximizing  the   Rate   for   a   System  of   Constraints
Suppose   we   have   a   set   of   constraints   on   sequences   of   symbols
that   is   of   the   finite   state  type  and  can  be  represented  therefore
by  a  linear graph,   as  in  Fig.   2. Let   l ~ ; > be  the  lengths  of  the  vari-
ous   symbols   that   can   occur   in   passing   from  state   i   to   state   j.
What  distribution  of   probabilities  Pi   for   the  different  states  and
pi;>   for   choosing  symbol   s  in  state  i   and  going  to  state  j   maxi-
mizes  the  rate of  generating  information  under these  constraints?
The  constraints  define  a  discrete  channel   and  the  maximum  rate
must   be   less   than   or   equal   to   the   capacity   C  of   this   channel,
since   if   all   blocks   of   large   length  were   equally   likely,   this   rate
would   result,   and   if   possible   this   would  be   best.   We   will   show
that this rate  can  be achieved by proper choice  of the Pi  and  p W.
The  rate  in  question  is
"P   (8)   I   (8)
-   ~ iPii   og  Pii
l,],  8
i. i,  8
Let
LP   (8)   Z(8)
iPif   if
(8)   B,   W   l ( ~ )
Pii   -   B.   'J
t
where  the Bi   satisfy  the  equations
This   homogeneous  system  has   a  non-vanishing  solution  since  W
is such  that the determinant  of  the  coefficients is  zero:
120   The  Mathematical   Theory  of   Communication
-----------1   WI!;' -   Oii   1__0_.   _
The      defined  thus  are  satisfactory  transition  probabilities  for
in  the first place,
"'"   (3)   _   "'"   Bi         
LJp"   -LJ-   ./
.     .   B
1.3   1.3   i
so that the  sum  of  the  probabilities  from  any  particular  junction
point is unity.   Furthermore they  are non-negative  as  can  be seen
from  a   consideration  of   the  quantities   Ai   given   in  Appendix  1.
The Ai   are  necessarily  non-negative  and  the  B;   satisfy  a  similar
system  of equations but with  i   and  j   interchanged.   This  amounts
to  reversing the orientation on the  lines  of the  graph.
Substituting the  assumed  values  of pi;)   in  the  general  equation
for   the rate  we obtain
P
  (3)   I   Bi   W   1(')
 iPii   og &   -  'i
         
..   ,      
log           -        log  Bj +       log  Bi
         
   
=  log  W  =  C.
Hence  the  rate  with  this  set   of   transition  probabilities  is  C  and
since  this rate could  never be exceeded  this  is the maximum.
Appendix 6
Let  8
1
  be  any  measurable  subset   of   the   9  ensemble,   and  8
2
  the
subset   of  the f   ensemble  which  gives  8
1
  under   the  operation  T.
Then
Let   H>.   be  the  operator  which  ghiftg  all   functiong  in  a  get  by  the
time  A.   Then
Appendixes   121
8ince  T  i8 invariant   and  therefore  commutes  with  H>".   Hence   if
mLS1   is the probability  measure  of the set S
m[H>"SI]   =   m[TH>"S2]   =   m[H>"S2]
=   m[S2]   =   m[Sd
where  the  second  equality  is   by   definition  of   measure   in   the   g
space,   the  third  since   the f   ensemble  is   stationary,   and  the  last
by  definition of   g measure  again.   This shows that the  g ensemble
is  stationary.
To  prove that the ergodic property  is preserved  under invariant
operations, let 8
1
 be  a subset of  the  g ensemble which  is invariant
under  H>..,   and  let 8
2
  be the set of  all   functions f  which transform
into  8
1
•   Then
H>"SI   =  H>..TS
2
=  TH>"S2  =  SI
so that H>"8
2
  is included in 8
2
  for  all   A.   Now,   since
m[H>"S2]   =  m[S2]   =  m[SI]
this  implies
H>"8
2
 =  S2
for  all  Awith m[8
2
 ]   =1= 0,1. This contradiction shows that 8
1
 does
not exist.
Appendix 6
The   upper   bound,   N
3
 < N
1
 + N
2
 ,   is   due   to   the   fact   that   the
maximum  possible  entropy  for   a  power N
1
 + N
2
  occurs  when  we
have a white  noise of this power. In this  case the entropy power is
To  obtain  the  lower   bound,   suppose  we  have  two  distributions
in  n dimensions  p (xd   and  q (Xi)   with  entropy  powers N1   and N2.
What   form  should  p  and  q  have  to  minimize  the  entropy  power
1"'1
3
 of their convolution  r(xd :
The  entropy  H
3
  of  r   is  given  by
H;   -
  (
122   The  Mathematical   Theory  of Communication
Vie  wish  to  minimize  this  subj ect  to  the   constraints
We  consider then
u  =  - f [r(x)   log  r(x) + Ap(X) log  p(x) + J.l.q(x)   log  q(x)]   dx
oU  =   -   f[[1 + log  r(x)] or(x) + A[1 + log  p(x)] op(x)
+J.I.[1 + log  q(x)  oq(x)]]   dx.
If p(x)   is varied at a particular argument  Xi =  Si, the variation
in  r (x)   is
8r(x)  = qi»,  -   s.)
and
and  similarly  when  q  is  varied.   Hence  the  conditions  for   a  mini-
mum  are
fq(Xi   -   s.)   log  r(xi)   =   -   Alog  P(Si)
fP(Xi   -   s.)   log  r(xi)   =   -   J.I.   log  q(Si).
If   we   multiply   the   first   by  p(sd   and  the   second  by  q(sd   and
integrate  with  respect   to   S   we  obtain
H
3=-AH1
H
3=-p.H2
or solving for  Aand  p. and replacing in  the  equations
J
Now suppose p(xd  and q(xd  are  normal
Appendixes   123
Then  r(x
l
  )   will   also   be   normal   with   quadratic   form  C
l   J
 •  If  the
inverses of  these forms  are  au, bij,  Cij  then
We  wish  to  show that  these  functions  satisfy  the  minimizing  con-
ditions  if   and  only  if   aij = Kbi,   and  thus  give  the  minimum  H
3
under the constraints. First we have
log  r(xi)   =   ~ log   2
17r
I c; I -   !  '};CiiXiXi
Jq(Xi -   Si)   log r(xi) =   ~ log   2
17r
I c; I-! '};Ci i
8i8i
-!  '};Ciibi i·
This  should  equal
u,   [n   1   I   I
ffi   2   log   27r   A if
which  requires  A
i i
  =   Z:   c.;
HI
In   this   case   A
i i
  =   H
2
Bu   and   both   equations   reduce   to
identities.
Appendix 7
The   following   will   indicate   a   more   general   and   more   rigorous
approach   to   the   central   definitions   of   communication   theory.
Consider a  probability  measure  space whose  elements  are  ordered
pairs   (x, y). The variables  x,  yare  to  be  identified  as  the  possible
transmitted  and  received  signals  of   some   long  duration  T.   Let us
call  the set of all   points whose  x  belongs to  a  subset 8
1
  of  x  points
the  strip  over 8
1
,   and  sImilarly  the  set   whose  y   belong  to  8
2
  the
strip  over   S.).   We   divide   x   and   y   into   a   collection  of   non-over-
lapping  measurable   subsets   Xi   and   Y
 i
  approximate   to   the  rate
of  transmission  R  by
where
p (Xd  is  the  probability  measure  of  the  strip  over Xi
P( Yd  is  the  probability  measure  of   the  strip  over   Y
i
124   The  Mathematical   Theory  of  Communication
P ( X ~ , Y
l
  )   is the  probability  measure  of  the  intersection  of  the
strips.
A  further   subdivision   can   never   decrease   R
l.
  For   let   Xl   be
divided  into  Xl =  X ~ + Xr   and  let
P(Y1)   =  a   P(X1)   =  b +c
P ( X ~ ) =   b   P ( X ~ , Y1)   =   d
P ( X ~ ) =  c   P ( X ~ , Y1)   =  e
P(X1,   Y1)   =  d + e.
Then in the sum  we have replaced  (for  the  Xl,   Y
l
  intersection)
d +e   d   e
(d + e) log   a(b + c)   by d log   ab  + e log   ac '
It is easily shown that with the  limitation we have  on b, c, d, e,
[
  d +e Jd+e   dd·ee
<---
b +c   -   b"c
e
and  consequently  the sum  is increased.   Thus  the  various  possible
subdivisions   form  a   directed   set,   with   R  monotonic   increasing
with  refinement   of   the  subdivision.   We   may  define  R  unambig-
uously  as  the  least  upper  bound  for   the   R
l
  and  write  it
1 Jf   P(x, y)
R  = T   P(x,  y)   log   P(x)P( y)   dx  dy.
This   integral,   understood  in  the   above   sense,   includes   both  the
continuous   and  discrete   cases   and  of   course   many  others   which
cannot   be  represented  in  either-form.   It  is  trivial   in  this   formu-
lation  that  if   x  and  u  are   in  one-to-one  correspondence,   the   rate
from  u  to  y  is equal  to that from  x  to  y.   If  v  is any  function  of  y
(not   necessarily  with   an   inverse)   then   the   rate   from  x   to   y   is
greater than  or  equal to that from  x  to  v  since,   in  the  calculation
of   the   approximations,   the   subdivisions   of   yare   essentially   a
finer   subdIviSIOn  of   those   for   v.   More   generally  If   y   and   v   are
related   not   functionally   but   statistically,   i.e.,   we   have   a   prob-
ability  measure  space   (Y,v),   then   R(x,v)   <  R(x,y).   This  means
that  any  operation  applied  to  the  received  signal,   even  though  it
involves  statistical   elements,   does  not   increase  R.
Another   notion  which  should  be   defined  precisely  in  an  ab-
Appendixes   125
stract   formulation   of   the   theory  is   that   of   "dimension  rate,"
that   is  the  average  number   of  dimensions  required  per  second
to  specIfy  a  member  of  an  ensemble.   In  the   band  limited  case
2W  numbers  per  second  are  sufficient.   A general   definition  can
be  framed  as  follows.   Let faCt)   be an  ensemble  of functions  and
let   PT[fa(t), ftJ(t)]   be  a  metric  measuring  the  "distance"  from  fa
to  ffJ over the time  T  (for example  the  R.M.S.   discrepancy  over
this  interval).   Let   N(E,   0,   T)   be  the  least  number  of  elements f
which   can   be   chosen  such  that   all   elements   of   the   ensemble
apart   from  a   set   of   measure  0  are  within  the  distance   E  of  at
least   one   of   those   chosen.   Thus   we  are  covering  the  space  to
within   E  apart   from  a   set   of   small   measure   o.   We   define   the
dimension rate  Afor   the  ensemble  by  the  triple  limit
"   L·   L·   L·   log  N(E, 0, T)
A=   1m   1m   1m   .
~ - + O e-+O   T-+oo   T  log  E
This is a generalization of the  measure type  definitions of  dimen-
sion in  topology,   and  agrees  with  the  intuitive dimension rate  for
simple  ensembles   where  the   desired  result   is  obvious.